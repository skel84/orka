This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  workflows/
    ops-smoke.yml
crates/
  api/
    src/
      lib.rs
    Cargo.toml
  apply/
    src/
      lib.rs
    Cargo.toml
  cli/
    src/
      main.rs
    Cargo.toml
    README.md
  core/
    src/
      lib.rs
    Cargo.toml
  kubehub/
    src/
      lib.rs
    Cargo.toml
  ops/
    src/
      lib.rs
    Cargo.toml
  persist/
    src/
      lib.rs
    Cargo.toml
  schema/
    src/
      lib.rs
    Cargo.toml
  search/
    examples/
      bench.rs
    src/
      lib.rs
    tests/
      multigvk_determinism.rs
      replay.rs
      shards.rs
    Cargo.toml
  store/
    src/
      lib.rs
    tests/
      multigvk_replay.rs
      replay.rs
      sharded_determinism.rs
    Cargo.toml
docs/
  imperative_ops.md
examples/
  configmap.yaml
scripts/
  kind-nightly.sh
  kind-ops-smoke.sh
.gitignore
Cargo.toml
MILESTONE-0.md
MILESTONE-1.md
MILESTONE-2.md
MILESTONE-3.md
MILESTONE-5-gui.md
MILESTONE-API.md
MILESTONE-OPS-3.1.md
plan.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="crates/api/src/lib.rs">
//! Orka public API façade (in-process).
//!
//! This crate defines the stable traits and types frontends (CLI/GUI) depend on.
//! Implementations can be in-process (direct) or remote (RPC) in later milestones.

#![forbid(unsafe_code)]

use serde::{Deserialize, Serialize};

pub use orka_ops::OrkaOps; // Re-export imperative ops trait
pub use orka_schema::CrdSchema; // Re-export schema type
use std::collections::HashMap;

/// A served Kubernetes resource kind (incl. CRDs).
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
pub struct ResourceKind {
    pub group: String,
    pub version: String,
    pub kind: String,
    pub namespaced: bool,
}

impl From<orka_kubehub::DiscoveredResource> for ResourceKind {
    fn from(v: orka_kubehub::DiscoveredResource) -> Self {
        Self { group: v.group, version: v.version, kind: v.kind, namespaced: v.namespaced }
    }
}

/// Object reference for raw access.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
pub struct ResourceRef {
    /// Optional cluster identifier (empty/current when in-process)
    pub cluster: Option<String>,
    pub gvk: ResourceKind,
    pub namespace: Option<String>,
    pub name: String,
}

/// Selector describing the current world scope (single GVK + optional namespace).
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
pub struct Selector {
    pub gvk: ResourceKind,
    pub namespace: Option<String>,
}

/// Stats and runtime configuration exposed to clients.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, Default)]
pub struct Stats {
    pub shards: usize,
    pub relist_secs: u64,
    pub watch_backoff_max_secs: u64,
    pub max_labels_per_obj: Option<usize>,
    pub max_annos_per_obj: Option<usize>,
    pub max_postings_per_key: Option<usize>,
    pub max_rss_mb: Option<usize>,
    pub max_index_bytes: Option<usize>,
    pub metrics_addr: Option<String>,
}

/// API errors suitable for transport over RPC later.
#[derive(Debug, thiserror::Error, Serialize, Deserialize)]
pub enum OrkaError {
    #[error("capability: {0}")]
    Capability(String),
    #[error("validation: {0}")]
    Validation(String),
    #[error("conflict: {0}")]
    Conflict(String),
    #[error("not_found: {0}")]
    NotFound(String),
    #[error("internal: {0}")]
    Internal(String),
}

pub type OrkaResult<T> = Result<T, OrkaError>;

/// Lightweight change event carrying shaped objects for UI consumption.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum LiteEvent {
    Applied(orka_core::LiteObj),
    Deleted(orka_core::LiteObj),
}

/// Declarative Orka API surface.
#[async_trait::async_trait]
pub trait OrkaApi: Send + Sync {
    async fn discover(&self) -> OrkaResult<Vec<ResourceKind>>;

    /// Return a consistent snapshot for the given selector (single-GVK, optional ns).
    async fn snapshot(&self, selector: Selector) -> OrkaResult<orka_core::WorldSnapshot>;

    /// Search within the selector scope.
    async fn search(
        &self,
        selector: Selector,
        query: &str,
        limit: usize,
    ) -> OrkaResult<(Vec<orka_search::Hit>, orka_search::SearchDebugInfo)>;

    /// Fetch raw JSON bytes for a given object reference.
    async fn get_raw(&self, reference: ResourceRef) -> OrkaResult<Vec<u8>>;

    /// Server-side dry-run for a YAML payload; returns humanized diff summary.
    async fn dry_run(&self, yaml: &str) -> OrkaResult<orka_apply::DiffSummary>;

    /// Server-side apply (SSA) for a YAML payload.
    async fn apply(&self, yaml: &str) -> OrkaResult<orka_apply::ApplyResult>;

    /// Runtime stats and limits.
    async fn stats(&self) -> OrkaResult<Stats>;

    /// Stream deltas for a GVK + optional namespace.
    async fn watch(&self, selector: Selector) -> OrkaResult<StreamHandle<orka_core::Delta>>;

    /// Stream Lite events (Applied/Deleted) with projected fields and basic dedup.
    async fn watch_lite(&self, selector: Selector) -> OrkaResult<StreamHandle<LiteEvent>>;

    /// Fetch CRD schema for a GVK key (e.g. "group/v1/Kind" or "v1/Kind").
    /// Returns None for built-in kinds without a CRD.
    async fn schema(&self, gvk_key: &str) -> OrkaResult<Option<CrdSchema>>;
}

// ----------------- Mock implementation -----------------

/// Simple in-memory mock implementation for tests.
pub struct MockApi {
    pub kinds: Vec<ResourceKind>,
    pub snapshot: Option<orka_core::WorldSnapshot>,
    pub hits: Vec<orka_search::Hit>,
    pub debug: orka_search::SearchDebugInfo,
    pub raw_obj: Option<Vec<u8>>, // JSON
    pub dry: Option<orka_apply::DiffSummary>,
    pub apply: Option<orka_apply::ApplyResult>,
    pub stats: Stats,
    pub schemas: HashMap<String, CrdSchema>,
}

impl Default for MockApi {
    fn default() -> Self {
        Self {
            kinds: Vec::new(),
            snapshot: None,
            hits: Vec::new(),
            debug: orka_search::SearchDebugInfo {
                total: 0,
                after_ns: 0,
                after_label_keys: 0,
                after_labels: 0,
                after_anno_keys: 0,
                after_annos: 0,
                after_fields: 0,
            },
            raw_obj: None,
            dry: None,
            apply: None,
            stats: Stats::default(),
            schemas: HashMap::new(),
        }
    }
}

impl MockApi { pub fn new() -> Self { Self::default() } }

// ----------------- In-process implementation -----------------

/// In-process implementation that calls internal crates directly.
pub struct InProcApi;

impl InProcApi {
    pub fn new() -> Self { Self }

    fn map_err(e: anyhow::Error) -> OrkaError { OrkaError::Internal(e.to_string()) }

    fn gvk_key(gvk: &ResourceKind) -> String {
        if gvk.group.is_empty() { format!("{}/{}", gvk.version, gvk.kind) } else { format!("{}/{}/{}", gvk.group, gvk.version, gvk.kind) }
    }
}

#[async_trait::async_trait]
impl OrkaApi for InProcApi {
    async fn discover(&self) -> OrkaResult<Vec<ResourceKind>> {
        let v = orka_kubehub::discover(false).await.map_err(Self::map_err)?;
        Ok(v.into_iter().map(|r| r.into()).collect())
    }

    async fn snapshot(&self, selector: Selector) -> OrkaResult<orka_core::WorldSnapshot> {
        use std::sync::Arc;
        use tokio::sync::mpsc;
        let gvk_key = Self::gvk_key(&selector.gvk);
        // Projector from CRD schema if available
        let projector = match orka_schema::fetch_crd_schema(&gvk_key).await {
            Ok(Some(schema)) => Some(Arc::new(schema.projector()) as Arc<dyn orka_core::Projector + Send + Sync>),
            _ => None,
        };
        let cap = std::env::var("ORKA_QUEUE_CAP").ok().and_then(|s| s.parse::<usize>().ok()).unwrap_or(2048);
        let (tx, mut rx) = mpsc::channel::<orka_core::Delta>(cap);
        // Fire a one-shot list
        let _ = orka_kubehub::prime_list(&gvk_key, selector.namespace.as_deref(), &tx).await.map_err(|e| OrkaError::Internal(e.to_string()))?;
        drop(tx);
        // Collect deltas
        let mut deltas: Vec<orka_core::Delta> = Vec::new();
        while let Some(d) = rx.recv().await { deltas.push(d); }
        // Build snapshot
        let mut builder = orka_store::WorldBuilder::with_projector(projector);
        builder.apply(deltas);
        let snap = builder.freeze();
        Ok((*snap).clone())
    }

    async fn search(
        &self,
        selector: Selector,
        query: &str,
        limit: usize,
    ) -> OrkaResult<(Vec<orka_search::Hit>, orka_search::SearchDebugInfo)> {
        let snap = self.snapshot(selector.clone()).await?;
        // Field mapping and metadata
        let gvk_key = Self::gvk_key(&selector.gvk);
        let (group, kind) = (selector.gvk.group.clone(), selector.gvk.kind.clone());
        let pairs: Option<Vec<(String, u32)>> = match orka_schema::fetch_crd_schema(&gvk_key).await {
            Ok(Some(schema)) => Some(schema.projected_paths.iter().map(|p| (p.json_path.clone(), p.id)).collect()),
            _ => None,
        };
        let index = match pairs {
            Some(p) => orka_search::Index::build_from_snapshot_with_meta(&snap, Some(&p), Some(&kind), Some(&group)),
            None => orka_search::Index::build_from_snapshot_with_meta(&snap, None, Some(&kind), Some(&group)),
        };
        let (hits, dbg) = index.search_with_debug_opts(query, limit, Default::default());
        Ok((hits, dbg))
    }

    async fn get_raw(&self, reference: ResourceRef) -> OrkaResult<Vec<u8>> {
        use kube::{discovery::{Discovery, Scope}, core::DynamicObject, api::Api};
        let client = kube::Client::try_default().await.map_err(|e| OrkaError::Internal(e.to_string()))?;
        // Locate ApiResource via discovery
        let gvk = kube::core::GroupVersionKind { group: reference.gvk.group.clone(), version: reference.gvk.version.clone(), kind: reference.gvk.kind.clone() };
        let discovery = Discovery::new(client.clone()).run().await.map_err(|e| OrkaError::Internal(e.to_string()))?;
        let mut ar_opt: Option<(kube::core::ApiResource, bool)> = None;
        for group in discovery.groups() {
            for (ar, caps) in group.recommended_resources() {
                if ar.group == gvk.group && ar.version == gvk.version && ar.kind == gvk.kind {
                    ar_opt = Some((ar.clone(), matches!(caps.scope, Scope::Namespaced)));
                    break;
                }
            }
        }
        let (ar, namespaced) = ar_opt.ok_or_else(|| OrkaError::NotFound(format!("GVK not found: {}/{}/{}", gvk.group, gvk.version, gvk.kind)))?;
        let api: Api<DynamicObject> = if namespaced {
            match reference.namespace.as_deref() {
                Some(ns) => Api::namespaced_with(client.clone(), ns, &ar),
                None => return Err(OrkaError::Validation("namespace required for namespaced kind".into())),
            }
        } else { Api::all_with(client.clone(), &ar) };
        let obj = api.get(&reference.name).await.map_err(|e| OrkaError::Internal(e.to_string()))?;
        let bytes = serde_json::to_vec(&obj).map_err(|e| OrkaError::Internal(e.to_string()))?;
        Ok(bytes)
    }

    async fn dry_run(&self, yaml: &str) -> OrkaResult<orka_apply::DiffSummary> {
        let (live, _last) = orka_apply::diff_from_yaml(yaml, None).await.map_err(|e| OrkaError::Internal(e.to_string()))?;
        Ok(live)
    }

    async fn apply(&self, yaml: &str) -> OrkaResult<orka_apply::ApplyResult> {
        let res = orka_apply::edit_from_yaml(yaml, None, false, true).await.map_err(|e| OrkaError::Internal(e.to_string()))?;
        Ok(res)
    }

    async fn stats(&self) -> OrkaResult<Stats> {
        let shards = std::env::var("ORKA_SHARDS").ok().and_then(|s| s.parse().ok()).unwrap_or(1);
        let relist_secs = std::env::var("ORKA_RELIST_SECS").ok().and_then(|s| s.parse().ok()).unwrap_or(300);
        let watch_backoff_max_secs = std::env::var("ORKA_WATCH_BACKOFF_MAX_SECS").ok().and_then(|s| s.parse().ok()).unwrap_or(30);
        let max_labels_per_obj = std::env::var("ORKA_MAX_LABELS_PER_OBJ").ok().and_then(|s| s.parse().ok());
        let max_annos_per_obj = std::env::var("ORKA_MAX_ANNOS_PER_OBJ").ok().and_then(|s| s.parse().ok());
        let max_postings_per_key = std::env::var("ORKA_MAX_POSTINGS_PER_KEY").ok().and_then(|s| s.parse().ok());
        let max_rss_mb = std::env::var("ORKA_MAX_RSS_MB").ok().and_then(|s| s.parse().ok());
        let max_index_bytes = std::env::var("ORKA_MAX_INDEX_BYTES").ok().and_then(|s| s.parse().ok());
        let metrics_addr = std::env::var("ORKA_METRICS_ADDR").ok();
        Ok(Stats { shards, relist_secs, watch_backoff_max_secs, max_labels_per_obj, max_annos_per_obj, max_postings_per_key, max_rss_mb, max_index_bytes, metrics_addr })
    }

    async fn watch(&self, selector: Selector) -> OrkaResult<StreamHandle<orka_core::Delta>> {
        use tokio::sync::mpsc;
        let cap = std::env::var("ORKA_QUEUE_CAP").ok().and_then(|s| s.parse::<usize>().ok()).unwrap_or(2048);
        let (tx, rx) = mpsc::channel::<orka_core::Delta>(cap);
        let gvk_key = Self::gvk_key(&selector.gvk);
        let ns = selector.namespace.clone();
        let handle = tokio::spawn(async move {
            let _ = orka_kubehub::start_watcher(&gvk_key, ns.as_deref(), tx).await;
        });
        Ok(StreamHandle { rx, cancel: CancelHandle { task: Some(handle) } })
    }

    async fn watch_lite(&self, selector: Selector) -> OrkaResult<StreamHandle<LiteEvent>> {
        use tokio::sync::mpsc;
        let cap = std::env::var("ORKA_QUEUE_CAP").ok().and_then(|s| s.parse::<usize>().ok()).unwrap_or(2048);
        let (delta_tx, mut delta_rx) = mpsc::channel::<orka_core::Delta>(cap);
        let (evt_tx, evt_rx) = mpsc::channel::<LiteEvent>(cap);
        let gvk_key = Self::gvk_key(&selector.gvk);
        let ns = selector.namespace.clone();
        // Optional projector from CRD
        let projector = match orka_schema::fetch_crd_schema(&gvk_key).await {
            Ok(Some(schema)) => Some(std::sync::Arc::new(schema.projector()) as std::sync::Arc<dyn orka_core::Projector + Send + Sync>),
            _ => None,
        };
        let max_labels_per_obj = std::env::var("ORKA_MAX_LABELS_PER_OBJ").ok().and_then(|s| s.parse::<usize>().ok());
        let max_annos_per_obj = std::env::var("ORKA_MAX_ANNOS_PER_OBJ").ok().and_then(|s| s.parse::<usize>().ok());

        // Spawn watcher feeding deltas
        let watcher = tokio::spawn({
            let g = gvk_key.clone();
            async move {
                let _ = orka_kubehub::start_watcher(&g, ns.as_deref(), delta_tx).await;
            }
        });

        // Shaping helper
        fn shape_lite(
            d: &orka_core::Delta,
            projector: &Option<std::sync::Arc<dyn orka_core::Projector + Send + Sync>>,
            max_labels: Option<usize>,
            max_annos: Option<usize>,
        ) -> orka_core::LiteObj {
            use smallvec::SmallVec;
            let meta = d.raw.get("metadata");
            let name = meta.and_then(|m| m.get("name")).and_then(|v| v.as_str()).unwrap_or("").to_string();
            let namespace = meta.and_then(|m| m.get("namespace")).and_then(|v| v.as_str()).map(|s| s.to_string());
            let creation_ts = meta
                .and_then(|m| m.get("creationTimestamp")).and_then(|v| v.as_str())
                .and_then(|s| chrono::DateTime::parse_from_rfc3339(s).ok())
                .map(|dt| dt.timestamp())
                .unwrap_or(0);
            let projected: SmallVec<[(u32, String); 8]> = if let Some(pj) = projector { pj.project(&d.raw) } else { SmallVec::new() };
            let mut labels = SmallVec::<[(String, String); 8]>::new();
            let mut annotations = SmallVec::<[(String, String); 4]>::new();
            if let Some(meta_obj) = d.raw.get("metadata").and_then(|m| m.as_object()) {
                if let Some(lbls) = meta_obj.get("labels").and_then(|m| m.as_object()) {
                    for (k, v) in lbls.iter() {
                        if let Some(val) = v.as_str() { labels.push((k.clone(), val.to_string())); }
                        if let Some(cap) = max_labels { if labels.len() >= cap { break; } }
                    }
                }
                if let Some(ann) = meta_obj.get("annotations").and_then(|m| m.as_object()) {
                    for (k, v) in ann.iter() {
                        if let Some(val) = v.as_str() { annotations.push((k.clone(), val.to_string())); }
                        if let Some(cap) = max_annos { if annotations.len() >= cap { break; } }
                    }
                }
            }
            orka_core::LiteObj { uid: d.uid, namespace, name, creation_ts, projected, labels, annotations }
        }

        // Processing task: dedup by resourceVersion and emit LiteEvent
        let proc_task = tokio::spawn(async move {
            let mut last_rv: HashMap<orka_core::Uid, String> = HashMap::new();
            let mut last_lite: HashMap<orka_core::Uid, orka_core::LiteObj> = HashMap::new();
            while let Some(d) = delta_rx.recv().await {
                match d.kind {
                    orka_core::DeltaKind::Applied => {
                        let rv = d
                            .raw.get("metadata").and_then(|m| m.get("resourceVersion")).and_then(|v| v.as_str())
                            .unwrap_or("").to_string();
                        let should_emit = match last_rv.get(&d.uid) {
                            Some(prev) if prev == &rv => false,
                            _ => true,
                        };
                        if !should_emit { continue; }
                        last_rv.insert(d.uid, rv);
                        let lo = shape_lite(&d, &projector, max_labels_per_obj, max_annos_per_obj);
                        last_lite.insert(d.uid, lo.clone());
                        let _ = evt_tx.send(LiteEvent::Applied(lo)).await;
                    }
                    orka_core::DeltaKind::Deleted => {
                        if let Some(lo) = last_lite.remove(&d.uid) {
                            let _ = evt_tx.send(LiteEvent::Deleted(lo)).await;
                        } else {
                            // Best-effort shape from deletion payload
                            let lo = shape_lite(&d, &projector, max_labels_per_obj, max_annos_per_obj);
                            let _ = evt_tx.send(LiteEvent::Deleted(lo)).await;
                        }
                        last_rv.remove(&d.uid);
                    }
                }
            }
            // channel closed; watcher will also stop once delta_tx is dropped
            let _ = watcher.abort();
        });

        // The processing task owns delta_rx; aborting it will drop rx and end watcher
        Ok(StreamHandle { rx: evt_rx, cancel: CancelHandle { task: Some(proc_task) } })
    }

    async fn schema(&self, gvk_key: &str) -> OrkaResult<Option<CrdSchema>> {
        orka_schema::fetch_crd_schema(gvk_key)
            .await
            .map_err(|e| OrkaError::Internal(e.to_string()))
    }
}

// ----------------- Streaming primitives -----------------

/// Cancellation handle that aborts the underlying task.
pub struct CancelHandle { task: Option<tokio::task::JoinHandle<()>> }

impl CancelHandle { pub fn cancel(mut self) { if let Some(h) = self.task.take() { h.abort(); } } }

/// Generic stream handle used by API streaming endpoints.
pub struct StreamHandle<T> { pub rx: tokio::sync::mpsc::Receiver<T>, pub cancel: CancelHandle }

#[async_trait::async_trait]
impl OrkaApi for MockApi {
    async fn discover(&self) -> OrkaResult<Vec<ResourceKind>> { Ok(self.kinds.clone()) }

    async fn snapshot(&self, _selector: Selector) -> OrkaResult<orka_core::WorldSnapshot> {
        self.snapshot.clone().ok_or_else(|| OrkaError::NotFound("no snapshot".into()))
    }

    async fn search(
        &self,
        _selector: Selector,
        _query: &str,
        _limit: usize,
    ) -> OrkaResult<(Vec<orka_search::Hit>, orka_search::SearchDebugInfo)> {
        Ok((self.hits.clone(), self.debug.clone()))
    }

    async fn get_raw(&self, _reference: ResourceRef) -> OrkaResult<Vec<u8>> {
        self.raw_obj.clone().ok_or_else(|| OrkaError::NotFound("no raw".into()))
    }

    async fn dry_run(&self, _yaml: &str) -> OrkaResult<orka_apply::DiffSummary> {
        self.dry.clone().ok_or_else(|| OrkaError::Internal("no dry-run configured".into()))
    }

    async fn apply(&self, _yaml: &str) -> OrkaResult<orka_apply::ApplyResult> {
        self.apply.clone().ok_or_else(|| OrkaError::Internal("no apply configured".into()))
    }

    async fn stats(&self) -> OrkaResult<Stats> { Ok(self.stats.clone()) }

    async fn watch(&self, _selector: Selector) -> OrkaResult<StreamHandle<orka_core::Delta>> {
        use tokio::sync::mpsc;
        // Empty stream by default for the mock
        let (_tx, rx) = mpsc::channel(1);
        Ok(StreamHandle { rx, cancel: CancelHandle { task: None } })
    }

    async fn watch_lite(&self, _selector: Selector) -> OrkaResult<StreamHandle<LiteEvent>> {
        use tokio::sync::mpsc;
        let (_tx, rx) = mpsc::channel(1);
        Ok(StreamHandle { rx, cancel: CancelHandle { task: None } })
    }

    async fn schema(&self, gvk_key: &str) -> OrkaResult<Option<CrdSchema>> {
        Ok(self.schemas.get(gvk_key).cloned())
    }
}
</file>

<file path="crates/api/Cargo.toml">
[package]
name = "orka_api"
version = "0.1.0"
edition = "2021"

[dependencies]
anyhow = { workspace = true }
thiserror = { workspace = true }
serde = { workspace = true, features = ["derive"] }
serde_json = { workspace = true }
async-trait = "0.1"
orka-core = { path = "../core" }
orka-apply = { path = "../apply" }
orka-search = { path = "../search" }
orka_ops = { path = "../ops" }
orka-kubehub = { path = "../kubehub" }
orka-schema = { path = "../schema" }
kube = { workspace = true }
k8s-openapi = { workspace = true }
tokio = { workspace = true }
orka-store = { path = "../store" }
chrono = { workspace = true }
smallvec = { workspace = true }
</file>

<file path="MILESTONE-API.md">
# milestone-api.md

orka_api Façade
===============

Introduce a stable API surface (`orka_api`) that decouples frontends (CLI, GUI)
from internal crates.  
This milestone delivers an **in-process façade only**. Remote RPC transport is
out of scope and will be addressed later (M4).

---

## Scope

1. **Trait definitions**
   - `OrkaApi`: declarative operations
     - `discover() -> [ResourceKind]`
     - `search(query, limit) -> (Vec<Hit>, Explain)`
     - `snapshot(selector) -> WorldSnapshot`
     - `get_raw(ref) -> bytes`
     - `dry_run(yaml) -> DiffSummary`
     - `apply(yaml) -> ApplyResult`
     - `stats() -> Stats`
   - `OrkaOps`: imperative operations (defined in milestone-ops.md)
     - Logs, exec, port-forward, scale, rollout restart, delete pod, cordon, drain.

2. **In-proc implementation**
   - Wrap existing crates (`store`, `search`, `apply`, `persist`) behind `OrkaApi`.
   - Wrap `orka_ops` crate behind `OrkaOps`.

3. **Error model**
   - Return typed errors (`CapabilityError`, `ValidationError`, `ConflictError`).
   - Ensure errors are serializable for later RPC use.

4. **Testing & mocks**
   - Provide mock implementations for unit testing frontends.
   - Verify end-to-end: CLI uses only `orka_api`/`orka_ops` interfaces.

---

## Non-Goals

- No gRPC/JSON-RPC server.  
- No auth/session layer.  
- No multi-tenant or networked semantics.  

---

## Deliverables

- `orka_api` crate in workspace.  
- Public traits: `OrkaApi`, `OrkaOps`.  
- In-proc impls calling existing crates.  
- Mock implementations for GUI tests.  
- Documentation page: “orka_api” describing the surface and intended stability.  

---

## Notes

- This is a **compatibility layer**, not a rewrite.  
- All types used are existing stable structs (LiteObj, DiffSummary, ApplyResult, Stats, etc.).  
- RPC transport (M4) will implement these traits remotely, reusing the same shapes.  
- Both `orka_cli` and `orka_gui` depend only on `orka_api`/`orka_ops`, not on internal crates directly.
</file>

<file path=".github/workflows/ops-smoke.yml">
name: ops-smoke

on:
  push:
    branches: [ main ]
  pull_request:
  workflow_dispatch:

jobs:
  kind-smoke:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo
        uses: Swatinem/rust-cache@v2

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: v1.30.2

      - name: Setup kind
        uses: helm/kind-action@v1.10.0
        with:
          cluster_name: orka-ops-smoke

      - name: Ops smoke test
        env:
          RUSTFLAGS: -D warnings
        run: |
          bash scripts/kind-ops-smoke.sh
</file>

<file path="crates/apply/Cargo.toml">
[package]
name = "orka-apply"
version = "0.1.0"
edition = "2021"

[dependencies]
anyhow = { workspace = true }
serde = { workspace = true, features = ["derive"] }
serde_json = { workspace = true }
serde_yaml = "0.9"
tracing = { workspace = true }
metrics = { workspace = true }
kube = { workspace = true }
k8s-openapi = { workspace = true }
uuid = { workspace = true }
orka-persist = { path = "../persist" }
time = "0.3"
</file>

<file path="crates/ops/src/lib.rs">
//! Orka Ops (Milestone OPS 3.1): imperative Kubernetes operations.
//! Initial scaffold with `logs` implemented and other ops stubbed.

#![forbid(unsafe_code)]

use anyhow::{anyhow, Result};
use futures::StreamExt;
use kube::{api::{Api, LogParams, Patch, PatchParams, PostParams, ListParams, DeleteParams}, Client};
use serde::{Deserialize, Serialize};
use tokio::sync::{mpsc, oneshot};
use tracing::{info, warn};

/// A single chunk of log output (line oriented for now).
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LogChunk {
    pub line: String,
}

/// Options for `logs` operation.
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct LogOptions {
    /// Follow the stream (default: true)
    pub follow: bool,
    /// Tail last n lines (server-side), if supported
    pub tail_lines: Option<i64>,
    /// Only return logs newer than X seconds
    pub since_seconds: Option<i64>,
}

/// Cancellation handle for an in-flight operation.
#[derive(Debug)]
pub struct CancelHandle {
    tx: Option<oneshot::Sender<()>>,
}

impl CancelHandle {
    pub fn cancel(mut self) {
        if let Some(tx) = self.tx.take() { let _ = tx.send(()); }
    }
}

/// Result of starting a streaming operation.
pub struct StreamHandle<T> {
    pub rx: mpsc::Receiver<T>,
    pub cancel: CancelHandle,
}

/// Imperative ops trait. Methods may stream or perform one-shot mutations.
#[allow(unused_variables)]
#[async_trait::async_trait]
pub trait OrkaOps: Send + Sync {
    /// Stream logs from a pod/container.
    async fn logs(&self, namespace: Option<&str>, pod: &str, container: Option<&str>, opts: LogOptions) -> Result<StreamHandle<LogChunk>>;

    // Stubs for upcoming ops in this milestone
    async fn exec(&self, namespace: Option<&str>, pod: &str, container: Option<&str>, cmd: &[String], pty: bool) -> Result<()> { Err(anyhow!("exec: not implemented yet")) }
    async fn port_forward(&self, namespace: Option<&str>, pod: &str, local: u16, remote: u16) -> Result<StreamHandle<ForwardEvent>> { Err(anyhow!("port-forward: not implemented yet")) }
    async fn scale(&self, gvk_key: &str, namespace: Option<&str>, name: &str, replicas: i32, use_subresource: bool) -> Result<()>;
    async fn rollout_restart(&self, gvk_key: &str, namespace: Option<&str>, name: &str) -> Result<()>;
    async fn delete_pod(&self, namespace: &str, pod: &str, grace_seconds: Option<i64>) -> Result<()>;
    async fn cordon(&self, node: &str, on: bool) -> Result<()>;
    async fn drain(&self, node: &str) -> Result<()>;
}

/// Default implementation using kube-rs client APIs.
pub struct KubeOps;

impl KubeOps {
    pub fn new() -> Self { Self }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpsCaps {
    pub namespace: Option<String>,
    pub pods_log_get: bool,
    pub pods_exec_create: bool,
    pub pods_portforward_create: bool,
    pub nodes_patch: bool,
    pub pods_eviction_create: Option<bool>,
    pub scale: Option<ScaleCaps>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ScaleCaps {
    pub gvk: String,
    pub resource: String,
    pub subresource_patch: bool,
    pub spec_replicas_patch: bool,
}

impl KubeOps {
    async fn ssar_check(client: Client, ns: Option<&str>, group: &str, resource: &str, subresource: Option<&str>, verb: &str) -> Result<bool> {
        use k8s_openapi::api::authorization::v1::{ResourceAttributes, SelfSubjectAccessReview, SelfSubjectAccessReviewSpec};
        let api: Api<SelfSubjectAccessReview> = Api::all(client);
        let ra = ResourceAttributes {
            group: if group.is_empty() { None } else { Some(group.to_string()) },
            resource: Some(resource.to_string()),
            subresource: subresource.map(|s| s.to_string()),
            verb: Some(verb.to_string()),
            namespace: ns.map(|s| s.to_string()),
            ..Default::default()
        };
        let ssar = SelfSubjectAccessReview {
            spec: SelfSubjectAccessReviewSpec { resource_attributes: Some(ra), ..Default::default() },
            ..Default::default()
        };
        let created = api.create(&PostParams::default(), &ssar).await?;
        Ok(created.status.map(|s| s.allowed).unwrap_or(false))
    }

    /// Discover imperative ops capabilities (subresources + RBAC) for current user.
    /// If `scale_gvk` is provided (e.g. "apps/v1/Deployment"), also probes Scale subresource.
    pub async fn discover_caps(namespace: Option<&str>, scale_gvk: Option<&str>) -> Result<OpsCaps> {
        let client = Client::try_default().await?;
        let ns_owned = namespace.map(|s| s.to_string());
        // Pods subresources (namespace-scoped)
        let pods_log_get = KubeOps::ssar_check(client.clone(), namespace, "", "pods", Some("log"), "get").await.unwrap_or(false);
        let pods_exec_create = KubeOps::ssar_check(client.clone(), namespace, "", "pods", Some("exec"), "create").await.unwrap_or(false);
        let pods_portforward_create = KubeOps::ssar_check(client.clone(), namespace, "", "pods", Some("portforward"), "create").await.unwrap_or(false);
        // Nodes patch (cluster-scoped)
        let nodes_patch = KubeOps::ssar_check(client.clone(), None, "", "nodes", None, "patch").await.unwrap_or(false);
        // Eviction create (namespace-scoped, group=policy)
        let pods_eviction_create = match namespace { Some(ns) => Some(KubeOps::ssar_check(client.clone(), Some(ns), "policy", "pods", Some("eviction"), "create").await.unwrap_or(false)), None => None };

        // Scale subresource for provided GVK
        let mut scale_caps: Option<ScaleCaps> = None;
        if let Some(gvk_key) = scale_gvk {
            use kube::core::GroupVersionKind;
            let (group, version, kind) = parse_gvk_key(gvk_key)?;
            let gvk = GroupVersionKind { group, version, kind };
            let (ar, namespaced) = find_api_resource(client.clone(), &gvk).await?;
            let resource = ar.plural.clone();
            // Check patch on subresource 'scale'
            let sub_patch = KubeOps::ssar_check(client.clone(), if namespaced { namespace } else { None }, ar.group.as_str(), resource.as_str(), Some("scale"), "patch").await.unwrap_or(false);
            // Check patch on main resource (SSA fallback of .spec.replicas)
            let spec_patch = KubeOps::ssar_check(client.clone(), if namespaced { namespace } else { None }, ar.group.as_str(), resource.as_str(), None, "patch").await.unwrap_or(false);
            scale_caps = Some(ScaleCaps { gvk: format!("{}/{}/{}", ar.group, ar.version, ar.kind), resource, subresource_patch: sub_patch, spec_replicas_patch: spec_patch });
        }

        Ok(OpsCaps {
            namespace: ns_owned,
            pods_log_get,
            pods_exec_create,
            pods_portforward_create,
            nodes_patch,
            pods_eviction_create,
            scale: scale_caps,
        })
    }

    /// Instance wrapper for capability discovery.
    pub async fn caps(&self, namespace: Option<&str>, scale_gvk: Option<&str>) -> Result<OpsCaps> {
        // Reuse the static helper to avoid duplicating logic
        Self::discover_caps(namespace, scale_gvk).await
    }
}

#[async_trait::async_trait]
impl OrkaOps for KubeOps {
    async fn logs(&self, namespace: Option<&str>, pod: &str, container: Option<&str>, opts: LogOptions) -> Result<StreamHandle<LogChunk>> {
        use k8s_openapi::api::core::v1::Pod;

        let client = Client::try_default().await?;
        let api: Api<Pod> = match namespace {
            Some(ns) => Api::namespaced(client, ns),
            None => return Err(anyhow!("namespace is required for pod logs")),
        };

        let mut lp = LogParams::default();
        lp.follow = opts.follow;
        lp.tail_lines = opts.tail_lines.map(|v| v as i64);
        lp.since_seconds = opts.since_seconds.map(|v| v as i64);
        if let Some(c) = container { lp.container = Some(c.to_string()); }

        let cap = std::env::var("ORKA_OPS_QUEUE_CAP").ok().and_then(|s| s.parse().ok()).unwrap_or(1024);
        let (tx, rx) = mpsc::channel::<LogChunk>(cap);
        let (cancel_tx, cancel_rx) = oneshot::channel::<()>();
        let cancel = CancelHandle { tx: Some(cancel_tx) };

        // Spawn a task to stream logs and forward into bounded channel.
        let pod_name = pod.to_string();
        let container_label = container.map(|s| s.to_string());
        tokio::spawn(async move {
            use tokio_util::{compat::FuturesAsyncReadCompatExt, io::ReaderStream};
            info!(pod = %pod_name, container = ?container_label, follow = lp.follow, tail = ?lp.tail_lines, since = ?lp.since_seconds, "logs stream starting");
            let stream_res = api.log_stream(&pod_name, &lp).await;
            let reader = match stream_res {
                Ok(r) => r,
                Err(e) => { warn!(error = %e, "log_stream failed to open"); return; }
            };
            // Convert futures::io::AsyncRead into tokio::io::AsyncRead, then into a bytes Stream
            let compat_reader = reader.compat();
            let stream = ReaderStream::new(compat_reader);
            pump_bytes_to_lines(stream, tx, cancel_rx, Some(&pod_name)).await;
        });

        Ok(StreamHandle { rx, cancel })
    }

    async fn exec(&self, namespace: Option<&str>, pod: &str, container: Option<&str>, cmd: &[String], pty: bool) -> Result<()> {
        use k8s_openapi::api::core::v1::Pod;
        use kube::api::{AttachParams, TerminalSize};
        use futures::SinkExt;
        use tokio::io::AsyncWriteExt;
        use std::io::Read;

        struct RawGuard;
        impl Drop for RawGuard { fn drop(&mut self) { let _ = crossterm::terminal::disable_raw_mode(); } }

        let ns = namespace.ok_or_else(|| anyhow!("namespace is required for exec"))?;
        let client = Client::try_default().await?;
        let api: Api<Pod> = Api::namespaced(client, ns);
        let mut ap = if pty { AttachParams::interactive_tty() } else { AttachParams::default() };
        if let Some(c) = container { ap = ap.container(c); }
        if pty { ap = ap.stderr(false); } else { ap = ap.stdout(true).stderr(true); }

        let mut attached = api.exec(pod, cmd.to_vec(), &ap).await?;

        // TTY raw mode + resize support
        let mut resize_task = None;
        let _raw_guard = if pty {
            let _ = crossterm::terminal::enable_raw_mode();
            // Initial size and SIGWINCH updates
            if let Some(mut tx) = attached.terminal_size() {
                let (w, h) = crossterm::terminal::size().unwrap_or((80, 24));
                let _ = tx.send(TerminalSize { height: h as u16, width: w as u16 }).await;
                #[cfg(unix)]
                {
                    use tokio::signal::unix::{signal, SignalKind};
                    let mut sig = signal(SignalKind::window_change())?;
                    resize_task = Some(tokio::spawn(async move {
                        while sig.recv().await.is_some() {
                            if let Ok((w, h)) = crossterm::terminal::size() {
                                let _ = tx.send(TerminalSize { height: h as u16, width: w as u16 }).await;
                            }
                        }
                    }));
                }
            }
            Some(RawGuard)
        } else { None };

        // stdin: spawn blocking reader thread -> channel -> async writer
        let mut stdin_task = None;
        if pty {
            // Only wire stdin in TTY/interactive mode
            let mut writer = attached.stdin().expect("stdin writer missing");
            let (tx, mut rx) = tokio::sync::mpsc::channel::<Vec<u8>>(32);
            std::thread::spawn(move || {
                let mut input = std::io::stdin();
                let mut buf = [0u8; 1024];
                loop {
                    match input.read(&mut buf) {
                        Ok(0) => { let _ = tx.blocking_send(Vec::new()); break; }
                        Ok(n) => { let _ = tx.blocking_send(buf[..n].to_vec()); }
                        Err(_) => { let _ = tx.blocking_send(Vec::new()); break; }
                    }
                }
            });
            stdin_task = Some(tokio::spawn(async move {
                while let Some(chunk) = rx.recv().await { if chunk.is_empty() { break; } if writer.write_all(&chunk).await.is_err() { break; } }
            }));
        }

        // stdout/stderr pumps
        let mut out_task = None;
        if let Some(stdout_reader) = attached.stdout() {
            let mut stream = tokio_util::io::ReaderStream::new(stdout_reader);
            let task = tokio::spawn(async move {
                while let Some(Ok(bytes)) = stream.next().await { print!("{}", String::from_utf8_lossy(&bytes)); }
            });
            out_task = Some(task);
        }
        let mut err_task = None;
        if let Some(stderr_reader) = attached.stderr() {
            let mut stream = tokio_util::io::ReaderStream::new(stderr_reader);
            let task = tokio::spawn(async move {
                while let Some(Ok(bytes)) = stream.next().await { eprint!("{}", String::from_utf8_lossy(&bytes)); }
            });
            err_task = Some(task);
        }

        // Wait for remote to terminate OR Ctrl-C for graceful abort
        tokio::select! {
            _ = attached.join() => {
                // remote process finished
            }
            _ = tokio::signal::ctrl_c() => {
                warn!("Ctrl-C received during exec; closing session");
                // Drop attached to close underlying streams/websocket
            }
        }
        // Don't await stdin task; process is over. It will end when process exits.
        if let Some(t) = stdin_task { t.abort(); }
        if let Some(t) = out_task { let _ = t.await; }
        if let Some(t) = err_task { let _ = t.await; }
        if let Some(t) = resize_task { let _ = t.await; }
        drop(_raw_guard); // ensure raw mode disabled
        Ok(())
    }

    async fn port_forward(&self, namespace: Option<&str>, pod: &str, local: u16, remote: u16) -> Result<StreamHandle<ForwardEvent>> {
        let ns = namespace.ok_or_else(|| anyhow!("namespace is required for port-forward"))?;
        KubeOps::pf_internal(ns, pod, local, remote).await
    }

    async fn scale(&self, gvk_key: &str, namespace: Option<&str>, name: &str, replicas: i32, use_subresource: bool) -> Result<()> {
        use kube::core::{DynamicObject, GroupVersionKind};
        let client = Client::try_default().await?;
        let (group, version, kind) = parse_gvk_key(gvk_key)?;
        let gvk = GroupVersionKind { group, version, kind };
        let (ar, namespaced) = find_api_resource(client.clone(), &gvk).await?;
        let api_do: Api<DynamicObject> = if namespaced {
            match namespace { Some(ns) => Api::namespaced_with(client.clone(), ns, &ar), None => return Err(anyhow!("namespace required for namespaced kind")), }
        } else { Api::all_with(client.clone(), &ar) };

        // Try scale subresource if requested
        if use_subresource {
            let pp = PatchParams::default();
            let payload = serde_json::json!({"spec": {"replicas": replicas}});
            match api_do.patch_scale(name, &pp, &Patch::Merge(&payload)).await {
                Ok(_) => return Ok(()),
                Err(e) => warn!(error = %e, "patch_scale failed; falling back to spec.replicas"),
            }
        }
        // Fallback to patching .spec.replicas
        let pp = PatchParams::default();
        let payload = serde_json::json!({"spec": {"replicas": replicas}});
        let _ = api_do.patch(name, &pp, &Patch::Merge(&payload)).await?;
        Ok(())
    }

    async fn rollout_restart(&self, gvk_key: &str, namespace: Option<&str>, name: &str) -> Result<()> {
        use kube::{core::{DynamicObject, GroupVersionKind}};
        let client = Client::try_default().await?;
        let (group, version, kind) = parse_gvk_key(gvk_key)?;
        let gvk = GroupVersionKind { group, version, kind };
        let (ar, namespaced) = find_api_resource(client.clone(), &gvk).await?;
        let api_do: Api<DynamicObject> = if namespaced {
            match namespace { Some(ns) => Api::namespaced_with(client.clone(), ns, &ar), None => return Err(anyhow!("namespace required for namespaced kind")), }
        } else { Api::all_with(client.clone(), &ar) };
        let ts = chrono::Utc::now().to_rfc3339();
        let patch = serde_json::json!({
            "spec": {"template": {"metadata": {"annotations": {"kubectl.kubernetes.io/restartedAt": ts}}}}
        });
        let pp = PatchParams::default();
        let _ = api_do.patch(name, &pp, &Patch::Merge(&patch)).await?;
        Ok(())
    }

    async fn delete_pod(&self, namespace: &str, pod: &str, grace_seconds: Option<i64>) -> Result<()> {
        use k8s_openapi::api::core::v1::Pod;
        let client = Client::try_default().await?;
        let api: Api<Pod> = Api::namespaced(client, namespace);
        let dp = DeleteParams { grace_period_seconds: grace_seconds.map(|v| v as u32), ..Default::default() };
        let _ = api.delete(pod, &dp).await?;
        Ok(())
    }

    async fn cordon(&self, node: &str, on: bool) -> Result<()> {
        use k8s_openapi::api::core::v1::Node;
        let client = Client::try_default().await?;
        let api: Api<Node> = Api::all(client);
        let pp = PatchParams::default();
        let patch = serde_json::json!({"spec": {"unschedulable": on}});
        let _ = api.patch(node, &pp, &Patch::Merge(&patch)).await?;
        Ok(())
    }

    async fn drain(&self, node: &str) -> Result<()> {
        use k8s_openapi::api::core::v1::Pod;
        use std::collections::HashSet;
        let client = Client::try_default().await?;
        let all_pods: Api<Pod> = Api::all(client.clone());
        let lp = ListParams::default().fields(&format!("spec.nodeName={}", node));

        let timeout_secs: u64 = std::env::var("ORKA_DRAIN_TIMEOUT_SECS").ok().and_then(|s| s.parse().ok()).unwrap_or(300);
        let poll_secs: u64 = std::env::var("ORKA_DRAIN_POLL_SECS").ok().and_then(|s| s.parse().ok()).unwrap_or(2);
        let deadline = std::time::Instant::now() + std::time::Duration::from_secs(timeout_secs);

        // Helper to filter target pods for eviction
        let mut list_target = || async {
            let mut targets: Vec<(String,String)> = Vec::new();
            let pods = all_pods.list(&lp).await?;
            for p in pods.items {
                let meta = &p.metadata;
                let ns = match meta.namespace.clone() { Some(ns) => ns, None => continue };
                let name = match meta.name.clone() { Some(n) => n, None => continue };
                // Skip DaemonSet-managed pods (ownerReferences contains DaemonSet)
                let skip_ds = meta.owner_references.as_ref().map(|ors| ors.iter().any(|o| o.kind == "DaemonSet")).unwrap_or(false);
                // Skip mirror/static pods
                let is_mirror = meta.annotations.as_ref().and_then(|a| a.get("kubernetes.io/config.mirror")).is_some();
                if skip_ds || is_mirror { continue; }
                targets.push((ns, name));
            }
            Ok::<Vec<(String,String)>, anyhow::Error>(targets)
        };

        // Initial eviction attempts
        let mut pending: HashSet<(String,String)> = list_target().await?.into_iter().collect();
        let ep = kube::api::EvictParams::default();
        for (ns, name) in pending.clone().into_iter() {
            let pods_ns: Api<Pod> = Api::namespaced(client.clone(), &ns);
            match pods_ns.evict(&name, &ep).await {
                Ok(_) => {}
                Err(kube::Error::Api(ae)) if ae.code == 429 => {
                    // Blocked by PDB; keep in pending
                    warn!(ns = %ns, pod = %name, "eviction blocked by PDB (429) - will retry");
                }
                Err(e) => {
                    warn!(ns = %ns, pod = %name, error = %e, "eviction error - will retry");
                }
            }
        }

        // Wait loop until all targets are gone or timeout
        loop {
            // Refresh pending set from live cluster
            let fresh = list_target().await?;
            pending = fresh.into_iter().collect();
            if pending.is_empty() { break; }
            if std::time::Instant::now() >= deadline {
                let remain: Vec<String> = pending.iter().map(|(ns,n)| format!("{}/{}", ns, n)).collect();
                return Err(anyhow!("drain timeout; remaining: {}", remain.join(", ")));
            }
            // Re-attempt evictions for remaining pods
            for (ns, name) in pending.clone().into_iter() {
                let pods_ns: Api<Pod> = Api::namespaced(client.clone(), &ns);
                match pods_ns.evict(&name, &ep).await {
                    Ok(_) => {}
                    Err(kube::Error::Api(ae)) if ae.code == 429 => {
                        // PDB still preventing disruption; keep waiting
                    }
                    Err(_e) => { /* best-effort; try again next round */ }
                }
            }
            tokio::time::sleep(std::time::Duration::from_secs(poll_secs)).await;
        }
        Ok(())
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ForwardEvent {
    Ready(String),
    Connected(String),
    Closed,
    Error(String),
}

impl KubeOps {
    async fn pf_internal(namespace: &str, pod: &str, local: u16, remote: u16) -> Result<StreamHandle<ForwardEvent>> {
        use k8s_openapi::api::core::v1::Pod;
        use tokio::net::TcpListener;
        let client = Client::try_default().await?;
        let api: Api<Pod> = Api::namespaced(client, namespace);
        let mut pf = api.portforward(pod, &[remote]).await?;
        let (tx, rx) = mpsc::channel::<ForwardEvent>(16);
        let (cancel_tx, mut cancel_rx) = oneshot::channel::<()>();
        let cancel = CancelHandle { tx: Some(cancel_tx) };
        let bind_addr = std::env::var("ORKA_PF_BIND").unwrap_or_else(|_| "127.0.0.1".to_string());
        let listener = TcpListener::bind((bind_addr.as_str(), local)).await?;
        let actual = listener.local_addr()?;
        let _ = tx.send(ForwardEvent::Ready(actual.to_string())).await;
        tokio::spawn(async move {
            loop {
                tokio::select! {
                    _ = &mut cancel_rx => { let _ = tx.send(ForwardEvent::Closed).await; break; }
                    accept_res = listener.accept() => {
                        match accept_res {
                            Ok((mut inbound, peer)) => {
                                let _ = tx.send(ForwardEvent::Connected(peer.to_string())).await;
                                match pf.take_stream(remote) {
                                    Some(mut stream) => {
                                        let _ = tokio::io::copy_bidirectional(&mut inbound, &mut stream).await;
                                        // drop stream and connection; continue accepting
                                    }
                                    None => {
                                        let _ = tx.send(ForwardEvent::Error("pf stream missing".into())).await;
                                    }
                                }
                            }
                            Err(e) => { let _ = tx.send(ForwardEvent::Error(format!("accept error: {}", e))).await; break; }
                        }
                    }
                }
            }
        });
        Ok(StreamHandle { rx, cancel })
    }
}

/// Internal: consume a stream of bytes, split into lines, send via bounded channel.
/// Drops lines when channel is full. Flushes last partial line on end.
async fn pump_bytes_to_lines<S, E>(stream: S, mut tx: mpsc::Sender<LogChunk>, mut cancel_rx: oneshot::Receiver<()>, ctx: Option<&str>)
where
    S: futures::Stream<Item = Result<bytes::Bytes, E>>,
    E: std::fmt::Display,
{
    let mut stream = stream.fuse();
    futures::pin_mut!(stream);
    let mut buf = bytes::BytesMut::new();
    loop {
        tokio::select! {
            _ = &mut cancel_rx => { if let Some(c) = ctx { info!(ctx = %c, "log pump cancelled"); } break; }
            next = stream.next() => {
                match next {
                    Some(Ok(chunk)) => {
                        buf.extend_from_slice(&chunk);
                        while let Some(pos) = buf.iter().position(|&b| b == b'\n') {
                            let line = buf.split_to(pos);
                            let _ = buf.split_to(1); // drop '\n'
                            if let Ok(s) = std::str::from_utf8(&line) {
                                let _ = tx.try_send(LogChunk { line: s.to_string() });
                            }
                        }
                    }
                    Some(Err(e)) => { if let Some(c) = ctx { warn!(ctx = %c, error = %e, "log stream error"); } else { warn!(error = %e, "log stream error"); } break; }
                    None => break,
                }
            }
        }
    }
    if !buf.is_empty() {
        if let Ok(s) = std::str::from_utf8(&buf) {
            let _ = tx.try_send(LogChunk { line: s.to_string() });
        }
    }
    if let Some(c) = ctx { info!(ctx = %c, "log pump ended"); } else { info!("log pump ended"); }
}

fn parse_gvk_key(key: &str) -> Result<(String, String, String)> {
    let parts: Vec<_> = key.split('/').collect();
    match parts.as_slice() {
        [version, kind] => Ok((String::new(), (*version).to_string(), (*kind).to_string())),
        [group, version, kind] => Ok(((*group).to_string(), (*version).to_string(), (*kind).to_string())),
        _ => Err(anyhow!("invalid gvk key: {} (expect v1/Kind or group/v1/Kind)", key)),
    }
}

async fn find_api_resource(client: Client, gvk: &kube::core::GroupVersionKind) -> Result<(kube::core::ApiResource, bool)> {
    use kube::discovery::{Discovery, Scope};
    let discovery = Discovery::new(client).run().await?;
    for group in discovery.groups() {
        for (ar, caps) in group.recommended_resources() {
            if ar.group == gvk.group && ar.version == gvk.version && ar.kind == gvk.kind {
                let namespaced = matches!(caps.scope, Scope::Namespaced);
                return Ok((ar.clone(), namespaced));
            }
        }
    }
    Err(anyhow!("GVK not found: {}/{}/{}", gvk.group, gvk.version, gvk.kind))
}

#[cfg(test)]
mod tests {
    use super::*;
    use futures::stream;

    #[tokio::test]
    async fn splits_lines_across_chunks_and_flushes_tail() {
        let (tx, mut rx) = mpsc::channel::<LogChunk>(16);
        let (_cancel_tx, cancel_rx) = oneshot::channel::<()>();
        let chunks = vec![
            Ok::<bytes::Bytes, std::io::Error>(bytes::Bytes::from_static(b"hello\nwor")),
            Ok::<bytes::Bytes, std::io::Error>(bytes::Bytes::from_static(b"ld\n")),
            Ok::<bytes::Bytes, std::io::Error>(bytes::Bytes::from_static(b"tail")),
        ];
        let s = stream::iter(chunks);
        pump_bytes_to_lines(s, tx, cancel_rx, Some("test")).await;
        let mut out = Vec::new();
        while let Some(c) = rx.recv().await { out.push(c.line); }
        assert_eq!(out, vec!["hello", "world", "tail"]);
    }

    #[tokio::test]
    async fn bounded_channel_drops_when_full() {
        let (tx, mut rx) = mpsc::channel::<LogChunk>(1);
        let (_cancel_tx, cancel_rx) = oneshot::channel::<()>();
        let lines = vec![
            Ok::<bytes::Bytes, std::io::Error>(bytes::Bytes::from_static(b"a\n")),
            Ok::<bytes::Bytes, std::io::Error>(bytes::Bytes::from_static(b"b\n")),
            Ok::<bytes::Bytes, std::io::Error>(bytes::Bytes::from_static(b"c\n")),
        ];
        let s = stream::iter(lines);
        pump_bytes_to_lines(s, tx, cancel_rx, Some("drop-test")).await;
        // We expect at least 1 line (the first), subsequent may be dropped due to full channel
        let mut recv = Vec::new();
        while let Ok(Some(c)) = tokio::time::timeout(std::time::Duration::from_millis(50), rx.recv()).await { recv.push(c.line); }
        assert!(!recv.is_empty());
        assert!(recv.len() <= 2, "expected dropping when full (got {} lines)", recv.len());
    }

    #[tokio::test]
    async fn cancel_stops_pump_quickly() {
        let (tx, mut rx) = mpsc::channel::<LogChunk>(16);
        let (cancel_tx, cancel_rx) = oneshot::channel::<()>();
        // Slow stream: yields one big chunk after delay, then loops
        let s = async_stream::stream! {
            loop {
                tokio::time::sleep(std::time::Duration::from_millis(100)).await;
                yield Ok::<bytes::Bytes, std::io::Error>(bytes::Bytes::from_static(b"line\n"));
            }
        };
        let handle = tokio::spawn(pump_bytes_to_lines(s, tx, cancel_rx, Some("cancel-test")));
        // Cancel shortly after
        tokio::time::sleep(std::time::Duration::from_millis(120)).await;
        let _ = cancel_tx.send(());
        // Join with timeout to ensure it exits
        let _ = tokio::time::timeout(std::time::Duration::from_secs(1), handle).await.expect("pump did not stop");
        // Drain anything left
        let _ = tokio::time::timeout(std::time::Duration::from_millis(50), rx.recv()).await;
    }

    #[test]
    fn parse_gvk_key_parses_core() {
        let (g, v, k) = parse_gvk_key("v1/ConfigMap").expect("ok");
        assert_eq!(g, "");
        assert_eq!(v, "v1");
        assert_eq!(k, "ConfigMap");
    }

    #[test]
    fn parse_gvk_key_parses_group() {
        let (g, v, k) = parse_gvk_key("apps/v1/Deployment").expect("ok");
        assert_eq!(g, "apps");
        assert_eq!(v, "v1");
        assert_eq!(k, "Deployment");
    }

    #[test]
    fn parse_gvk_key_invalid_returns_err() {
        assert!(parse_gvk_key("invalid").is_err());
        assert!(parse_gvk_key("").is_err());
        assert!(parse_gvk_key("a/b/c/d").is_err());
    }
}
</file>

<file path="crates/ops/Cargo.toml">
[package]
name = "orka_ops"
version = "0.0.0"
edition = "2021"
license = "MIT OR Apache-2.0"
description = "Orka Ops: imperative Kubernetes operations (Milestone OPS 3.1)"

[dependencies]
anyhow = { workspace = true }
tracing = { workspace = true }
tokio = { workspace = true }
futures = { workspace = true }
serde = { workspace = true, features = ["derive"] }
serde_json = { workspace = true }
kube = { workspace = true }
k8s-openapi = { workspace = true }
async-trait = "0.1"
bytes = "1"
tokio-util = { version = "0.7", features = ["io", "compat"] }
crossterm = "0.27"
chrono = { workspace = true }

[dev-dependencies]
async-stream = "0.3"

[features]
default = []
</file>

<file path="crates/persist/src/lib.rs">
//! Orka persistence (Milestone 2): minimal SQLite store for last-applied.
//! Keep code tiny and predictable.

#![forbid(unsafe_code)]

use anyhow::{Context, Result};
use metrics::{counter, histogram};
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LastApplied {
    pub uid: [u8; 16],
    pub rv: String,
    pub ts: i64,
    pub yaml_zstd: Vec<u8>,
}

pub trait Store {
    fn put_last(&self, la: LastApplied) -> Result<()>;
    fn get_last(&self, uid: [u8; 16], limit: Option<usize>) -> Result<Vec<LastApplied>>;
}

/// SQLite-backed store. Simple, synchronous. The CLI isn’t latency sensitive here.
pub struct SqliteStore {
    db: std::sync::Mutex<rusqlite::Connection>,
}

impl SqliteStore {
    pub fn open_default() -> Result<Self> {
        let path = std::env::var("ORKA_DB_PATH").unwrap_or_else(|_| default_db_path());
        Self::open(&path)
    }

    pub fn open(path: &str) -> Result<Self> {
        let started = std::time::Instant::now();
        let db = rusqlite::Connection::open(path).with_context(|| format!("opening sqlite db at {}", path))?;
        db.pragma_update(None, "journal_mode", &"WAL").ok();
        db.pragma_update(None, "synchronous", &"NORMAL").ok();
        db.execute(
            "CREATE TABLE IF NOT EXISTS last_applied (
                uid  BLOB NOT NULL,
                rv   TEXT NOT NULL,
                ts   INTEGER NOT NULL,
                yaml BLOB NOT NULL
            )",
            [],
        ).context("creating last_applied table")?;
        db.execute(
            "CREATE INDEX IF NOT EXISTS idx_last_applied_uid_ts ON last_applied(uid, ts DESC)",
            [],
        ).ok();
        let me = Self { db: std::sync::Mutex::new(db) };
        histogram!("persist_open_ms", started.elapsed().as_secs_f64() * 1000.0);
        Ok(me)
    }
}

impl Store for SqliteStore {
    fn put_last(&self, la: LastApplied) -> Result<()> {
        let started = std::time::Instant::now();
        let mut db = self.db.lock().unwrap();
        let tx = db.transaction()?;
        tx.execute(
            "INSERT INTO last_applied(uid, rv, ts, yaml) VALUES (?1, ?2, ?3, ?4)",
            (
                &la.uid[..],
                &la.rv,
                la.ts,
                &la.yaml_zstd,
            ),
        )?;
        // Keep latest 3 by ts per uid (delete older rows by rowid)
        tx.execute(
            "DELETE FROM last_applied
             WHERE uid = ?1
               AND rowid NOT IN (
                   SELECT rowid FROM last_applied WHERE uid = ?1 ORDER BY ts DESC, rowid DESC LIMIT 3
               )",
            [&la.uid[..]],
        )?;
        tx.commit()?;
        histogram!("persist_put_ms", started.elapsed().as_secs_f64() * 1000.0);
        counter!("persist_put_total", 1u64);
        Ok(())
    }

    fn get_last(&self, uid: [u8; 16], limit: Option<usize>) -> Result<Vec<LastApplied>> {
        let started = std::time::Instant::now();
        let cap = limit.unwrap_or(3);
        let db = self.db.lock().unwrap();
        let mut stmt = db.prepare(
            "SELECT rv, ts, yaml FROM last_applied WHERE uid = ?1 ORDER BY ts DESC, rowid DESC LIMIT ?2",
        )?;
        let mut rows = stmt.query((uid.as_slice(), cap as i64))?;
        let mut out: Vec<LastApplied> = Vec::new();
        while let Some(row) = rows.next()? {
            let rv: String = row.get(0)?;
            let ts: i64 = row.get(1)?;
            let yaml: Vec<u8> = row.get(2)?;
            out.push(LastApplied { uid, rv, ts, yaml_zstd: yaml });
        }
        histogram!("persist_get_ms", started.elapsed().as_secs_f64() * 1000.0);
        Ok(out)
    }
}

fn default_db_path() -> String {
    if let Some(home) = std::env::var_os("HOME") {
        let mut p = std::path::PathBuf::from(home);
        p.push(".orka");
        let _ = std::fs::create_dir_all(&p);
        p.push("orka.db");
        return p.to_string_lossy().to_string();
    }
    // Fallback to current directory
    "orka.db".to_string()
}

pub fn now_ts() -> i64 {
    // seconds since epoch
    let now = std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap_or_default();
    now.as_secs() as i64
}

pub fn maybe_compress(yaml: &str) -> Vec<u8> {
    #[cfg(feature = "zstd")]
    {
        let lvl: i32 = std::env::var("ORKA_ZSTD_LEVEL").ok().and_then(|s| s.parse().ok()).unwrap_or(3);
        return zstd::encode_all(yaml.as_bytes(), lvl as i32).unwrap_or_else(|_| yaml.as_bytes().to_vec());
    }
    yaml.as_bytes().to_vec()
}

pub fn maybe_decompress(blob: &[u8]) -> String {
    #[cfg(feature = "zstd")]
    {
        if let Ok(de) = zstd::decode_all(std::io::Cursor::new(blob)) {
            return String::from_utf8_lossy(&de).to_string();
        }
    }
    String::from_utf8_lossy(blob).to_string()
}

#[cfg(test)]
mod tests {
    use super::*;

    fn temp_db() -> String {
        let dir = std::env::temp_dir();
        let f = format!("orka-test-{}.db", std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_nanos());
        dir.join(f).to_string_lossy().to_string()
    }

    #[test]
    fn put_get_rotate() {
        let path = temp_db();
        let s = SqliteStore::open(&path).unwrap();
        let uid = [7u8; 16];
        for i in 0..5 {
            let la = LastApplied { uid, rv: format!("rv-{}", i), ts: i as i64, yaml_zstd: maybe_compress(&format!("k: v{}\n", i)) };
            s.put_last(la).unwrap();
        }
        let rows = s.get_last(uid, None).unwrap();
        assert_eq!(rows.len(), 3);
        assert_eq!(rows[0].rv, "rv-4");
        assert_eq!(rows[1].rv, "rv-3");
        assert_eq!(rows[2].rv, "rv-2");
    }
}
</file>

<file path="crates/persist/Cargo.toml">
[package]
name = "orka-persist"
version = "0.1.0"
edition = "2021"

[dependencies]
anyhow = { workspace = true }
serde = { workspace = true, features = ["derive"] }
tracing = { workspace = true }
metrics = { workspace = true }
rusqlite = { version = "0.31", features = ["bundled"] }
time = "0.3"

[features]
default = []
zstd = ["dep:zstd"]

[dependencies.zstd]
version = "0.13"
optional = true
</file>

<file path="crates/search/examples/bench.rs">
use orka_core::{LiteObj, WorldSnapshot, Uid};
use orka_search::{Index, SearchOpts};
use std::time::{Duration, Instant};

fn uid(n: u64) -> Uid {
    let mut u = [0u8; 16];
    u[0..8].copy_from_slice(&n.to_le_bytes());
    u
}

fn gen_obj(i: usize) -> LiteObj {
    let ns_idx = i % 10;
    let team_idx = i % 20;
    let app = match i % 3 { 0 => "web", 1 => "api", _ => "batch" };
    let name = format!("obj-{i:06}");
    let ns = format!("ns{ns_idx}");
    let proj1 = format!("v{}", i % 1000);
    let proj2 = format!("zone-{}", i % 20);
    LiteObj {
        uid: uid(i as u64),
        namespace: Some(ns),
        name,
        creation_ts: 1_577_836_800, // 2020-01-01
        projected: smallvec::smallvec![(1u32, proj1), (2u32, proj2)],
        labels: smallvec::smallvec![("app".to_string(), app.to_string()), (format!("team{team_idx}"), "1".to_string())],
        annotations: smallvec::SmallVec::new(),
    }
}

fn gen_snapshot(n: usize) -> WorldSnapshot {
    let mut items = Vec::with_capacity(n);
    for i in 0..n { items.push(gen_obj(i)); }
    WorldSnapshot { epoch: 1, items }
}

fn percentile_us(xs: &mut [u128], p: f64) -> u128 {
    xs.sort_unstable();
    let idx = ((xs.len() as f64 - 1.0) * p).round() as usize;
    xs[idx]
}

fn approx_index_bytes(idx: &Index) -> usize {
    // Crude: sum lengths of stored strings and postings counts
    // NOTE: This intentionally avoids private fields; put approximation here as a placeholder.
    // We rebuild a rough snapshot-based estimate instead.
    0 // Placeholder: private fields are not accessible; rely on snapshot-based estimate below.
}

fn main() {
    let n: usize = std::env::var("ORKA_BENCH_DOCS").ok().and_then(|s| s.parse().ok()).unwrap_or(100_000);
    let limit: usize = std::env::var("ORKA_BENCH_LIMIT").ok().and_then(|s| s.parse().ok()).unwrap_or(50);
    let max_candidates: usize = std::env::var("ORKA_BENCH_MAXC").ok().and_then(|s| s.parse().ok()).unwrap_or(10_000);
    let min_score: Option<f32> = std::env::var("ORKA_BENCH_MINSCORE").ok().and_then(|s| s.parse().ok());

    eprintln!("building snapshot: {} docs", n);
    let t0 = Instant::now();
    let snap = gen_snapshot(n);
    let build_snap_ms = t0.elapsed().as_secs_f64() * 1_000.0;

    eprintln!("building index...");
    let t1 = Instant::now();
    let index = Index::build_from_snapshot_with_meta(&snap, Some(&[("spec.unit".to_string(), 1u32)]), Some("Demo"), Some("demo.example.com"));
    let build_idx_ms = t1.elapsed().as_secs_f64() * 1_000.0;

    // Prepare queries
    let mut typed_only: Vec<String> = Vec::new();
    for ns in 0..10 { typed_only.push(format!("ns:ns{} label:app=web", ns)); }
    let mut typed_fuzzy: Vec<String> = Vec::new();
    for step in (0..n).step_by(n.saturating_div(200).max(1)) {
        typed_fuzzy.push(format!("ns:ns{} obj-{:06}", step % 10, step));
    }
    // Field filters (projected)
    let mut field_queries: Vec<String> = Vec::new();
    for v in 0..20 { field_queries.push(format!("field:spec.unit=v{}", v)); }

    let opts = SearchOpts { max_candidates: Some(max_candidates), min_score };

    // Run and time searches
    let mut run = |label: &str, qs: &[String]| {
        let mut times: Vec<u128> = Vec::with_capacity(qs.len());
        for q in qs { let t = Instant::now(); let _ = index.search_with_debug_opts(q, limit, opts).0; times.push(t.elapsed().as_micros()); }
        let p50 = percentile_us(&mut times.clone(), 0.50) as f64 / 1000.0;
        let p99 = percentile_us(&mut times, 0.99) as f64 / 1000.0;
        println!("{}: p50={:.3}ms p99={:.3}ms ({} queries, limit={})", label, p50, p99, qs.len(), limit);
    };

    println!("index_build: snapshot={:.1}ms index={:.1}ms docs={}", build_snap_ms, build_idx_ms, n);
    run("typed_only", &typed_only);
    run("typed+fuzzy", &typed_fuzzy);
    run("field", &field_queries);
}
</file>

<file path="crates/search/tests/multigvk_determinism.rs">
#![forbid(unsafe_code)]

use orka_core::{Delta, DeltaKind, Uid, WorldSnapshot};
use orka_store::spawn_ingest;
use orka_search::Index;

fn uid(n: u8) -> Uid { let mut u = [0u8; 16]; u[0] = n; u }

fn obj_raw(name: &str, ns: &str, ts: &str) -> serde_json::Value {
    serde_json::json!({
        "apiVersion": "v1",
        "kind": "ConfigMap",
        "metadata": {
            "name": name,
            "namespace": ns,
            "uid": format!("00000000-0000-0000-0000-{:012}", 1),
            "creationTimestamp": ts,
            // resourceVersion/jitter stripped in builder; safe to omit here
        }
    })
}

fn obj_raw_cert(name: &str, ns: &str, ts: &str) -> serde_json::Value {
    serde_json::json!({
        "apiVersion": "cert-manager.io/v1",
        "kind": "Certificate",
        "metadata": {
            "name": name,
            "namespace": ns,
            "uid": format!("00000000-0000-0000-0000-{:012}", 2),
            "creationTimestamp": ts,
        }
    })
}

async fn run_stream(seq: &[Delta]) -> WorldSnapshot {
    let (tx, backend) = spawn_ingest(128);
    for d in seq.iter().cloned() { let _ = tx.send(d).await; }
    drop(tx);
    // Allow ingest loop to flush
    tokio::time::sleep(std::time::Duration::from_millis(30)).await;
    (*backend.current()).clone()
}

fn canonicalize_hits(world: &WorldSnapshot, hits: &Vec<orka_search::Hit>) -> Vec<(String, String, [u8;16])> {
    hits.iter().map(|h| {
        let o = &world.items[h.doc as usize];
        (o.namespace.clone().unwrap_or_default(), o.name.clone(), o.uid)
    }).collect()
}

#[tokio::test(flavor = "multi_thread", worker_threads = 2)]
async fn multigvk_search_topk_deterministic_across_runs() {
    // Exercise sharding
    let prev = std::env::var_os("ORKA_SHARDS");
    std::env::set_var("ORKA_SHARDS", "3");

    // Stream A (ConfigMaps)
    let a_seq = vec![
        Delta { uid: uid(1), kind: DeltaKind::Applied, raw: obj_raw("cm-a", "ns1", "2020-01-01T00:00:00Z") },
        Delta { uid: uid(2), kind: DeltaKind::Applied, raw: obj_raw("cm-b", "ns2", "2020-01-01T00:00:01Z") },
        Delta { uid: uid(1), kind: DeltaKind::Applied, raw: obj_raw("cm-a2","ns1","2020-01-01T00:00:02Z") },
    ];
    // Stream B (Certificates)
    let b_seq = vec![
        Delta { uid: uid(10), kind: DeltaKind::Applied, raw: obj_raw_cert("cert-x", "prod", "2020-01-01T00:00:10Z") },
        Delta { uid: uid(11), kind: DeltaKind::Applied, raw: obj_raw_cert("cert-y", "prod", "2020-01-01T00:00:11Z") },
        Delta { uid: uid(11), kind: DeltaKind::Deleted, raw: serde_json::json!({}) },
    ];

    // First run
    let a1 = run_stream(&a_seq).await;
    let b1 = run_stream(&b_seq).await;
    let mut world1 = WorldSnapshot { epoch: 1, items: { let mut v = a1.items.clone(); v.extend(b1.items.clone()); v } };
    // Build index over combined world and query
    let idx1 = Index::build_from_snapshot(&world1);
    let (hits1, _dbg1) = idx1.search_with_debug("ns:ns1", 10);
    let canon1 = canonicalize_hits(&world1, &hits1);

    // Second run
    let a2 = run_stream(&a_seq).await;
    let b2 = run_stream(&b_seq).await;
    let mut world2 = WorldSnapshot { epoch: 1, items: { let mut v = a2.items.clone(); v.extend(b2.items.clone()); v } };
    let idx2 = Index::build_from_snapshot(&world2);
    let (hits2, _dbg2) = idx2.search_with_debug("ns:ns1", 10);
    let canon2 = canonicalize_hits(&world2, &hits2);

    assert_eq!(canon1, canon2, "top-k multi-GVK search results must be deterministic across runs");

    // Restore env var
    match prev { Some(v) => std::env::set_var("ORKA_SHARDS", v), None => std::env::remove_var("ORKA_SHARDS") };
}
</file>

<file path="crates/search/tests/replay.rs">
use orka_core::{Delta, DeltaKind, Uid};
use orka_search::Index;

fn uid(n: u8) -> Uid { let mut u = [0u8; 16]; u[0] = n; u }

fn obj_raw(name: &str, ns: &str, labels: &[(&str, &str)]) -> serde_json::Value {
    let mut meta = serde_json::json!({
        "name": name,
        "namespace": ns,
        "creationTimestamp": "2020-01-01T00:00:00Z",
    });
    if !labels.is_empty() {
        let mut map = serde_json::Map::new();
        for (k, v) in labels.iter() { map.insert((*k).to_string(), serde_json::Value::String((*v).to_string())); }
        meta["labels"] = serde_json::Value::Object(map);
    }
    serde_json::json!({ "metadata": meta })
}

#[test]
fn replay_deltas_produce_stable_index_and_ordering() {
    // Build a world by replaying deltas (apply updates and a delete)
    let mut wb = orka_store::WorldBuilder::new();
    let u1 = uid(1);
    let u2 = uid(2);
    let u3 = uid(3);
    // initial apply for three objects
    wb.apply(vec![
        Delta { uid: u1, kind: DeltaKind::Applied, raw: obj_raw("alpha", "default", &[("app","web")]) },
        Delta { uid: u2, kind: DeltaKind::Applied, raw: obj_raw("beta",  "default", &[("app","api")]) },
        Delta { uid: u3, kind: DeltaKind::Applied, raw: obj_raw("gamma", "default", &[]) },
    ]);
    // rename beta -> alpha (tests tie-break by uid when names equal)
    let mut o2 = obj_raw("alpha", "default", &[("app","api")]);
    wb.apply(vec![Delta { uid: u2, kind: DeltaKind::Applied, raw: o2 }]);
    // delete gamma
    wb.apply(vec![Delta { uid: u3, kind: DeltaKind::Deleted, raw: serde_json::json!({}) }]);

    let snap = wb.freeze();
    // Build index and search; with no free text it sorts by name then uid
    let idx = Index::build_from_snapshot(&snap);
    let hits = idx.search("ns:default", 10);
    // Two items remain, both named "alpha"; ordering should be by uid asc (1 before 2)
    assert_eq!(hits.len(), 2);
    let d0 = hits[0].doc as usize; let d1 = hits[1].doc as usize;
    assert_eq!(snap.items[d0].name, "alpha");
    assert_eq!(snap.items[d1].name, "alpha");
    assert_eq!(snap.items[d0].uid[0], 1);
    assert_eq!(snap.items[d1].uid[0], 2);

    // Typed filter via label should isolate u1 (web)
    let hits_label = idx.search("label:app=web", 10);
    assert_eq!(hits_label.len(), 1);
    let d = hits_label[0].doc as usize;
    assert_eq!(snap.items[d].uid[0], 1);
}
</file>

<file path="crates/search/tests/shards.rs">
use orka_core::{LiteObj, WorldSnapshot, Uid};
use orka_search::Index;

fn uid(n: u8) -> Uid { let mut u = [0u8; 16]; u[0] = n; u }

fn obj(id: u8, name: &str, ns: Option<&str>) -> LiteObj {
    LiteObj {
        uid: uid(id),
        namespace: ns.map(|s| s.to_string()),
        name: name.to_string(),
        creation_ts: 0,
        projected: smallvec::SmallVec::new(),
        labels: smallvec::SmallVec::new(),
        annotations: smallvec::SmallVec::new(),
    }
}

fn snap(items: Vec<LiteObj>) -> WorldSnapshot { WorldSnapshot { epoch: 1, items } }

#[test]
fn sharded_index_preserves_global_doc_ids_and_ordering() {
    // Force sharding to 2 buckets to exercise cross-shard search
    let prev = std::env::var_os("ORKA_SHARDS");
    std::env::set_var("ORKA_SHARDS", "2");

    let s = snap(vec![
        obj(1, "alpha", Some("default")),
        obj(2, "alpha", Some("prod")),
        obj(3, "beta", Some("tools")),
    ]);
    let idx = Index::build_from_snapshot(&s);
    let (hits, _dbg) = idx.search_with_debug("", 10);
    assert_eq!(hits.len(), 3);

    // Ordering should be stable across shards: by name asc, then uid asc
    let mut names_uids: Vec<(String, u8)> = hits
        .iter()
        .map(|h| (s.items[h.doc as usize].name.clone(), s.items[h.doc as usize].uid[0]))
        .collect();
    // Expect: alpha(uid 1), alpha(uid 2), beta(uid 3)
    assert_eq!(names_uids.remove(0), ("alpha".to_string(), 1));
    assert_eq!(names_uids.remove(0), ("alpha".to_string(), 2));
    assert_eq!(names_uids.remove(0), ("beta".to_string(), 3));

    // Restore env
    match prev {
        Some(v) => std::env::set_var("ORKA_SHARDS", v),
        None => std::env::remove_var("ORKA_SHARDS"),
    }
}
</file>

<file path="crates/store/tests/multigvk_replay.rs">
#![forbid(unsafe_code)]

use orka_store::spawn_ingest;
use orka_core::{Delta, DeltaKind};

fn uid(n: u8) -> [u8; 16] { let mut u = [0u8; 16]; u[0] = n; u }

fn obj(name: &str, ns: Option<&str>, ts: &str) -> serde_json::Value {
    let mut meta = serde_json::json!({
        "name": name,
        "uid": format!("00000000-0000-0000-0000-{:012}", 1),
        "creationTimestamp": ts,
    });
    if let Some(ns) = ns { meta["namespace"] = serde_json::Value::String(ns.to_string()); }
    serde_json::json!({ "metadata": meta })
}

async fn run_stream(seq: &[Delta]) -> Vec<(String, String, u8)> {
    let (tx, backend) = spawn_ingest(128);
    for d in seq.iter().cloned() { let _ = tx.send(d).await; }
    drop(tx);
    // Allow flush
    tokio::time::sleep(std::time::Duration::from_millis(20)).await;
    let snap = backend.current();
    snap.items.iter().map(|o| (o.namespace.clone().unwrap_or_default(), o.name.clone(), o.uid[0])).collect()
}

fn compose(mut a: Vec<(String, String, u8)>, mut b: Vec<(String, String, u8)>) -> Vec<(String, String, u8)> {
    let mut all = Vec::new();
    all.append(&mut a);
    all.append(&mut b);
    all.sort_unstable();
    all
}

#[tokio::test(flavor = "multi_thread", worker_threads = 2)]
async fn scaffold_multigvk_determinism() {
    // This test simulates two independent GVK streams (A and B) and asserts that
    // composing their snapshots is deterministic across runs. Real multi-GVK
    // world composition would happen in a higher-level container.
    let prev = std::env::var_os("ORKA_SHARDS");
    std::env::set_var("ORKA_SHARDS", "3");

    // Stream A (e.g., ConfigMaps)
    let a_seq = vec![
        Delta { uid: uid(1), kind: DeltaKind::Applied, raw: obj("cm-a", Some("ns1"), "2020-01-01T00:00:00Z") },
        Delta { uid: uid(2), kind: DeltaKind::Applied, raw: obj("cm-b", Some("ns2"), "2020-01-01T00:00:01Z") },
        Delta { uid: uid(1), kind: DeltaKind::Applied, raw: obj("cm-a2", Some("ns1"), "2020-01-01T00:00:02Z") },
    ];
    // Stream B (e.g., Certificates)
    let b_seq = vec![
        Delta { uid: uid(10), kind: DeltaKind::Applied, raw: obj("cert-x", Some("prod"), "2020-01-01T00:00:10Z") },
        Delta { uid: uid(11), kind: DeltaKind::Applied, raw: obj("cert-y", Some("prod"), "2020-01-01T00:00:11Z") },
        Delta { uid: uid(11), kind: DeltaKind::Deleted, raw: serde_json::json!({}) },
    ];

    let a1 = run_stream(&a_seq).await; let b1 = run_stream(&b_seq).await;
    let comp1 = compose(a1, b1);
    let a2 = run_stream(&a_seq).await; let b2 = run_stream(&b_seq).await;
    let comp2 = compose(a2, b2);
    assert_eq!(comp1, comp2, "composed multi-GVK snapshot must be deterministic across runs");

    match prev { Some(v) => std::env::set_var("ORKA_SHARDS", v), None => std::env::remove_var("ORKA_SHARDS") };
}
</file>

<file path="crates/store/tests/replay.rs">
#![forbid(unsafe_code)]

use orka_store::WorldBuilder;
use orka_core::{Delta, DeltaKind};

fn uid(n: u8) -> [u8; 16] { let mut u = [0u8; 16]; u[0] = n; u }

fn obj(name: &str, ns: Option<&str>, ts: &str) -> serde_json::Value {
    let mut meta = serde_json::json!({
        "name": name,
        "uid": format!("00000000-0000-0000-0000-{:012}", 1),
        "creationTimestamp": ts,
    });
    if let Some(ns) = ns { meta["namespace"] = serde_json::Value::String(ns.to_string()); }
    serde_json::json!({ "metadata": meta })
}

#[test]
fn replay_basic_sequence() {
    let mut wb = WorldBuilder::new();

    // Simulate a stream of deltas (applied, updated, deleted)
    let deltas = vec![
        // add a/ns
        Delta { uid: uid(1), kind: DeltaKind::Applied, raw: obj("a", Some("ns"), "2020-01-01T00:00:00Z") },
        // duplicate add should coalesce at queue normally; here builder just replaces
        Delta { uid: uid(1), kind: DeltaKind::Applied, raw: obj("a", Some("ns"), "2020-01-01T00:00:00Z") },
        // add b cluster-scoped
        Delta { uid: uid(2), kind: DeltaKind::Applied, raw: obj("b", None, "2020-01-01T00:00:01Z") },
        // update a -> a2
        Delta { uid: uid(1), kind: DeltaKind::Applied, raw: obj("a2", Some("ns"), "2020-01-01T00:00:00Z") },
        // delete b
        Delta { uid: uid(2), kind: DeltaKind::Deleted, raw: serde_json::json!({}) },
    ];

    // Apply in two batches like ingest would
    wb.apply(deltas[..2].to_vec());
    let snap1 = wb.freeze();
    assert_eq!(snap1.epoch, 1);
    assert_eq!(snap1.items.len(), 1);
    assert_eq!(snap1.items[0].name, "a");

    wb.apply(deltas[2..].to_vec());
    let snap2 = wb.freeze();
    assert_eq!(snap2.epoch, 2);
    assert_eq!(snap2.items.len(), 1);
    assert_eq!(snap2.items[0].name, "a2");
    assert_eq!(snap2.items[0].namespace.as_deref(), Some("ns"));
}
</file>

<file path="crates/store/tests/sharded_determinism.rs">
#![forbid(unsafe_code)]

use orka_store::spawn_ingest;
use orka_core::{Delta, DeltaKind};

fn uid(n: u8) -> [u8; 16] { let mut u = [0u8; 16]; u[0] = n; u }

fn obj(name: &str, ns: Option<&str>, ts: &str) -> serde_json::Value {
    let mut meta = serde_json::json!({
        "name": name,
        "uid": format!("00000000-0000-0000-0000-{:012}", 1),
        "creationTimestamp": ts,
    });
    if let Some(ns) = ns { meta["namespace"] = serde_json::Value::String(ns.to_string()); }
    serde_json::json!({ "metadata": meta })
}

async fn run_sequence(seq: &[Delta]) -> Vec<(String, String, u8)> {
    let (tx, backend) = spawn_ingest(128);
    // Send all deltas (out-of-order and duplicates allowed)
    for d in seq.iter().cloned() { let _ = tx.send(d).await; }
    drop(tx);
    // Let ingest flush final snapshot
    tokio::time::sleep(std::time::Duration::from_millis(30)).await;
    let snap = backend.current();
    let mut canon: Vec<(String, String, u8)> = snap.items.iter().map(|o| {
        (o.namespace.clone().unwrap_or_else(|| "".into()), o.name.clone(), o.uid[0])
    }).collect();
    canon.sort_unstable();
    canon
}

#[tokio::test(flavor = "multi_thread", worker_threads = 2)]
async fn deterministic_across_runs_with_shards() {
    // Force multiple shards
    let prev = std::env::var_os("ORKA_SHARDS");
    std::env::set_var("ORKA_SHARDS", "4");

    let seq = vec![
        // initial adds across namespaces
        Delta { uid: uid(1), kind: DeltaKind::Applied, raw: obj("a", Some("ns1"), "2020-01-01T00:00:00Z") },
        Delta { uid: uid(2), kind: DeltaKind::Applied, raw: obj("b", Some("ns2"), "2020-01-01T00:00:01Z") },
        Delta { uid: uid(3), kind: DeltaKind::Applied, raw: obj("c", Some("ns3"), "2020-01-01T00:00:02Z") },
        // out-of-order update and duplicate events
        Delta { uid: uid(2), kind: DeltaKind::Applied, raw: obj("bb", Some("ns2"), "2020-01-01T00:00:01Z") },
        Delta { uid: uid(2), kind: DeltaKind::Applied, raw: obj("bb", Some("ns2"), "2020-01-01T00:00:01Z") },
        // delete one
        Delta { uid: uid(3), kind: DeltaKind::Deleted, raw: serde_json::json!({}) },
        // add another in a different ns bucket
        Delta { uid: uid(4), kind: DeltaKind::Applied, raw: obj("d", Some("prod"), "2020-01-01T00:00:03Z") },
    ];

    let c1 = run_sequence(&seq).await;
    let c2 = run_sequence(&seq).await;
    assert_eq!(c1, c2, "canonical snapshot view must be deterministic across runs");

    // Restore env
    match prev { Some(v) => std::env::set_var("ORKA_SHARDS", v), None => std::env::remove_var("ORKA_SHARDS") };
}
</file>

<file path="docs/imperative_ops.md">
# Imperative Ops (Milestone OPS 3.1)

This page documents the initial, backend-first implementation of imperative Kubernetes operations in Orka.

Status: implemented: `logs`, `exec` (PTY + resize), `pf` (single port), `scale`, `rollout-restart`, `delete`, `cordon`, `drain`. Also includes a capabilities probe (`ops caps`).

## Crate

- `orka_ops`: reusable library providing an `OrkaOps` trait and a default `KubeOps` implementation backed by kube-rs.

## CLI Usage

- Stream pod logs (human readable by default; add `-o json` for JSON lines):

  - Follow and print continuously (Ctrl-C to stop):
    `orkactl --ns default ops logs my-pod`

  - Specific container, tail 100 lines:
    `orkactl --ns default ops logs my-pod -c app --tail 100`

  - Multiple containers and regex filter:
    `orkactl --ns default ops logs my-pod -c app -c sidecar --grep 'ERROR|WARN'`

  - Show logs from the last 60 seconds:
    `orkactl --ns default ops logs my-pod --since 60`

## Exec and Port‑Forward

- Exec:
  - Non‑TTY: `orkactl --ns default ops exec my-pod -- sh -c 'echo hello'`
  - TTY interactive: `orkactl --ns default ops exec my-pod --tty -- sh`
  - Container selection: `-c app`
  - Notes: TTY uses raw mode; window resize (SIGWINCH) updates are sent to the pod.
  - Ctrl‑C closes the remote exec cleanly.

- Port-forward:
  - Same local/remote: `orkactl --ns default ops pf my-pod 8080`
  - Map local 9999 to remote 80: `orkactl --ns default ops pf my-pod 9999:80`
  - Events print in human or JSON (`-o json`). Ctrl‑C stops.

## Scale and Rollout Restart

- Scale a Deployment to 5 replicas (uses Scale subresource when available; falls back to patching `.spec.replicas`):

  `orkactl --ns default ops scale apps/v1/Deployment my-dep 5`

- Rollout restart a Deployment:

  `orkactl --ns default ops rr apps/v1/Deployment my-dep`

Both support JSON output with `-o json`.

## Delete, Cordon, Drain

- Delete a pod (grace 0):

  `orkactl --ns default ops delete my-pod --grace 0`

- Cordon a node; uncordon with `--off`:

  `orkactl ops cordon my-node`

- Drain a node (best‑effort evictions; respects PDBs):

  `orkactl ops drain my-node`

All support JSON output with `-o json`.

## Capabilities

- Probe RBAC/subresource capabilities for the current user:

  `orkactl --ns default ops caps`

- Include Scale checks for a specific GVK:

  `orkactl --ns default ops caps --gvk apps/v1/Deployment`

## Notes

- Streaming uses a bounded channel (cap: `ORKA_OPS_QUEUE_CAP`, default 1024). If the consumer can’t keep up, new chunks may be dropped to avoid unbounded memory growth.
- Additional ops will be implemented incrementally.
</file>

<file path="examples/configmap.yaml">
apiVersion: v1
kind: ConfigMap
metadata:
  name: orka-example-config
  namespace: default
  labels:
    app: orka-example
data:
  message: "hello, orka"
  version: "2"
</file>

<file path="scripts/kind-nightly.sh">
#!/usr/bin/env bash
set -euo pipefail

# Nightly kind integration (M3): sets up a local cluster and installs a minimal
# set of operators to exercise Orka end-to-end. This script is intended to be
# run manually or as a scheduled job; CI should skip it by default.

echo "[kind-nightly] starting"

if ! command -v kind >/dev/null 2>&1; then
  echo "kind not found; please install kind (https://kind.sigs.k8s.io/)" >&2
  exit 1
fi

CLUSTER_NAME=${CLUSTER_NAME:-orka-nightly}

if ! kind get clusters | grep -q "^${CLUSTER_NAME}$"; then
  echo "[kind-nightly] creating cluster ${CLUSTER_NAME}"
  kind create cluster --name "${CLUSTER_NAME}"
else
  echo "[kind-nightly] reusing cluster ${CLUSTER_NAME}"
fi

echo "[kind-nightly] installing operators (cert-manager, kube-prometheus-stack)"
echo "Note: requires kubectl and helm with network access."
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.5/cert-manager.crds.yaml || true
kubectl create namespace cert-manager 2>/dev/null || true
helm repo add jetstack https://charts.jetstack.io 1>/dev/null
helm repo update 1>/dev/null
helm upgrade --install cert-manager jetstack/cert-manager --namespace cert-manager --version v1.14.5 --set installCRDs=false || true

helm repo add prometheus-community https://prometheus-community.github.io/helm-charts 1>/dev/null
helm repo update 1>/dev/null
helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack --namespace monitoring --create-namespace --set grafana.enabled=false --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false || true

echo "[kind-nightly] waiting for components to be ready"
kubectl wait --for=condition=Available --timeout=180s deploy/cert-manager -n cert-manager || true
kubectl wait --for=condition=Available --timeout=180s deploy/cert-manager-webhook -n cert-manager || true
kubectl wait --for=condition=Available --timeout=180s deploy/cert-manager-cainjector -n cert-manager || true
kubectl get pods -A

echo "[kind-nightly] recording fixtures is user-defined; run orkactl discover/ls/watch"
echo "Example: ORKA_METRICS_ADDR=0.0.0.0:9090 ORKA_SHARDS=4 orkactl ls v1/ConfigMap --ns default"

echo "[kind-nightly] done"
</file>

<file path="scripts/kind-ops-smoke.sh">
#!/usr/bin/env bash
set -euo pipefail

# Minimal smoke test for Orka OPS logs against a local kind cluster.
# - Creates/uses a kind cluster
# - Deploys a busybox logger pod
# - Runs `orkactl ops logs` to stream lines
# - Exercises ops caps, logs regex/multi-container, and JSON outputs for one-shot ops

CLUSTER_NAME=${CLUSTER_NAME:-orka-ops-smoke}
NS=${NS:-ops-test}
POD=${POD:-logger}
DURATION_SECS=${DURATION_SECS:-6}

echo "[ops-smoke] cluster=${CLUSTER_NAME} ns=${NS} pod=${POD} duration=${DURATION_SECS}s"

if ! command -v kind >/dev/null 2>&1; then
  echo "kind not found; please install kind (https://kind.sigs.k8s.io/)" >&2
  exit 1
fi

if ! kind get clusters | grep -q "^${CLUSTER_NAME}$"; then
  echo "[ops-smoke] creating cluster ${CLUSTER_NAME}"
  kind create cluster --name "${CLUSTER_NAME}"
else
  echo "[ops-smoke] reusing cluster ${CLUSTER_NAME}"
fi

kubectl create namespace "${NS}" 2>/dev/null || true

# Wait for the default serviceaccount in the namespace (controller may take a moment)
echo "[ops-smoke] ensuring default serviceaccount in ${NS}"
ok=false
for i in {1..60}; do
  if kubectl -n "${NS}" get sa default >/dev/null 2>&1; then
    ok=true; break
  fi
  sleep 1
done
if [ "${ok}" != true ]; then
  echo "[ops-smoke] default serviceaccount missing; creating explicitly"
  kubectl -n "${NS}" create sa default 2>/dev/null || true
fi

if ! kubectl -n "${NS}" get pod "${POD}" >/dev/null 2>&1; then
  echo "[ops-smoke] creating logger pod"
  kubectl -n "${NS}" run "${POD}" --image=busybox --restart=Never -- /bin/sh -c 'i=0; while true; do echo "$(date -Iseconds) tick $i"; i=$((i+1)); sleep 1; done'
fi

echo "[ops-smoke] waiting for pod ready"
kubectl -n "${NS}" wait --for=condition=Ready --timeout=60s pod/"${POD}"

echo "[ops-smoke] running orkactl logs for ${DURATION_SECS}s"
if command -v timeout >/dev/null 2>&1; then
  timeout "${DURATION_SECS}"s cargo run -p orkactl -- --ns "${NS}" ops logs "${POD}" --tail 5 || true
else
  echo "(no 'timeout' found; press Ctrl-C to stop)"
  cargo run -p orkactl -- --ns "${NS}" ops logs "${POD}" --tail 5
fi

# Capabilities probe
echo "[ops-smoke] probing ops caps (JSON)"
cargo run -p orkactl -- --ns "${NS}" -o json ops caps || true
cargo run -p orkactl -- --ns "${NS}" -o json ops caps --gvk apps/v1/Deployment || true

# Logs with grep and JSON output
echo "[ops-smoke] logs with --grep 'tick' as JSON for ${DURATION_SECS}s"
if command -v timeout >/dev/null 2>&1; then
  timeout "${DURATION_SECS}"s cargo run -p orkactl -- --ns "${NS}" -o json ops logs "${POD}" --tail 5 --grep tick || true
else
  cargo run -p orkactl -- --ns "${NS}" -o json ops logs "${POD}" --tail 5 --grep tick || true
fi

# Multi-container pod for multi-logs
MC_POD=${MC_POD:-logger-multi}
if ! kubectl -n "${NS}" get pod "${MC_POD}" >/dev/null 2>&1; then
  echo "[ops-smoke] creating multi-container logger pod"
  cat <<EOF | kubectl -n "${NS}" apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: ${MC_POD}
spec:
  restartPolicy: Never
  containers:
  - name: app
    image: busybox
    command: ["/bin/sh", "-c", "i=0; while true; do echo \"$(date -Iseconds) app tick $i\"; i=$((i+1)); sleep 1; done"]
  - name: sidecar
    image: busybox
    command: ["/bin/sh", "-c", "i=0; while true; do echo \"$(date -Iseconds) side tick $i\"; i=$((i+1)); sleep 1; done"]
EOF
fi
echo "[ops-smoke] waiting for multi pod ready"
kubectl -n "${NS}" wait --for=condition=Ready --timeout=60s pod/"${MC_POD}"

echo "[ops-smoke] multi-logs (-c app -c sidecar) for ${DURATION_SECS}s"
if command -v timeout >/dev/null 2>&1; then
  timeout "${DURATION_SECS}"s cargo run -p orkactl -- --ns "${NS}" ops logs "${MC_POD}" -c app -c sidecar --tail 3 --grep tick || true
else
  cargo run -p orkactl -- --ns "${NS}" ops logs "${MC_POD}" -c app -c sidecar --tail 3 --grep tick || true
fi

echo "[ops-smoke] running orkactl exec (echo)"
cargo run -p orkactl -- --ns "${NS}" ops exec "${POD}" -- sh -c 'echo hello-from-exec'

# Simple HTTP pod for port-forward test
WEB_POD=${WEB_POD:-web}
WEB_PORT_REMOTE=${WEB_PORT_REMOTE:-5678}
WEB_PORT_LOCAL=${WEB_PORT_LOCAL:-18081}
if ! kubectl -n "${NS}" get pod "${WEB_POD}" >/dev/null 2>&1; then
  echo "[ops-smoke] creating http-echo pod"
  kubectl -n "${NS}" run "${WEB_POD}" --image=hashicorp/http-echo --port="${WEB_PORT_REMOTE}" -- -text=hello-from-http
fi
echo "[ops-smoke] waiting for http pod ready"
kubectl -n "${NS}" wait --for=condition=Ready --timeout=60s pod/"${WEB_POD}"

echo "[ops-smoke] starting port-forward ${WEB_PORT_LOCAL}:${WEB_PORT_REMOTE}"
set +e
cargo run -p orkactl -- --ns "${NS}" ops pf "${WEB_POD}" "${WEB_PORT_LOCAL}:${WEB_PORT_REMOTE}" > /tmp/orka_pf.log 2>&1 &
PF_PID=$!
set -e
sleep 2
echo "[ops-smoke] probing http endpoint"
if command -v curl >/dev/null 2>&1; then
  OUT=$(curl -fsS "http://127.0.0.1:${WEB_PORT_LOCAL}/" || true)
elif command -v wget >/dev/null 2>&1; then
  OUT=$(wget -qO- "http://127.0.0.1:${WEB_PORT_LOCAL}/" || true)
else
  echo "neither curl nor wget available; skipping http probe"
  OUT=""
fi
echo "[ops-smoke] http response: ${OUT}"
if [[ "${OUT}" != *"hello-from-http"* ]]; then
  echo "[ops-smoke] WARN: unexpected http echo response"
fi
kill "${PF_PID}" >/dev/null 2>&1 || true
wait "${PF_PID}" >/dev/null 2>&1 || true

# -------- scale + rollout-restart (Deployment) --------
DEPLOY=${DEPLOY:-web-deploy}
echo "[ops-smoke] ensuring deployment ${DEPLOY}"
kubectl -n "${NS}" create deployment "${DEPLOY}" \
  --image=nginx --port=80 --replicas=1 \
  --dry-run=client -o yaml | kubectl -n "${NS}" apply -f -
echo "[ops-smoke] waiting for rollout ${DEPLOY}=1"
kubectl -n "${NS}" rollout status deploy/"${DEPLOY}" --timeout=90s

echo "[ops-smoke] scaling ${DEPLOY} to 2 via orkactl"
cargo run -p orkactl -- --ns "${NS}" -o json ops scale apps/v1/Deployment "${DEPLOY}" 2
kubectl -n "${NS}" rollout status deploy/"${DEPLOY}" --timeout=90s

echo "[ops-smoke] rollout-restart ${DEPLOY} via orkactl"
cargo run -p orkactl -- --ns "${NS}" -o json ops rr apps/v1/Deployment "${DEPLOY}"
kubectl -n "${NS}" rollout status deploy/"${DEPLOY}" --timeout=120s

echo "[ops-smoke] scaling ${DEPLOY} back to 1 via orkactl"
cargo run -p orkactl -- --ns "${NS}" -o json ops scale apps/v1/Deployment "${DEPLOY}" 1
kubectl -n "${NS}" rollout status deploy/"${DEPLOY}" --timeout=90s

# -------- cordon/uncordon node --------
NODE_NAME=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')
echo "[ops-smoke] cordoning node ${NODE_NAME}"
cargo run -p orkactl -- -o json ops cordon "${NODE_NAME}"
UNSCHED=$(kubectl get node "${NODE_NAME}" -o jsonpath='{.spec.unschedulable}')
echo "[ops-smoke] node unschedulable=${UNSCHED}"
echo "[ops-smoke] uncordoning node ${NODE_NAME}"
cargo run -p orkactl -- -o json ops cordon "${NODE_NAME}" --off
UNSCHED=$(kubectl get node "${NODE_NAME}" -o jsonpath='{.spec.unschedulable}')
echo "[ops-smoke] node unschedulable=${UNSCHED}"

# -------- delete pod (logger) --------
echo "[ops-smoke] deleting pod ${POD} via orkactl"
cargo run -p orkactl -- --ns "${NS}" -o json ops delete "${POD}" --grace 0 || true

# -------- optional: drain (opt-in; may fail due to PDBs) --------
if [[ "${TEST_DRAIN:-}" == "1" ]]; then
  echo "[ops-smoke] draining node (opt-in TEST_DRAIN=1)"
  if command -v timeout >/dev/null 2>&1; then
    timeout 20s ORKA_DRAIN_TIMEOUT_SECS=10 cargo run -p orkactl -- ops drain "${NODE_NAME}" || true
  else
    ORKA_DRAIN_TIMEOUT_SECS=10 cargo run -p orkactl -- ops drain "${NODE_NAME}" || true
  fi
fi

echo "[ops-smoke] done"
</file>

<file path=".gitignore">
/target
repomix-output.txt
repomix-output.xml
</file>

<file path="MILESTONE-3.md">
# Orka — Milestone 3 (Scale & Hardening)

Status: In Progress — sharded ingest/search, watcher jitter/backoff + 410 recovery, per‑shard metrics, and apply preflight shipped

> Goal: turn the feature‑complete core (M0–M2) into a predictable, bounded, and resilient engine that behaves under real cluster churn and size. Make ingest/search stable at scale, keep memory under control, and prove determinism via replay and real integrations.

---

## Scope (M3)

- Sharding: partition ingest/store/search by GVK and namespace to reduce hotspots and enable parallelism.
- Watch robustness: periodic relist with jitter, watch gap detection/recovery, clear backoff and resubscribe behavior.
- Memory caps: enforce hard/soft limits; drop non‑essential payloads (`raw_ptr`) first; cap index postings and label cardinality.
- Determinism: extend replay tests to multi‑GVK, multi‑namespace streams; assert stable snapshots and search results.
- Real‑world integration: run against kind with popular operators to surface schema/validator quirks.
- Observability: richer metrics for ingest lag, shard costs, snapshot sizes, memory pressure, restart counters.
- Safety guardrails for apply: refuse mutation when snapshot is stale beyond a threshold; preflight GET for diffs when needed.

Non‑goals (M3): new user features, RPC surface changes, UI work.

Success = steady p99 latency and bounded memory on large and noisy clusters; no silent divergence after watch errors; reproducible behavior from recorded streams.

---

## Architecture Slice

```
            +── list/watch (per GVK) ──► Coalescer ──►
 kube API ──┤                                     ┌────────────┐
            +── list/watch (per GVK) ──► Coalescer ├─► Shard #1 │
                                                ...│  Builder  │─► ArcSwap<WorldSnapshot>
            +── list/watch (per GVK) ──► Coalescer ├─► Shard #N │
                                                   └────────────┘
                 ▲                 ▲                 ▲
                 │                 │                 │
             Periodic            Gap det.         Memory mgr
              relist             & resume         (caps & drops)
```

Rules:

- Shard by `(GVK, namespace)` (configurable: modulo buckets or actual namespaces); one coalescer and builder per shard.
- Periodic relist for each watcher with jitter; detect `RV_TOO_OLD` and recover without lying.
- Memory manager enforces caps; first drops `raw_ptr`, then trims postings; never panics on pressure.

---

## Workspace Additions (M3)

- `crates/store`: shard‑aware ingest (namespace buckets via `ORKA_SHARDS`), merged snapshots, per‑object caps for labels/annos, snapshot size gauge.
- `crates/kubehub`: periodic relist with jitter, bounded backoff + restarts metric for watch errors.
- `crates/search`: postings caps per key and index size gauges (single‑shard for now).
- `crates/apply`: freshness guard before SSA with optional preflight GET.
- `crates/cli`: `stats` command to print runtime knobs and metrics endpoint.
- `crates/persist` (optional): track audit/events for apply with diff summary (reuse table), not required for M3.

---

## Detailed Tasks

1) Sharding
- Introduce `ShardKey { gvk_id: u32, ns_bucket: u16 }` and a pluggable `ShardPlanner` (exact namespace or modulo N).
- Coalescer per shard with capacity and drop counters; merge deltas into shard builders.
- Compose `WorldSnapshot` from shard snapshots; keep `epoch` monotonic and shard contributions versioned.

2) Watch Robustness
- Add periodic relist per watcher with jitter (±10%) and configurable period.
- Detect resourceVersion staleness (`Expired`, HTTP 410); recover via full relist and resume.
- Backoff strategy (bounded exponential with jitter); metrics for restarts and backoff duration.

3) Memory Caps & Pressure Handling
- Add gauges: `snapshot_bytes`, `index_bytes`, `raw_bytes`, `docs_total`, `labels_cardinality`.
- Drop policy: prefer dropping `raw_ptr` after TTL; cap postings per key; limit projected fields per doc if necessary.
- Config: `ORKA_MAX_RSS_MB`, `ORKA_MAX_RAW_BYTES`, `ORKA_MAX_INDEX_BYTES`, `ORKA_SHARDS`, `ORKA_DROP_RAW_TTL_SECS`.

4) Deterministic Replay
- Extend fixtures to multi‑GVK/namespace streams; include out‑of‑order and bursty sequences.
- Assert identical `WorldSnapshot` content and `search` top‑k for fixed seeds across runs.

5) Real‑world Integration (kind)
- Install selected operators: cert‑manager, prometheus‑operator, flux‑cd (minimum set). - DONE
- Run discover → list/watch → search; validate validator behavior and projection stability.
- Gate as manual or nightly (skipped in CI by default).

6) Observability
- Metrics: `watch_restarts_total`, `relist_total`, `ingest_lag_ms`, `coalescer_dropped_total{shard}`, `shard_build_ms`, `snapshot_swap_ms`, `snapshot_bytes`, `index_bytes`, `apply_stale_blocked_total`.
- Logging: structured reasons on restarts; pressure events at `warn` with current caps and actions.

7) Apply Freshness Guard
- Before SSA, if snapshot age > `ORKA_MAX_SNAPSHOT_AGE_SECS` or object missing, do a preflight GET for diff source.
- Abort with a friendly error when freshness cannot be established; suggest `--dry-run`.

---

## CLI Specs (M3)

- `orkactl stats` — prints runtime knobs and metrics endpoint (human or `-o json`).

Example:

```
$ orkactl stats
shards: 4
relist_secs: 300
watch_backoff_max_secs: 30
max_labels_per_obj: (none)
max_annos_per_obj: 16
max_postings_per_key: 5000
metrics_addr: 127.0.0.1:9898 (exposes Prometheus /metrics)
```

---

## Performance Targets (M3)

- Snapshot build/swap: ≤ 12 ms p99 under steady ingest at 100k docs across shards.
- Search: ≤ 10 ms p99 at 100k docs with `limit=50` (unchanged from M1).
- Memory: default cap ≤ 800 MB RSS on large clusters; dropping `raw_ptr` keeps latency stable.
- Watch robustness: auto‑recovery from `Expired` with full relist completes < 30 s for 50k objs; no stale snapshot served as “fresh”.

---

## Risks & Mitigations

- Too many shards → overhead: start with modulo buckets; tune via `ORKA_SHARDS`; expose shard metrics.
- Frequent relists overload API: jittered schedule and backoff; only one in flight per watcher.
- Cardinality explosions (labels/annos) → cap postings per key and fall back to text search.
- Memory caps hurting UX → prioritize droppable payloads (`raw_ptr`) and keep projected fields intact; clearly log pressure actions.

---

## Definition of Done (M3)

- Under a recorded 100k‑doc replay, snapshot p99 ≤ 12 ms; search p99 ≤ 10 ms; memory ≤ configured cap with graceful drops.
- Watchers survive `Expired` and network blips without lying; periodic relist repairs drift.
- Replay tests deterministic across runs; integration on kind executes end‑to‑end.
- Apply respects freshness guard; stale snapshots do not lead to blind mutations.
- Metrics surface shard costs, drops, restarts, and memory usage; optional `stats` shows a concise summary.

---

## Implementation Order (Checklist)

- [x] Route coalesced deltas per shard (namespace buckets via `ORKA_SHARDS`).
- [x] Implement shard builders; compose global `WorldSnapshot` with monotonic epoch.
- [x] Wire periodic relist with jitter; restart on errors with bounded backoff (HTTP 410 Expired handled with full relist).
- [~] Add memory and index gauges + caps (labels/annos per object, postings per key); document knobs.
- [x] Extend search to shard‑local indexes; enforce limits with stable ranking across shards.
- [~] Expand replay fixtures; add deterministic multi‑GVK tests. (Sharded determinism test added; multi‑GVK scaffold present.)
- [x] Add apply freshness guard with optional preflight GET.
- [x] Add metrics for restarts, relists, snapshot/index bytes.
- [x] Implement `orkactl stats` for quick operator view.
- [x] Update docs: operations guide (caps, relist), env vars, integration notes.
- [x] Per‑shard ingest metrics: `coalescer_len{shard}`, `coalescer_dropped_total{shard}`, `ingest_batch_size{shard}`, `shard_build_ms{shard}`, plus `snapshot_swap_ms`, `shard_merge_ms`.

Next up:

- [x] Introduce `ShardPlanner` and `ShardKey { gvk_id, ns_bucket }`; thread through store/index for pluggable partitioning.
- [~] Enforce caps: `ORKA_MAX_RSS_MB`, `ORKA_MAX_INDEX_BYTES`; add `raw_bytes`, `docs_total`, `labels_cardinality` gauges and pressure logs. (warnings in place; soft‑enforcement)
- [x] Add `ingest_lag_ms` (timestamp deltas) to surface end‑to‑end staleness.
- [x] Multi‑GVK world composition + search determinism (top‑k) across runs; promote scaffold to full test.
- [x] Nightly kind integration script and recorded fixtures (skipped in CI by default).

---

### Env & Metrics

Env knobs (current):

- `ORKA_SHARDS` (default: 1)
- `ORKA_RELIST_SECS` (default: 300)
- `ORKA_WATCH_BACKOFF_MAX_SECS` (default: 30)
- `ORKA_MAX_LABELS_PER_OBJ` (optional)
- `ORKA_MAX_ANNOS_PER_OBJ` (optional)
- `ORKA_MAX_POSTINGS_PER_KEY` (optional)
- `ORKA_MAX_RSS_MB` (optional)
- `ORKA_MAX_INDEX_BYTES` (optional)
- `ORKA_METRICS_ADDR` (optional `host:port` for Prometheus `/metrics`)
- `ORKA_DISABLE_APPLY_PREFLIGHT=1` to skip apply freshness guard

Metrics (current):

- Watch: `watch_restarts_total`, `watch_backoff_ms`, `relist_total`
- Ingest: `coalescer_len{shard}`, `coalescer_dropped_total{shard}`, `ingest_batch_size{shard}`, `shard_build_ms{shard}`, `snapshot_swap_ms`, `shard_merge_ms`, `ingest_epoch`, `snapshot_items`, `snapshot_bytes`
- Ingest (added): `ingest_lag_ms`, `docs_total`, `labels_cardinality`, `raw_bytes`
- Search: `index_docs`, `index_bytes`, `index_postings_truncated_keys`, `search_candidates`, `search_eval_ms`
- Apply: `apply_stale_blocked_total`

### Progress Notes

- Sharded ingest with namespace buckets; merged snapshot maintained.
- Sharded search with stable global ranking and per‑shard candidate evaluation.
- Watcher: periodic relist with jitter; HTTP 410 (Expired) gap detection with full relist; bounded backoff and restart metrics.
- Per‑object caps for labels/annos; postings caps in search; gauges for snapshot/index sizes.
- Per‑shard ingest metrics (coalescer len/drops, shard build, swap/merge) added.
- Determinism: sharded replay test added; multi‑GVK replay scaffold in place.
- Apply preflight freshness guard implemented; opt‑out via env.
- `orkactl stats` shows runtime knobs and metrics endpoint.
</file>

<file path="MILESTONE-5-gui.md">
# milestone-gui.md

egui GUI Integration
====================

Deliver a desktop GUI (eframe/egui) that provides a rich interface on top of the
`orka_api` + `orka_ops` layers.  
Focus on fast search, clear resource views, declarative Apply pipeline, and
Imperative Ops integration (logs, exec, scale, etc).

---

## Prerequisites

- `orka_api` façade (snapshot, search, apply, stats).  
- `orka_ops` crate + CLI (Imperative Ops: logs, exec, pf, scale, rollout, delete, cordon, drain).  

---

## Scope

1. **App skeleton**
   - `orka_gui` crate using `eframe` + `egui`.
   - Shared state model: query, results, detail, edit buffer, explain, stats, ops streams.

2. **Core layout**
   - **Top bar:** search input with grammar + autocomplete (`egui_autocomplete`), watch toggle, ns/kind dropdown.
   - **Results panel (left):**
     - Virtualized table of results (`egui_virtual_list`, fallback `egui_extras::TableBuilder`).
     - Sortable columns, filter box.
   - **Detail panel (right, tabs):**
     - **Details:** YAML view (`egui_code_editor`) + labels/annotations.
     - **Edit:** YAML editor (`egui_code_editor`) with Validate • Dry-run • Apply flow.
     - **Explain:** filter-stage counts from search.
     - **Logs:** streaming pod logs with follow/tail/regex. (think if we should use virtual list here)
     - **Terminal:** interactive exec with PTY resize. (`egui_term`)
   - **Bottom bar:** shard count, epoch, drops, memory cap banners, clickable to open Stats modal.

3. **Imperative Ops integration**
   - Contextual **Actions bar** and row right-click menu.
   - Supported: logs, exec, port-forward, scale, rollout restart, delete pod, cordon/drain.
   - Logs & exec use `egui_inbox` for streaming into UI.
   - Port-forward status popover: list active forwards, stop buttons.

4. **Async & streaming**
   - Use background tasks for search, snapshot rebuild, ops streams.
   - Bounded channels for logs/exec; drop old data when UI lags.
   - Cancellation via close events.

5. **UX polish**
   - Autocomplete in search field (`egui_autocomplete`).
   - Syntax highlighting in editor (`egui_code_editor`).
   - JSON/projection tree (`egui_json_tree`).
   - Icons (`egui_material_icons`).
   - Toasts (`egui_notify`) for ops results.
   - Flex layouts (`egui_flex`) for responsive resizing.

6. **Stats page**
   - Show relist/backoff/shards/memory caps, from backend metrics.
   - Surface posting caps and drop counters.

---

## Non-Goals

- Browser/WebAssembly build (desktop only for now).  
- Multi-user / auth; assume local kubeconfig context.  
- Persisted audit trail (later milestone).  

---

## Deliverables

- `orka_gui` crate in workspace.  
- Desktop binary: `orka gui` launches egui app.  
- Panels & tabs wired with real data from `orka_api` + `orka_ops`.  
- Working Logs and Exec streaming tabs.  
- Actions bar and Cmd-K palette.  
- Documentation page: “GUI” with screenshots and feature list.  
- CI build artifacts for Linux, macOS, Windows.

---

## Notes

- Latency budget: all heavy ops async; UI thread paints only.  
- Declarative edits go through Validate → Dry-run → Apply pipeline.  
- Imperative ops bypass Apply, execute immediately, and stream results.  
- Show pressure/drops explicitly in bottom bar and Stats page.  
- Feature flags: `gui`, `ops`, `persist`, `validate`.
</file>

<file path="MILESTONE-OPS-3.1.md">
# milestone-ops.md

Imperative Ops Layer
====================

Deliver an `orka_ops` crate + CLI subcommands implementing imperative Kubernetes operations.  
This layer is independent of the UI, and will be reused by both CLI and GUI.

---

## Scope

1. **orka_ops crate**
   - Defines `OrkaOps` trait.
   - Implements imperative operations against Kubernetes:
     - `logs(pod, container, opts) -> Stream<LogChunk>`
     - `exec(pod, container, cmd, pty) -> DuplexStream<ExecChunk>`
     - `port_forward(pod, ports) -> Stream<ForwardEvent>`
     - `scale(res, replicas, mode) -> Result<()>`
     - `rollout_restart(res) -> Result<()>`
     - `delete_pod(pod, grace) -> Result<()>`
     - `cordon(node, on) -> Result<()>`
     - `drain(node, opts) -> Result<()>`

2. **CLI integration**
   - Add `orka ops …` subcommands, one per operation.
   - Human-readable output by default; JSON with `--json`.
   - Subcommands cancellable via Ctrl-C.

3. **Streaming primitives**
   - All streaming ops use bounded channels; drop old data if the consumer lags.
   - Cancellation token per op (graceful shutdown).

4. **Capability discovery**
   - Probe subresources (`scale`, `log`, `exec`, `portforward`).
   - Handle RBAC errors gracefully.

5. **Testing**
   - Unit tests with mock kube clients.
   - Integration tests against kind cluster:
     - Tail logs from pods.
     - Exec into a pod.
     - Port-forward traffic.
     - Scale a deployment.
     - Rollout restart and watch status.
     - Node cordon/drain flow.

---

## Progress

- Done:
  - `orka_ops` crate and `OrkaOps` trait.
  - Ops implemented: `logs` (follow/tail/since), `exec` (PTY + resize), `port_forward` (single port; emits Ready/Connected/Closed/Error), `scale` (Scale subresource with SSA fallback), `rollout_restart`, `delete_pod`, `cordon`, `drain` (PDB-aware, wait with timeout/poll).
  - Streaming primitives: bounded channels + per-op cancellation (used by logs/pf).
  - CLI: `orka ops logs|exec|pf|scale|rr|delete|cordon|drain` wired and working.
  - Smoke tests: `scripts/kind-ops-smoke.sh` covers logs, exec, pf (+ HTTP probe), scale up/down, rollout-restart, cordon/uncordon, delete; optional drain.

- Remaining (strictly required to close this milestone):
  - Capability discovery: probe subresources and present RBAC “forbidden” errors as friendly messages; add `orkactl ops caps` (human/JSON).  [Done]
  - Logs completeness: add regex filter and multi-container selection flags.  [Done]
  - Consistent JSON output: respect `-o json` for one-shot ops (scale/rr/delete/cordon/drain) with structured outputs.  [Done]
  - Exec/pf cancellation polish: ensure Ctrl-C aborts remote exec cleanly (in addition to process exit); pf already supports cancel token.  [Done]
  - Tests/CI: unit tests and CI job to run the kind smoke workflow.  [Done]
  - Docs: expand “Imperative Ops” page to include scale/rr/delete/cordon/drain with examples and flags.  [Done]

---

## Non-Goals

- GUI integration (egui).  
- Validation/dry-run/apply (already covered by `orka_apply`).  
- Persisted audit trail (will follow later milestone).

---

## Deliverables

- `orka_ops` crate in workspace.  [Done]
- CLI: `orka ops logs`, `orka ops exec`, `orka ops pf`, `orka ops scale`, `orka ops rr`, `orka ops delete`, `orka ops cordon`, `orka ops drain`.  [Done]
- Documentation page: “Imperative Ops” with usage examples.  [Partial — expand for all ops]
- CI pipeline jobs running integration tests on kind.  [Pending]

---

## Notes

- Scale must support both `Scale` subresource and SSA to `.spec.replicas`.  
- Rollout restart = patch template annotation `kubectl.kubernetes.io/restartedAt`.  
- Exec must support PTY resize.  
- Logs must support `--follow`, `--tail`, `--since` [done], plus regex filter and multiple containers [pending].  
- Port-forward must expose lifecycle events (`Ready`, `Closed`). [Done]
</file>

<file path="plan.md">
# Orka – Backend‑First Design & Implementation Plan

> **Mission:** Build a CRD‑native, ultra‑responsive Kubernetes IDE engine in Rust, with a thin UI client layered on top. Ship the **backend first** (headless), then attach an egui-based UI using mostly existing crates.

---

## 1) Product Vision

* **CRD‑first:** Treat CustomResourceDefinitions as first‑class citizens: discover, list/watch, validate, and edit with server‑side apply (SSA).
* **Latency:** Interactive operations (search, list updates) feel instant: <10 ms p99 query latency, lock‑free UI reads via snapshots.
* **Safety:** Read‑only by default; dry‑run required before any mutation.
* **Portability:** Single static binary per OS, no external deps required for core operation.

### Non‑goals (v0)

* Helm UI, dashboards, collaborative sharing, terminal multiplexer.
* Disk‑backed full‑text history search (feature‑gated for later).

---

## 2) Repository Layout

```
orka/
├─ crates/
│  ├─ core/            # shared types, errors, feature flags
│  ├─ kubehub/         # kube client, discovery, watchers (DynamicObject)
│  ├─ schema/          # CRD OpenAPI model, printer-cols, projection, validation glue
│  ├─ store/           # in-RAM store (lite objs), interning, RCU snapshots
│  ├─ search/          # RAM index + fuzzy scoring (fuzzy-matcher); optional tantivy
│  ├─ apply/           # dry-run + server-side-apply, diffs, snapshots
│  ├─ persist/         # SQLite adapters (prefs, schema cache, last-applied, audit)
│  ├─ rpc/             # gRPC or JSON-RPC server + client
│  └─ cli/             # orkactl: exercise backend headlessly
├─ benches/
└─ docs/
```

Feature flags:

* `persist-sqlite`, `search-tantivy`, `strip-managed-fields`, `jsonschema-validate` (default on).

---

## 3) High-Level Architecture

```
+---------------- Orka Backend ----------------+
| Discovery ◁── kube API ──▶ Watchers (per GVK) |
|         │                         │          |
|         ▼                         ▼          |
|      Schema Engine        Coalescing Queue    |
|         │                         │          |
|         ▼                         ▼          |
|        Projection        World Builder (RCU)  |
|         │                         │          |
|         ▼                         ▼          |
|    Search Index  ◀──────  World Snapshot      |
|         │                         │          |
|         ▼                         ▼          |
|      RPC Server  ◀──────  Apply/Diff/Snap    |
+----------------------------------------------+
```

* **Watchers**: one per ApiResource (CRD version); list+watch with bookmarks.
* **Store**: builds immutable **WorldSnapshot** instances (RCU via `arc-swap`).
* **Search**: RAM index using `fuzzy-matcher`; typed filters; optional FST for prefix.
* **Apply**: dry‑run then SSA; local last‑applied snapshots; diffs.
* **RPC**: streaming endpoints for discover/list/watch/search; UI and CLI are clients.

---

## 4) Core Data Structures

```rust
// crates/core/src/types.rs
pub type InternId = u32;
pub type Uid = [u8; 16];

#[derive(Clone)]
pub struct LiteObj {
    pub uid: Uid,
    pub cluster: InternId,
    pub group: InternId,
    pub version: InternId,
    pub kind: InternId,
    pub namespaced: bool,
    pub namespace: Option<InternId>,
    pub name: InternId,
    pub creation_ts: i64,
    pub labels: smallvec::SmallVec<[(InternId, InternId); 8]>,
    pub annotations: smallvec::SmallVec<[(InternId, InternId); 4]>,
    pub projected: smallvec::SmallVec<[(u32, String); 8]>, // (PathId, rendered scalar)
    pub raw_ptr: Option<alloc::sync::Arc<[u8]>>,           // lazy JSON bytes
}

#[derive(Clone)]
pub struct WorldSnapshot {
    pub epoch: u64,
    pub kinds: alloc::collections::BTreeMap<u32, Vec<LiteObj>>, // by KindId
    // additional indexes for fast filtering
}
```

**Interning Pools**: `lasso` for namespaces, kinds, groups, label keys/values; stable across epochs.

**Projection**: schema engine selects a few scalar `spec`/`status` paths per GVK for list/search.

---

## 5) Kube Integration (kubehub)

* Use `kube` + `k8s-openapi`.
* Discovery via `kube::discovery::Discovery` + read CRDs: `apiextensions.k8s.io/v1/CustomResourceDefinition`.
* For each `ApiResource` (served version):

  * Create `Api<DynamicObject>` (namespaced/all depending on scope).
  * Start `reflector` with **bounded** channel and **coalescing** by UID.
  * Schedule periodic relist.
* Strip `metadata.managedFields` under `strip-managed-fields` feature to reduce memory.

**Coalescing Queue (sketch):**

```rust
struct Coalescer { map: FxHashMap<Uid, Delta>, order: VecDeque<Uid>, cap: usize }
impl Coalescer { /* insert/update by uid; drop oldest when cap exceeded */ }
```

---

## 6) Schema Engine (schema)

**Responsibilities**

* Normalize `openAPIV3Schema` per served version.
* Extract `additionalPrinterColumns` when present.
* Derive **projected fields** if columns absent: choose 3–6 scalar leaves with highest information value.
* Handle k8s quirks: `x-kubernetes-int-or-string`, `additionalProperties`, `oneOf/anyOf/allOf` (mark as YAML-only), `preserveUnknownFields`.
* Validate edited YAML (async) using `serde_yaml` → `serde_json::Value` → `jsonschema`.

**Outputs**

```rust
pub struct CrdSchema {
    pub served_version: String,
    pub printer_cols: Vec<PrinterCol>,
    pub projected_paths: Vec<PathSpec>, // with renderers
    pub flags: SchemaFlags,
}
```

**Schema Cache** (persist optionally): key `{cluster, group, plural, version}` with ETag `resourceVersion`.

---

## 7) Store & Snapshots (store)

* World built on an ingest thread from coalesced deltas.
* RCU via `arc_swap::ArcSwap<WorldSnapshot>` for **lock‑free reads**.
* Memory caps enforced by sharding and dropping `raw_ptr` for cold objects.

**Swap Loop (sketch):**

```rust
static WORLD: arc_swap::ArcSwap<WorldSnapshot> = arc_swap::ArcSwap::from_pointee(WorldSnapshot { epoch: 0, kinds: Default::default() });

fn ingest_loop(mut rx: Receiver<Delta>) {
    let mut builder = WorldBuilder::new();
    loop {
        let batch = coalesce_for(8 /* ms */ , &mut rx);
        if batch.is_empty() { continue; }
        builder.apply(batch);
        let next = builder.freeze(); // Arc<WorldSnapshot>
        WORLD.store(next);
    }
}
```

---

## 8) Search (search)

* **Default:** RAM index only; rebuild incrementally from deltas.
* **Scoring:** `fuzzy-matcher` (SkimMatcherV2).
* **Grammar:** `k:Kind g:group ns:foo label:app=bar anno:team=core field:spec.path=value` + free text.
* **Pipeline:** fast set intersections for typed filters → fuzzy scoring on candidates.
* Optional: `fst` for exact/prefix acceleration.

Targets: **<10 ms p99** at 100k docs, single thread.

---

## 9) Apply & Diffs (apply)

* **Dry‑run first** (server): validate and preview.
* **Server‑side apply** with `fieldManager="orka"` on success.
* Store **last‑applied** (up to 3) per object in SQLite (zstd blobs).
* Produce humanized diffs via `json_patch` + `similar` for presentation.
* Support `status` subresource explicitly when present.

---

## 10) Persistence (persist)

* Optional, via `rusqlite`:

  * `prefs(key TEXT PRIMARY KEY, val BLOB)`
  * `schema_cache(key TEXT PRIMARY KEY, etag TEXT, blob BLOB)`
  * `snap(uid TEXT, ts INTEGER, blob BLOB, PRIMARY KEY(uid, ts))`
  * `audit(ts INTEGER, cluster TEXT, verb TEXT, gvk TEXT, ns TEXT, name TEXT, diff_summary TEXT)`

---

## 11) RPC Surface (rpc)

Choose **gRPC** with `tonic` (recommended) or JSON‑RPC with `jsonrpsee`.

**Proto (sketch):**

```proto
syntax = "proto3";
package orka.v1;

message ResourceKind { string group=1; string version=2; string kind=3; bool namespaced=4; }
message ResourceRef  { string cluster=1; ResourceKind gvk=2; string namespace=3; string name=4; }
message Lite {
  string uid=1; string cluster=2;
  string group=3; string version=4; string kind=5;
  string namespace=6; string name=7; int64 creation_ts=8;
  map<string,string> labels=9; map<string,string> annotations=10;
  map<string,string> projected=11;
}
message SearchQuery { string q=1; string cluster=2; int32 limit=3; }
message SearchHit { Lite doc=1; float score=2; }
message ApplyReport { bool ok=1; string message=2; bytes server_patch=3; }

service Orka {
  rpc Discover(.google.protobuf.Empty) returns (stream ResourceKind);
  rpc List(ResourceKind) returns (stream Lite);
  rpc Watch(ResourceKind) returns (stream Lite);
  rpc GetRaw(ResourceRef) returns (bytes);
  rpc Search(SearchQuery) returns (stream SearchHit);
  rpc DryRunApply(bytes) returns (ApplyReport);
  rpc ServerApply(bytes) returns (ApplyReport);
}
```

Auth: respect kubeconfig contexts; pass through exec‑plugin flows.

---

## 12) CLI: **`orkactl`** (no UI required)

Commands:

* `orkactl discover`
* `orkactl ls gvk --ns default`
* `orkactl watch gvk --ns default`
* `orkactl get ref -o yaml`
* `orkactl search "k:Application ns:prod payments"`
* `orkactl edit ref.yaml --dry-run | --apply`
* `orkactl schema gvk`

Use this to validate backend behavior and performance in CI and locally.

---

## 13) Testing & Benchmarks

**Unit tests**: schema parsing, projection selection, validator edge cases, search scoring.

**Replay tests**: record List/Watch streams (newline‑delimited JSON). Feed into a `DeltaSource` trait to produce deterministic ingest.

**Integration (kind cluster)**:

* Install operators: cert‑manager, prometheus‑operator, argo‑cd, istio (selected CRDs).
* Run end‑to‑end flows: discover → list → watch → search → dry‑run → apply.

**Benches (criterion)**:

* 100k CRs synthetic dataset → measure ingest throughput, snapshot build time, search p99, memory footprint.

---

## 14) Performance Budgets & Tuning

* Snapshot swap cadence: build in ≤8–12 ms under steady state.
* Search: ≤10 ms p99 @100k docs.
* Memory cap: default 600–800 MB on large clusters. Strategies: interning, projected fields, drop raw bytes when idle, shard by GVK.
* Backpressure: bounded queues; coalesce by UID; periodic relists.

---

## 15) Security & RBAC

* Read verbs discovered via `SelfSubjectRulesReview`/`SelfSubjectAccessReview`.
* Gray‑out mutating RPCs when not allowed; enforce server checks regardless.
* Never store tokens unencrypted; rely on kubeconfig/exec‑plugins.

---

## 16) UI (Later) – Leverage Existing Crates

* `egui`, `eframe`, `egui_extras::TableBuilder`, `egui_dock`, `egui_code_editor`, `syntect`, `egui-toast`, `egui-modal`.
* UI acts as a **client** of Orka RPC: subscribe to `Watch`, render `Lite` tables, open YAML with `GetRaw`, call `DryRunApply`/`ServerApply`.

---

## 17) Milestones (Backend‑first)

**M0 – Skeleton (1–2 weeks)**

* kube client, discovery, single CRD watcher, coalescing, RCU snapshot, basic CLI `discover`, `ls`, `watch`.

**M1 – Schema & Search (2–3 weeks)**

* Schema engine (printer columns, projection, validation). RAM index + typed filters + fuzzy. CLI: `schema`, `search`.

**M2 – Apply & Persistence (2 weeks)**

* Dry‑run + SSA; last‑applied snapshots; diffs; optional SQLite cache. CLI: `edit --dry-run|--apply`.

**M3 – Scale & Hardening (2 weeks)**

* Namespaced sharding, periodic relist, memory caps, replay tests, integration on kind with operators.

**M4 – RPC Stabilization (1 week)**

* Finalize proto; add streaming endpoints; CLI switches to RPC path.

*(UI milestones follow afterwards.)*

---

## 18) Risks & Mitigations

* **CRD schema variance** → robust YAML fallback; tolerant validator.
* **Large clusters** → sharding, coalescing, projected fields, memory caps.
* **Auth/exec plugins** → rely on `kube` support; surface expiry and retry gracefully.
* **Index rebuild costs** → incremental ingest + generational swap; optional snapshot persistence.

---

## 19) Coding Standards & Tooling

* Edition 2021+, `clippy` pedantic profile; `rustfmt` enforced.
* Observability via `tracing` with targets per crate; feature‑gated in release.
* Error handling via `thiserror`/`anyhow` where appropriate.
* CI: fmt, clippy, tests, benches (smoke), kind integration (nightly).

---

## 20) Next Actions (Week 0 Checklist)

* [ ] Init repo + workspace layout
* [ ] `kubehub`: kube client + discovery; print all resources
* [ ] Start first CRD watcher (e.g., cert‑manager `Certificate`)
* [ ] Implement coalescing queue + ingest loop + `WorldSnapshot` swap
* [ ] `orkactl discover | ls | watch` minimal commands
* [ ] Decide RPC flavor (tonic vs jsonrpsee) & add crate skeleton
* [ ] Add criterion baseline bench with synthetic deltas

> When M0 is green, we’ll lock the RPC surface and start on schema/search.
</file>

<file path="crates/apply/src/lib.rs">
//! Orka apply (Milestone 2): dry-run and SSA helpers + minimal diffs.

#![forbid(unsafe_code)]

use anyhow::{anyhow, Context, Result};
use kube::{api::{Api, Patch, PatchParams}, core::{DynamicObject, GroupVersionKind}, discovery::{Discovery, Scope}, Client};
use metrics::{counter, histogram};
use serde::{Deserialize, Serialize};
use serde_json::Value as Json;
use tracing::warn;
use orka_persist::Store;
use uuid::Uuid;

#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct DiffSummary { pub adds: usize, pub updates: usize, pub removes: usize }

#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct ApplyResult {
    pub dry_run: bool,
    pub applied: bool,
    pub new_rv: Option<String>,
    pub warnings: Vec<String>,
    pub summary: DiffSummary,
}

pub async fn edit_from_yaml(
    yaml: &str,
    ns_override: Option<&str>,
    validate: bool,
    do_apply: bool,
) -> Result<ApplyResult> {
    let t0 = std::time::Instant::now();
    counter!("apply_attempts", 1u64);
    let (json, gvk, name, ns) = parse_yaml_for_target(yaml, ns_override)?;

    if validate {
        // M2: validation optional and only for CRDs; keep soft-fail with messages routed to CLI if needed.
        // We do not wire jsonschema here to avoid heavy deps in this crate. CLI will call into orka_schema feature if enabled.
    }

    let client = Client::try_default().await?;
    let (ar, namespaced) = find_api_resource(client.clone(), &gvk).await?;
    let api: Api<DynamicObject> = if namespaced {
        match ns.as_deref() {
            Some(n) => Api::namespaced_with(client.clone(), n, &ar),
            None => return Err(anyhow!("namespace required for namespaced kind")),
        }
    } else {
        Api::all_with(client.clone(), &ar)
    };

    // Load live to compute diff summary
    let live_json = match api.get_opt(&name).await? {
        Some(obj) => Some(strip_noisy(serde_json::to_value(&obj)?)),
        None => None,
    };
    let tgt_json = {
        let mut v = strip_noisy(json.clone());
        ensure_metadata(&mut v, &name, ns.as_deref());
        v
    };
    let summary = diff_summary(&tgt_json, &live_json.clone().unwrap_or(Json::Null));

    if !do_apply {
        // Dry-run: ask server to validate the SSA patch but don't persist
        let pp = PatchParams::apply("orka").dry_run();
        let res = api.patch(&name, &pp, &Patch::Apply(&json)).await;
        match res {
            Ok(_) => {
                histogram!("apply_latency_ms", t0.elapsed().as_secs_f64() * 1000.0);
                counter!("apply_dry_ok", 1u64);
                return Ok(ApplyResult { dry_run: true, applied: false, new_rv: None, warnings: vec![], summary });
            }
            Err(e) => {
                counter!("apply_err", 1u64);
                return Err(anyhow!("dry-run failed: {}", e));
            }
        }
    }

    // Optional preflight freshness guard: if live resourceVersion changed since we computed the diff, abort.
    // Enabled by default; set ORKA_DISABLE_APPLY_PREFLIGHT=1 to skip this guard.
    if std::env::var("ORKA_DISABLE_APPLY_PREFLIGHT").is_err() {
        if let Some(prev_live) = &live_json {
            let prev_rv = prev_live
                .get("metadata").and_then(|m| m.get("resourceVersion")).and_then(|v| v.as_str()).map(|s| s.to_string());
            if let Some(prev_rv) = prev_rv {
                if let Some(obj2) = api.get_opt(&name).await? {
                    let cur_rv = obj2.metadata.resource_version.clone().unwrap_or_default();
                    if !cur_rv.is_empty() && cur_rv != prev_rv {
                        metrics::counter!("apply_stale_blocked_total", 1u64);
                        return Err(anyhow!(
                            "live object changed (rv {} -> {}) during apply; re-run diff/dry-run and try again",
                            prev_rv, cur_rv
                        ));
                    }
                }
            }
        }
    }

    // Real apply (SSA)
    let pp = PatchParams::apply("orka");
    let obj = match api.patch(&name, &pp, &Patch::Apply(&json)).await {
        Ok(o) => o,
        Err(e) => { counter!("apply_err", 1u64); return Err(anyhow!("server-side apply failed: {}", e)); }
    };
    let new_rv = obj.metadata.resource_version.clone();
    histogram!("apply_latency_ms", t0.elapsed().as_secs_f64() * 1000.0);
    counter!("apply_ok", 1u64);

    // Persist last-applied YAML snapshot (zstd if enabled)
    let uid = obj.metadata.uid.as_deref().ok_or_else(|| anyhow!("applied object missing metadata.uid"))?;
    let uid_bin = parse_uid(uid)?;
    let rv = new_rv.clone().unwrap_or_default();
    let la = orka_persist::LastApplied { uid: uid_bin, rv, ts: orka_persist::now_ts(), yaml_zstd: orka_persist::maybe_compress(yaml) };
    match orka_persist::SqliteStore::open_default() {
        Ok(store) => { let _ = store.put_last(la); }
        Err(e) => warn!(error = %e, "persist open failed; skipping last-applied save"),
    }

    Ok(ApplyResult { dry_run: false, applied: true, new_rv, warnings: vec![], summary })
}

pub async fn diff_from_yaml(yaml: &str, ns_override: Option<&str>) -> Result<(DiffSummary, Option<DiffSummary>)> {
    let (json, gvk, name, ns) = parse_yaml_for_target(yaml, ns_override)?;
    let client = Client::try_default().await?;
    let (ar, namespaced) = find_api_resource(client.clone(), &gvk).await?;
    let api: Api<DynamicObject> = if namespaced {
        match ns.as_deref() {
            Some(n) => Api::namespaced_with(client.clone(), n, &ar),
            None => return Err(anyhow!("namespace required for namespaced kind")),
        }
    } else {
        Api::all_with(client.clone(), &ar)
    };
    let live_json = match api.get_opt(&name).await? {
        Some(obj) => Some(strip_noisy(serde_json::to_value(&obj)?)),
        None => None,
    };
    let tgt_json = {
        let mut v = strip_noisy(json.clone());
        ensure_metadata(&mut v, &name, ns.as_deref());
        v
    };
    let live_summary = diff_summary(&tgt_json, &live_json.clone().unwrap_or(Json::Null));

    // Diff against last-applied if present
    let last_summary = if let Some(uid_str) = live_json.as_ref().and_then(|v| v.get("metadata")).and_then(|m| m.get("uid")).and_then(|s| s.as_str()) {
        if let Ok(store) = orka_persist::SqliteStore::open_default() {
            if let Ok(rows) = store.get_last(parse_uid(uid_str)?, Some(1)) {
                if let Some(top) = rows.get(0) {
                    let prev_yaml = orka_persist::maybe_decompress(&top.yaml_zstd);
                    if let Ok(prev_val_yaml) = serde_yaml::from_str::<serde_yaml::Value>(&prev_yaml) {
                        let prev_json = serde_json::to_value(prev_val_yaml).unwrap_or(Json::Null);
                        let prev = strip_noisy(prev_json);
                        let sum = diff_summary(&tgt_json, &prev);
                        Some(sum)
                    } else { None }
                } else { None }
            } else { None }
        } else { None }
    } else { None };

    Ok((live_summary, last_summary))
}

fn parse_yaml_for_target(yaml: &str, ns_override: Option<&str>) -> Result<(Json, GroupVersionKind, String, Option<String>)> {
    let val: serde_yaml::Value = serde_yaml::from_str(yaml).context("parsing YAML")?;
    let json = serde_json::to_value(val).context("converting YAML to JSON")?;
    let api_version_s = json.get("apiVersion").and_then(|v| v.as_str()).ok_or_else(|| anyhow!("YAML missing apiVersion"))?.to_string();
    let kind_s = json.get("kind").and_then(|v| v.as_str()).ok_or_else(|| anyhow!("YAML missing kind"))?.to_string();
    let (group, version) = if let Some((g, v)) = api_version_s.split_once('/') { (g.to_string(), v.to_string()) } else { (String::new(), api_version_s) };
    let name = json.get("metadata").and_then(|m| m.get("name")).and_then(|v| v.as_str()).ok_or_else(|| anyhow!("YAML missing metadata.name"))?.to_string();
    let ns = ns_override.map(|s| s.to_string()).or_else(|| json.get("metadata").and_then(|m| m.get("namespace")).and_then(|v| v.as_str()).map(|s| s.to_string()));
    Ok((json, GroupVersionKind { group, version, kind: kind_s }, name, ns))
}

async fn find_api_resource(client: Client, gvk: &GroupVersionKind) -> Result<(kube::core::ApiResource, bool)> {
    let discovery = Discovery::new(client).run().await?;
    for group in discovery.groups() {
        for (ar, caps) in group.recommended_resources() {
            if ar.group == gvk.group && ar.version == gvk.version && ar.kind == gvk.kind {
                let namespaced = matches!(caps.scope, Scope::Namespaced);
                return Ok((ar.clone(), namespaced));
            }
        }
    }
    Err(anyhow!("GVK not found: {}/{}/{}", gvk.group, gvk.version, gvk.kind))
}

fn strip_noisy(mut v: Json) -> Json {
    if let Some(meta) = v.get_mut("metadata") {
        if let Some(obj) = meta.as_object_mut() {
            obj.remove("managedFields");
            obj.remove("resourceVersion");
            obj.remove("generation");
            obj.remove("creationTimestamp");
        }
    }
    // Status is server-populated; ignore it during diffs
    if let Some(obj) = v.as_object_mut() { obj.remove("status"); }
    v
}

fn ensure_metadata(v: &mut Json, name: &str, ns: Option<&str>) {
    let meta = v.as_object_mut().unwrap().entry("metadata").or_insert(Json::Object(serde_json::Map::new()));
    if let Some(obj) = meta.as_object_mut() {
        obj.insert("name".into(), Json::String(name.to_string()));
        if let Some(ns) = ns { obj.insert("namespace".into(), Json::String(ns.to_string())); }
    }
}

pub fn diff_summary(target: &Json, base: &Json) -> DiffSummary {
    fn walk(a: &Json, b: &Json, adds: &mut usize, ups: &mut usize, rems: &mut usize) {
        use serde_json::Value as V;
        match (a, b) {
            (V::Object(ao), V::Object(bo)) => {
                for (k, av) in ao.iter() {
                    if let Some(bv) = bo.get(k) {
                        if av == bv { continue; }
                        walk(av, bv, adds, ups, rems);
                    } else {
                        *adds += 1;
                    }
                }
                for (k, _bv) in bo.iter() {
                    if !ao.contains_key(k) { *rems += 1; }
                }
            }
            (V::Array(aa), V::Array(bb)) => {
                let min_len = aa.len().min(bb.len());
                for i in 0..min_len { if aa[i] != bb[i] { *ups += 1; } }
                if aa.len() > bb.len() { *adds += aa.len() - bb.len(); }
                if bb.len() > aa.len() { *rems += bb.len() - aa.len(); }
            }
            // Scalars differ or type differs
            (av, bv) => { if av != bv { *ups += 1; } }
        }
    }
    let mut adds = 0usize; let mut ups = 0usize; let mut rems = 0usize;
    walk(target, base, &mut adds, &mut ups, &mut rems);
    DiffSummary { adds, updates: ups, removes: rems }
}

fn parse_uid(uid_str: &str) -> Result<[u8; 16]> {
    let u = Uuid::parse_str(uid_str).context("parsing metadata.uid as uuid")?;
    Ok(*u.as_bytes())
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn strip_noisy_prunes_common_fields() {
        let v = serde_json::json!({
            "apiVersion": "v1",
            "kind": "ConfigMap",
            "metadata": {
                "name": "x",
                "namespace": "ns",
                "managedFields": [ {"foo": "bar"} ],
                "resourceVersion": "123",
                "generation": 5,
                "creationTimestamp": "2020-01-01T00:00:00Z"
            },
            "status": { "obs": true },
            "data": { "k": "v" }
        });
        let pruned = strip_noisy(v);
        let meta = pruned.get("metadata").unwrap().as_object().unwrap();
        assert!(!meta.contains_key("managedFields"));
        assert!(!meta.contains_key("resourceVersion"));
        assert!(!meta.contains_key("generation"));
        assert!(!meta.contains_key("creationTimestamp"));
        assert!(!pruned.as_object().unwrap().contains_key("status"));
    }

    #[test]
    fn diff_summary_counts_adds_updates_removes() {
        let base = serde_json::json!({
            "a": 1,
            "b": { "x": 1 },
            "c": [1, 2, 3]
        });
        let target = serde_json::json!({
            "a": 2,                // scalar update
            "b": { "x": 1, "y": 2 }, // object add
            "c": [1, 9],           // array element update + removals
            "d": true              // key add
        });
        let s = diff_summary(&target, &base);
        // adds: b.y, d, and array shrink by 1 element => 2 adds? array shrink is removes, not adds.
        // target has shorter array than base -> removes count 1; array index 1 changed -> updates 1
        // scalar a changed -> updates += 1
        // new key d -> adds += 1; new key b.y -> adds += 1
        assert_eq!(s.adds, 2);
        assert_eq!(s.updates, 2);
        assert_eq!(s.removes, 1);
    }

    #[test]
    fn parse_yaml_errors_are_friendly() {
        // missing apiVersion
        let y1 = "kind: Foo\nmetadata:\n  name: x\n";
        let e1 = parse_yaml_for_target(y1, None).unwrap_err().to_string();
        assert!(e1.contains("missing apiVersion"), "e1={}", e1);

        // missing kind
        let y2 = "apiVersion: v1\nmetadata:\n  name: x\n";
        let e2 = parse_yaml_for_target(y2, None).unwrap_err().to_string();
        assert!(e2.contains("missing kind"), "e2={}", e2);

        // missing metadata.name
        let y3 = "apiVersion: v1\nkind: ConfigMap\nmetadata: {}\n";
        let e3 = parse_yaml_for_target(y3, None).unwrap_err().to_string();
        assert!(e3.contains("missing metadata.name"), "e3={}", e3);
    }
}
</file>

<file path="crates/core/Cargo.toml">
[package]
name = "orka-core"
version = "0.0.0"
edition = "2021"
license = "MIT OR Apache-2.0"
description = "Orka core types and errors (Milestone 0)"

[features]
default = ["strip-managed-fields"]
strip-managed-fields = []
persist-sqlite = [] # stubbed for later milestones

[dependencies]
serde = { workspace = true }
serde_json = { workspace = true }
thiserror = { workspace = true }
anyhow = { workspace = true }
smallvec = { workspace = true }
</file>

<file path="crates/kubehub/Cargo.toml">
[package]
name = "orka-kubehub"
version = "0.0.0"
edition = "2021"
license = "MIT OR Apache-2.0"
description = "Orka kube integration (Milestone 0)"

[dependencies]
orka-core = { path = "../core" }
anyhow = { workspace = true }
thiserror = { workspace = true }
tracing = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
tokio = { workspace = true }
kube = { workspace = true }
k8s-openapi = { workspace = true }
futures = { workspace = true }
uuid = { workspace = true }
metrics = { workspace = true }
</file>

<file path="crates/schema/src/lib.rs">
//! Orka schema (Milestone 1 stub): discover CRD schema, printer columns, and projected paths.

#![forbid(unsafe_code)]

use anyhow::{anyhow, Context, Result};
use serde::{Deserialize, Serialize};
use smallvec::SmallVec;
// tracing optional here; keep code quiet for now
use orka_core::Projector;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PrinterCol {
    pub name: String,
    pub json_path: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PathSpec {
    pub id: u32,
    pub json_path: String,
}

#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct SchemaFlags {
    pub yaml_only_nodes: bool,
    pub preserves_unknown: bool,
}

#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct CrdSchema {
    pub served_version: String,
    pub printer_cols: Vec<PrinterCol>,
    pub projected_paths: Vec<PathSpec>,
    pub flags: SchemaFlags,
}

fn normalize_json_path(jp: &str) -> Option<String> {
    // Accept only simple paths like .spec.foo.bar[0]
    if jp.contains('?') || jp.contains('*') { return None; }
    let s = if let Some(stripped) = jp.strip_prefix('.') { stripped } else { jp };
    if s.is_empty() { return None; }
    // Validate segments: allow alnum/underscore/hyphen keys; optional single [index] at end
    for seg in s.split('.') {
        if seg.is_empty() { return None; }
        let bytes = seg.as_bytes();
        // count '[' occurrences
        let mut open_idx: Option<usize> = None;
        for (i, ch) in bytes.iter().enumerate() {
            match *ch as char {
                '[' => {
                    if open_idx.is_some() { return None; } // multiple [
                    open_idx = Some(i);
                }
                ']' => {
                    // ']' only allowed if we saw '[' and it must be the last char
                    match open_idx {
                        Some(start) => {
                            if i != bytes.len() - 1 { return None; }
                            // ensure digits between [ and ]
                            if start + 1 >= i { return None; }
                            if !seg[start+1..i].chars().all(|c| c.is_ascii_digit()) { return None; }
                        }
                        None => return None,
                    }
                }
                c => {
                    // before '[' ensure key chars are safe
                    if open_idx.is_none() {
                        if !(c.is_ascii_alphanumeric() || c == '_' || c == '-') { return None; }
                    } else {
                        // inside index; digits validated on closing
                    }
                }
            }
        }
        // if '[' opened, we must have seen a closing ']' (enforced above by requiring it as last char)
        if let Some(start) = open_idx {
            if !seg.ends_with(']') || start >= seg.len()-1 { return None; }
        }
    }
    Some(s.to_string())
}

/// Try to fetch CRD schema for the provided `gvk_key` (e.g. "group/v1/Kind" or "v1/Kind").
/// Returns Ok(None) for built-in kinds without a CRD.
pub async fn fetch_crd_schema(gvk_key: &str) -> Result<Option<CrdSchema>> {
    use kube::Client;
    use k8s_openapi::apiextensions_apiserver::pkg::apis::apiextensions::v1 as apiextv1;
    use kube::{Api, api::ListParams};

    let client = Client::try_default().await?;
    // Parse key
    let parts: Vec<_> = gvk_key.split('/').collect();
    let (group, version, kind) = match parts.as_slice() {
        [version, kind] => ("", *version, *kind),
        [group, version, kind] => (*group, *version, *kind),
        _ => return Err(anyhow!("invalid gvk key: {}", gvk_key)),
    };

    if group.is_empty() {
        // Builtins have no CRD
        return Ok(None);
    }

    // List CRDs and find the one matching group + kind (robust across discovery quirks)
    let api: Api<apiextv1::CustomResourceDefinition> = Api::all(client.clone());
    let crds = api.list(&ListParams::default()).await.context("listing CustomResourceDefinitions")?;
    let mut v_opt: Option<serde_json::Value> = None;
    for crd in crds {
        let v = serde_json::to_value(&crd)?;
        let spec = match v.get("spec") { Some(s) => s, None => continue };
        let g = spec.get("group").and_then(|s| s.as_str()).unwrap_or("");
        let k = spec.get("names").and_then(|n| n.get("kind")).and_then(|s| s.as_str()).unwrap_or("");
        if g == group && k == kind { v_opt = Some(v); break; }
    }
    let v = match v_opt { Some(v) => v, None => return Err(anyhow!("CRD not found for {}", gvk_key)) };
    let versions = v
        .get("spec").and_then(|s| s.get("versions"))
        .and_then(|vv| vv.as_array())
        .cloned()
        .unwrap_or_default();

    // Pick a served version: prefer storage=true, else first served=true, else requested version
    let mut served_version = version.to_string();
    if !versions.is_empty() {
        if let Some(storage_v) = versions.iter().find(|ver| ver.get("storage").and_then(|b| b.as_bool()).unwrap_or(false)) {
            if let Some(name) = storage_v.get("name").and_then(|s| s.as_str()) { served_version = name.to_string(); }
        } else if let Some(served_v) = versions.iter().find(|ver| ver.get("served").and_then(|b| b.as_bool()).unwrap_or(false)) {
            if let Some(name) = served_v.get("name").and_then(|s| s.as_str()) { served_version = name.to_string(); }
        }
    }

    // Extract additionalPrinterColumns for the chosen version; fallback to top-level spec.additionalPrinterColumns (v1beta1 style)
    let mut printer_cols: Vec<PrinterCol> = Vec::new();
    if let Some(ver) = versions.iter().find(|ver| ver.get("name").and_then(|s| s.as_str()) == Some(served_version.as_str())) {
        if let Some(cols) = ver.get("additionalPrinterColumns").and_then(|c| c.as_array()) {
            for c in cols {
                let name = c.get("name").and_then(|s| s.as_str()).unwrap_or("").to_string();
                let raw = c.get("jsonPath").and_then(|s| s.as_str()).unwrap_or("");
                if !name.is_empty() {
                    if let Some(jp) = normalize_json_path(raw) {
                        printer_cols.push(PrinterCol { name, json_path: jp });
                    }
                }
            }
        }
    }
    if printer_cols.is_empty() {
        if let Some(cols) = v.get("spec").and_then(|s| s.get("additionalPrinterColumns")).and_then(|c| c.as_array()) {
            for c in cols {
                let name = c.get("name").and_then(|s| s.as_str()).unwrap_or("").to_string();
                let raw = c.get("jsonPath").and_then(|s| s.as_str()).unwrap_or("");
                if !name.is_empty() {
                    if let Some(jp) = normalize_json_path(raw) {
                        printer_cols.push(PrinterCol { name, json_path: jp });
                    }
                }
            }
        }
    }

    // Projected paths: prefer printer columns; else derive from OpenAPI schema
    let mut projected_paths: Vec<PathSpec> = Vec::new();
    if !printer_cols.is_empty() {
        for (i, c) in printer_cols.iter().enumerate() {
            projected_paths.push(PathSpec { id: i as u32, json_path: c.json_path.clone() });
            if projected_paths.len() >= 6 { break; }
        }
    } else {
        // Try to locate openAPIV3Schema for the chosen version
        let mut schema_opt: Option<&serde_json::Value> = None;
        if let Some(ver) = versions.iter().find(|ver| ver.get("name").and_then(|s| s.as_str()) == Some(served_version.as_str())) {
            schema_opt = ver.get("schema").and_then(|s| s.get("openAPIV3Schema"));
        }
        if schema_opt.is_none() {
            // legacy v1beta1 location
            schema_opt = v.get("spec").and_then(|s| s.get("validation")).and_then(|s| s.get("openAPIV3Schema"));
        }

        let candidates = schema_opt
            .and_then(|s| derive_projected_from_openapi(s))
            .unwrap_or_else(|| vec![
                "spec.name".to_string(),
                "spec.namespace".to_string(),
            ]);
        for (i, p) in candidates.iter().take(6).enumerate() {
            projected_paths.push(PathSpec { id: i as u32, json_path: p.clone() });
        }
    }

    Ok(Some(CrdSchema { served_version, printer_cols, projected_paths, flags: SchemaFlags::default() }))
}

/// Simple projector built from a `CrdSchema` projected paths.
#[derive(Clone)]
pub struct SchemaProjector {
    specs: Vec<PathSpec>,
}

impl SchemaProjector {
    pub fn new(specs: Vec<PathSpec>) -> Self { Self { specs } }

    /// Extract a scalar string from a JSON value following a minimal json-path-like grammar:
    /// dot fields and single `[index]` on a segment, e.g., `spec.dnsNames[0]`.
    fn extract_path<'a>(root: &'a serde_json::Value, path: &str) -> Option<&'a serde_json::Value> {
        use serde_json::Value;
        let mut cur = root;
        for seg in path.split('.') {
            if seg.is_empty() { return None; }
            // Handle optional [index]
            let (key, idx_opt) = if let Some(brk) = seg.find('[') {
                let end = seg.get(brk+1..)?.find(']')? + brk + 1;
                let key = &seg[..brk];
                let idx_str = &seg[brk+1..end];
                let idx: usize = idx_str.parse().ok()?;
                (key, Some(idx))
            } else {
                (seg, None)
            };
            match cur {
                Value::Object(map) => {
                    cur = map.get(key)?;
                }
                _ => return None,
            }
            if let Some(i) = idx_opt {
                match cur {
                    Value::Array(arr) => { cur = arr.get(i)?; }
                    _ => return None,
                }
            }
        }
        Some(cur)
    }
}

impl Projector for SchemaProjector {
    fn project(&self, raw: &serde_json::Value) -> SmallVec<[(u32, String); 8]> {
        let mut out: SmallVec<[(u32, String); 8]> = SmallVec::new();
        for spec in self.specs.iter() {
            if let Some(v) = Self::extract_path(raw, &spec.json_path) {
                let s = match v {
                    serde_json::Value::String(s) => s.clone(),
                    serde_json::Value::Number(n) => n.to_string(),
                    serde_json::Value::Bool(b) => b.to_string(),
                    _ => continue,
                };
                out.push((spec.id, s));
                if out.len() >= 8 { break; }
            }
        }
        out
    }
}

impl CrdSchema {
    pub fn projector(&self) -> SchemaProjector {
        SchemaProjector::new(self.projected_paths.clone())
    }
}

// Feature-gated JSON Schema validation utilities
#[cfg(feature = "jsonschema-validate")]
pub mod validate {
    use super::*;
    use anyhow::{Context, Result};
    use jsonschema::{Draft, JSONSchema};

    #[derive(Debug, Clone, Serialize, Deserialize)]
    pub struct ValidationIssue {
        pub path: String,
        pub error: String,
        pub hint: Option<String>,
    }

    async fn fetch_openapi_schema(gvk_key: &str) -> Result<Option<serde_json::Value>> {
        use kube::Client;
        use k8s_openapi::apiextensions_apiserver::pkg::apis::apiextensions::v1 as apiextv1;
        use kube::{Api, api::ListParams};

        let client = Client::try_default().await?;
        let parts: Vec<_> = gvk_key.split('/').collect();
        let (group, version, kind) = match parts.as_slice() {
            [version, kind] => ("", *version, *kind),
            [group, version, kind] => (*group, *version, *kind),
            _ => return Err(anyhow::anyhow!("invalid gvk key: {}", gvk_key)),
        };
        if group.is_empty() { return Ok(None); }

        let api: Api<apiextv1::CustomResourceDefinition> = Api::all(client.clone());
        let crds = api.list(&ListParams::default()).await.context("listing CustomResourceDefinitions")?;
        let mut v_opt: Option<serde_json::Value> = None;
        for crd in crds {
            let v = serde_json::to_value(&crd)?;
            let spec = match v.get("spec") { Some(s) => s, None => continue };
            let g = spec.get("group").and_then(|s| s.as_str()).unwrap_or("");
            let k = spec.get("names").and_then(|n| n.get("kind")).and_then(|s| s.as_str()).unwrap_or("");
            if g == group && k == kind { v_opt = Some(v); break; }
        }
        let v = match v_opt { Some(v) => v, None => return Err(anyhow::anyhow!("CRD not found for {}", gvk_key)) };
        let versions = v.get("spec").and_then(|s| s.get("versions")).and_then(|vv| vv.as_array()).cloned().unwrap_or_default();
        let mut served_version = version.to_string();
        if !versions.is_empty() {
            if let Some(storage_v) = versions.iter().find(|ver| ver.get("storage").and_then(|b| b.as_bool()).unwrap_or(false)) {
                if let Some(name) = storage_v.get("name").and_then(|s| s.as_str()) { served_version = name.to_string(); }
            } else if let Some(served_v) = versions.iter().find(|ver| ver.get("served").and_then(|b| b.as_bool()).unwrap_or(false)) {
                if let Some(name) = served_v.get("name").and_then(|s| s.as_str()) { served_version = name.to_string(); }
            }
        }
        // Find openAPIV3Schema in chosen version or legacy location
        let mut schema_opt: Option<serde_json::Value> = None;
        if let Some(ver) = versions.iter().find(|ver| ver.get("name").and_then(|s| s.as_str()) == Some(served_version.as_str())) {
            if let Some(s) = ver.get("schema").and_then(|s| s.get("openAPIV3Schema")).cloned() { schema_opt = Some(s); }
        }
        if schema_opt.is_none() {
            schema_opt = v.get("spec").and_then(|s| s.get("validation")).and_then(|s| s.get("openAPIV3Schema")).cloned();
        }
        Ok(schema_opt)
    }

    /// Validate a YAML document against the CRD's `openAPIV3Schema` for the given GVK.
    /// Returns a list of human-friendly issues; empty on success.
    pub async fn validate_yaml_for_gvk(gvk_key: &str, yaml: &str) -> Result<Vec<ValidationIssue>> {
        let schema = match fetch_openapi_schema(gvk_key).await? {
            Some(s) => s,
            None => return Ok(vec![ValidationIssue { path: "".into(), error: "no CRD schema available for builtin kind".into(), hint: None }]),
        };
        let json: serde_json::Value = match serde_yaml::from_str::<serde_yaml::Value>(yaml) {
            Ok(v) => serde_json::to_value(v).context("converting YAML to JSON")?,
            Err(e) => return Ok(vec![ValidationIssue { path: "".into(), error: format!("YAML parse error: {}", e), hint: Some("check indentation and syntax".into()) }]),
        };
        // JSONSchema 0.17 requires a 'static schema reference; leak for now (acceptable for CLI usage).
        let schema_static: &'static serde_json::Value = Box::leak(Box::new(schema));
        let compiled = JSONSchema::options().with_draft(Draft::Draft7).compile(schema_static).context("compiling CRD JSON Schema")?;
        let mut issues: Vec<ValidationIssue> = Vec::new();
        let result = compiled.validate(&json);
        if let Err(errors) = result {
            for err in errors {
                let path = err.instance_path.to_string();
                let error = err.to_string();
                // Keep hints minimal to avoid depending on specific jsonschema internals
                let hint = if error.contains("required property") {
                    Some("missing required field".into())
                } else if error.contains("type:") || error.contains("expected type") {
                    Some("mismatched type".into())
                } else if error.contains("enum") {
                    Some("value not in allowed set".into())
                } else {
                    None
                };
                issues.push(ValidationIssue { path, error, hint });
            }
        }
        Ok(issues)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn normalize_json_path_accepts_simple_paths() {
        assert_eq!(normalize_json_path(".spec.foo"), Some("spec.foo".to_string()));
        assert_eq!(normalize_json_path("spec.dnsNames[0]"), Some("spec.dnsNames[0]".to_string()));
        assert_eq!(normalize_json_path("").is_none(), true);
        assert_eq!(normalize_json_path("spec.*").is_none(), true);
        assert_eq!(normalize_json_path("spec.foo[0][1]").is_some(), false);
    }

    #[test]
    fn projector_extracts_scalars() {
        let json = serde_json::json!({
            "spec": {
                "dnsNames": ["a.example.com", "b.example.com"],
                "replicas": 3,
                "paused": false
            }
        });
        let specs = vec![
            PathSpec { id: 1, json_path: "spec.dnsNames[0]".to_string() },
            PathSpec { id: 2, json_path: "spec.replicas".to_string() },
            PathSpec { id: 3, json_path: "spec.paused".to_string() },
        ];
        let pj = SchemaProjector::new(specs);
        let out = pj.project(&json);
        assert!(out.contains(&(1, "a.example.com".to_string())));
        assert!(out.contains(&(2, "3".to_string())));
        assert!(out.contains(&(3, "false".to_string())));
    }
}

fn derive_projected_from_openapi(schema: &serde_json::Value) -> Option<Vec<String>> {
    use serde_json::Value;
    let mut out: Vec<String> = Vec::new();
    let spec_props = schema.get("properties")?.get("spec")?.get("properties")?.as_object()?;

    fn is_scalar_type(ty: &str) -> bool { matches!(ty, "string" | "integer" | "number" | "boolean") }

    fn walk_object(obj: &serde_json::Map<String, Value>, base: &str, depth: usize, out: &mut Vec<String>) {
        if depth > 3 || out.len() >= 16 { return; }
        for (k, v) in obj.iter() {
            let path = if base.is_empty() { k.clone() } else { format!("{}.{}", base, k) };
            let ty = v.get("type").and_then(|s| s.as_str()).unwrap_or("");
            match ty {
                "object" => {
                    if let Some(props) = v.get("properties").and_then(|p| p.as_object()) {
                        walk_object(props, &path, depth+1, out);
                    }
                }
                "array" => {
                    if let Some(items) = v.get("items") {
                        let ity = items.get("type").and_then(|s| s.as_str()).unwrap_or("");
                        if is_scalar_type(ity) {
                            out.push(format!("{}[0]", path));
                        } else if ity == "object" {
                            if let Some(props) = items.get("properties").and_then(|p| p.as_object()) {
                                walk_object(props, &format!("{}[0]", path), depth+1, out);
                            }
                        }
                    }
                }
                t if is_scalar_type(t) => {
                    out.push(path);
                }
                _ => {}
            }
            if out.len() >= 16 { return; }
        }
    }

    walk_object(spec_props, "spec", 0, &mut out);

    if out.is_empty() { None } else { Some(out) }
}
</file>

<file path="crates/schema/Cargo.toml">
[package]
name = "orka-schema"
version = "0.0.0"
edition = "2021"
license = "MIT OR Apache-2.0"
description = "Orka CRD schema engine (Milestone 1 stub)"

[dependencies]
orka-core = { path = "../core" }
anyhow = { workspace = true }
thiserror = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
tracing = { workspace = true }
smallvec = { workspace = true }
kube = { workspace = true }
k8s-openapi = { workspace = true }
serde_yaml = { version = "0.9", optional = true }
jsonschema = { version = "0.17", optional = true }

[features]
jsonschema-validate = ["serde_yaml", "jsonschema"]
</file>

<file path="MILESTONE-0.md">
# Orka — Milestone 0 (Skeleton Backend)

> Goal: get a boring, predictable skeleton that does less but never lies. Make it correct and observable first; then fast. Keep the moving parts few. Ship a single binary that can discover, list, and watch one CRD reliably.

---

## Scope (M0)

- kube client and discovery of all served resources (incl. CRDs).
- One CRD watcher end-to-end (namespaced or cluster-scoped), coalescing queue, ingest, and RCU snapshots.
- Minimal in-RAM representations (LiteObj subset) aimed at listing and watching.
- CLI: `discover`, `ls`, `watch` implemented against the in-process backend (no RPC yet).
- Logging, backpressure, and simple metrics; graceful shutdown.

Non-goals (M0): schema engine, search index, apply/diffs, persistence, UI, multi-cluster.

Success = commands behave deterministically on a real cluster and under replay, with bounded memory/CPU.

---

## Architecture Slice

```
+--------- kube API ---------+
        list/watch (1 GVK)
               │
               ▼
        Coalescing Queue  →  Ingest Thread  →  ArcSwap<WorldSnapshot>
               ▲                                   ▲
               │                                   │
            orkactl --------------------------→ read-only views
```

Rules:

- No blocking in the watch path; coalesce before ingest.
- Reads are lock-free via `arc-swap` snapshots.
- Bounded memory: fixed-capacity queues; drop oldest on pressure.

---

## Workspace Layout (M0)

- `crates/core`: types, tiny errors, feature flags.
- `crates/kubehub`: kube client, discovery, single-GVK watcher.
- `crates/store`: Delta, Coalescer, WorldBuilder, WorldSnapshot.
- `crates/cli` (binary `orkactl`): `discover | ls | watch` using in-process backend.

Later crates (schema, search, apply, rpc, persist) are out of scope for M0.

---

## Detailed Tasks

Progress summary (as of M0 scaffold implemented):

- [x] Bootstrap: workspace, tracing, feature flags (strip-managed-fields on; persist-sqlite stubbed)
- [x] Discovery: list served resources via kube discovery; CLI `discover` + `-o json`
- [~] Watcher: list+watch for a selected GVK works; bookmarks/backoff defaulted; periodic relist not wired yet
- [~] Coalescer: UID coalescing with bounded capacity and drop counter; metrics not exported yet
- [x] Store & Snapshots: builder applies batches; `arc-swap` snapshots for lock-free reads
- [x] CLI: `discover`, `ls`, `watch` using in-process backend; human + JSON output
- [~] Observability & Limits: `ORKA_LOG` + `ORKA_QUEUE_CAP` + `ORKA_RELIST_SECS`; basic tracing; graceful shutdown DONE; metrics export TODO
- [ ] Tests & Replay: unit tests and replay fixture pending
- [ ] Docs & Examples: README and fixtures pending

1) Bootstrap (Day 0–1) — Status: Completed
- Cargo workspace with crates above; CI with fmt/clippy/test.
- Minimal `tracing` setup with env filter; error handling via `anyhow`/`thiserror`.
- Feature flags: `strip-managed-fields` (on), `persist-sqlite` (off), others stubbed.

2) Discovery (Day 1) — Status: Completed
- Use `kube::discovery::Discovery` to list served resources (incl. CRDs).
- Print canonical key per resource: `group/version/kind (namespaced|cluster)`.
- Accept a `--prefer-crd` flag to select the first CRD automatically for M0 demos.

3) Watcher (Day 2–3) — Status: Partial
- For the selected GVK, start `Api<DynamicObject>` with list+watch.
- Set `watch` with bookmarks and a small, bounded internal channel.
- Periodic relist every N minutes; detect resourceVersion staleness and recover.

4) Coalescer (Day 3) — Status: Completed
- Map `uid` → latest `Delta` with FIFO `VecDeque<uid>` order, capacity N (configurable).
- Insert/update collapses multiple changes into one; when full, drop oldest with a counter.
- Export metrics: `coalescer_dropped_total`, `coalescer_len`. (Completed)

5) Store & Snapshots (Day 3–4) — Status: Completed
- `WorldBuilder` applies batches of `Delta` and produces `Arc<WorldSnapshot>`.
- Use `arc_swap::ArcSwap<WorldSnapshot>` for readers; swap on every non-empty batch.
- Snapshot contains only what CLI needs for `ls` (a Vec of `LiteObj` for the GVK).

6) CLI (Day 4–5) — Status: Completed
- `orkactl discover`: print served resources.
- `orkactl ls gvk --ns default`: read current snapshot and render a table.
- `orkactl watch gvk --ns default`: subscribe to snapshot swaps and print concise lines.
- JSON output via `-o json` for machine checks; default human output.

7) Observability & Limits (Day 5) — Status: Completed
- `tracing`: targets per crate; `info` for lifecycle, `debug` for delta counts.
- Env vars: `ORKA_QUEUE_CAP`, `ORKA_RELIST_SECS`, `ORKA_LOG`.
- Graceful shutdown: Ctrl-C triggers stop of watcher and ingest; flush a final snapshot. (Completed)
- Metrics export: Prometheus at `ORKA_METRICS_ADDR` with `coalescer_*`, `ingest_*`, `snapshot_*`. (Completed)

8) Tests & Replay (Day 5–6) — Status: Completed
- Unit: coalescer behavior (coalesce, drop), builder apply, JSON shaping of `LiteObj`. (Completed)
- Replay: deterministic test simulating deltas, asserting snapshot contents. (Completed)
- Optional kind-based integration (manual gate): skip in CI if no cluster. (Deferred)

9) Docs & Examples (Day 6) — Status: Completed
- Add a short README for `orkactl` with copy-paste sessions. (Completed)
- Provide a tiny replay fixture under `benches/fixtures/` (few dozen deltas). (Deferred; tests embed small fixture)

---

## Minimal Types (M0)

```rust
// crates/core
pub type Uid = [u8; 16];

pub enum DeltaKind { Applied, Deleted }

pub struct Delta {
    pub uid: Uid,
    pub kind: DeltaKind,
    pub raw: serde_json::Value, // for now, keep small; drop large fields optionally
}

pub struct LiteObj {
    pub uid: Uid,
    pub namespace: Option<String>,
    pub name: String,
    pub creation_ts: i64,
}

pub struct WorldSnapshot {
    pub epoch: u64,
    pub items: Vec<LiteObj>, // for the selected GVK only in M0
}
```

Notes:
- Convert from `DynamicObject` to `LiteObj` once, at ingest.
- Under `strip-managed-fields`, remove `.metadata.managedFields` immediately.

---

## Coalescer Sketch

```rust
pub struct Coalescer {
    map: rustc_hash::FxHashMap<Uid, Delta>,
    order: std::collections::VecDeque<Uid>,
    cap: usize,
    dropped: u64,
}
```

- `push(delta)`: if new uid and full, pop_front → increment `dropped`; insert; else update.
- `drain_for(ms)`: collect up to T milliseconds of arrivals or until empty.

---

## Ingest Loop

- Single thread pulls batches from Coalescer, applies to `WorldBuilder`, then swaps snapshot.
- Swap cadence goal: ≤ 50 ms in M0 (loose), we’ll tighten later.
- On errors, log and continue; builder must be total (never panic on user data).

---

## CLI Specs (M0)

- `orkactl discover`
  - Output: one resource per line: `group/version • Kind • namespaced|cluster`
- `orkactl ls group/version/kind --ns <ns>`
  - Output: table of `NAMESPACE NAME AGE` (AGE as short human string).
- `orkactl watch group/version/kind --ns <ns>`
  - Output: `+` on add/update, `-` on delete, with `ns/name`.

Examples:

```
$ orkactl discover
apiextensions.k8s.io/v1 • CustomResourceDefinition • cluster
cert-manager.io/v1 • Certificate • namespaced
...

$ orkactl ls cert-manager.io/v1/Certificate --ns prod
NAMESPACE   NAME                 AGE
prod        payments-cert        3d4h
...

$ orkactl watch cert-manager.io/v1/Certificate --ns prod
+ prod/payments-cert
- prod/old-cert
```

---

## Config & Defaults

- Kubeconfig detection via `kube` defaults (env var first, then files).
- Namespace default: current context namespace; override with `--ns`.
- For M0 demos, if no CRDs present, fallback to a built-in (e.g., `v1/ConfigMap`).

---

## Performance Targets (M0)

- Able to ingest and hold 5k objects with ≤ 200 MB RSS.
- Coalescer never blocks producer; dropped counter visible.
- List/print of snapshot under 30 ms for 5k entries (single thread).

---

## Risks & Mitigations

- No CRDs in cluster → builtin fallback; document clearly.
- RBAC incomplete → exit with a clear error and suggested `kubectl auth can-i` check.
- Watch staleness → periodic relist; backoff on errors; resume from RV when possible.

---

## Definition of Done (M0)

- `orkactl discover | ls | watch` work against a kind cluster and minikube.
- Replay tests pass deterministically; unit tests cover coalescer and builder edge cases.
- Swap-based reads are lock-free; no panics under malformed objects.
- Documented flags and example sessions; short README in `crates/cli`.

---

## Implementation Order (Checklist)

- [x] Workspace scaffold (`core`, `kubehub`, `store`, `cli`).
- [x] Discovery prints all served resources.
- [~] Select target GVK (`--prefer-crd`, fallback builtin). (Flag accepted; selection remains manual)
- [x] Start watcher with periodic relist.
- [x] Implement Coalescer with drops and metrics.
- [x] WorldBuilder + ArcSwap snapshot; convert to `LiteObj`.
- [x] `orkactl ls` from snapshot; `watch` prints changes.
- [x] Replay tests + unit tests.
- [x] Basic metrics/logging; graceful shutdown.

> Keep it simple. If a feature wants more complexity, tell it to wait for M1.
</file>

<file path="MILESTONE-2.md">
# Orka — Milestone 2 (Apply & Persistence)

Goal: make edits real. Dry‑run and server‑side apply (SSA). Persist last‑applied snapshots in SQLite. Show minimal diffs. Keep it simple and fast.

---

## Scope (M2)

- Apply engine: `--dry-run` and real SSA with `fieldManager="orka"`.
- Validation: reuse M1 `jsonschema-validate` (optional feature), friendly errors.
- Persistence: SQLite store for last‑applied (keep up to 3 per UID), optional schema cache.
- Diff: minimal JSON/YAML diff vs live and vs last‑applied; human summary.
- CLI: `edit -f file.yaml --dry-run|--apply`, `diff -f file.yaml`, `last-applied get`.

Non‑goals (M2): RPC surface, UI, bulk imports, advanced 3‑way merging beyond SSA semantics, multi‑object transactions.

Success = edit flows are reliable and predictable; last‑applied survives restarts; diffs are useful, not perfect.

---

## Architecture Slice

```
YAML  ──► Validate ──► Diff (live/last) ──► SSA (server)
                           │                 │
                           └──────── Persist ◄┘
```

Rules:

- Always validate before apply if feature is enabled.
- Always persist last‑applied after successful SSA, keeping the last 3.
- Diffs are pragmatic: key additions/removals/changes; skip noisy managed fields.

---

## Workspace Additions (M2)

- `crates/persist`: SQLite adapter (rusqlite), zstd blobs for YAML (optional feature).
- `crates/apply`: dry‑run + SSA helpers, diff renderer, field pruning.
- Extend `crates/cli`: `edit`, `diff`, `last-applied` subcommands.

---

## Minimal Types (M2)

```rust
// crates/persist
pub struct LastApplied { pub uid: [u8;16], pub rv: String, pub ts: i64, pub yaml_zstd: Vec<u8> }
pub trait Store { fn put_last(&self, la: LastApplied) -> Result<()>; fn get_last(&self, uid: [u8;16]) -> Result<Vec<LastApplied>>; }

// crates/apply
pub struct ApplyResult { pub dry_run: bool, pub applied: bool, pub new_rv: Option<String>, pub warnings: Vec<String>, pub summary: DiffSummary }
pub struct DiffSummary { pub adds: usize, pub updates: usize, pub removes: usize }
```

---

## CLI Specs (M2)

- `orkactl edit -f file.yaml [--ns NS] [--dry-run | --apply] [--validate]`
  - Output: diff summary (when `--dry-run`), or `applied rv=<rv>`.
- `orkactl diff -f file.yaml [--ns NS]`
  - Output: minimal diff vs live and vs last‑applied (two panes or sections).
- `orkactl last-applied get <gvk> <name> [--ns NS] [--limit N]`
  - Output: timestamps and RVs; `-o json` to dump payload if requested.

Env:

- `ORKA_DB_PATH` (default: `~/.orka/orka.db`)
- `ORKA_ZSTD_LEVEL` (default: 3)

---

## Detailed Tasks

1) Persistence
- Schema: `last_applied(uid BLOB, rv TEXT, ts INTEGER, yaml BLOB)`; index on `(uid, ts DESC)`.
- APIs: `put_last`, `get_last(uid, limit)`, basic migrations, corruption handling.

2) Apply
- Load live object; compute minimal diff (prune `managedFields`, timestamps).
- Dry‑run: server `?dryRun=All`; render diff summary; no persist.
- SSA: set `fieldManager=orka`, apply; on success persist last‑applied.

3) CLI
- `edit -f`: read from file/stdin; detect GVK, name, ns; options `--validate`, `--dry-run|--apply`.
- `diff -f`: compare given YAML vs live and vs last‑applied (if present).
- `last-applied get`: list recent entries for a resource.

4) Tests
- Unit: persist put/get/rotate; diff pruning; apply error mapping.
- (Optional) Smoke: dry‑run flow behind feature flag requiring cluster.

5) Observability
- Counters: `apply_attempts`, `apply_ok`, `apply_err`, `apply_dry_ok`.
- Histograms: `persist_put_ms`, `persist_get_ms`, `apply_latency_ms`.

---

## Performance Targets (M2)

- Dry‑run p99 ≤ 150 ms (small/medium CRs).
- Apply p99 ≤ 300 ms.
- Persist ops p50 ≤ 2 ms; ≤ 5 ms p95.
- DB size: ≤ 50 MB default cap; prune older than 3 entries per UID.

---

## Risks & Mitigations

- RBAC / SSA denied → clear error messages; suggest `--dry-run`.
- Drift vs last‑applied → show both live and last diffs; don’t block apply.
- DB corruption → recreate DB; log and proceed without blocking applies.

---

## Definition of Done (M2)

- `edit --dry-run` shows a sane diff; `edit --apply` succeeds and persists last‑applied (rotates to 3).
- `diff -f` works without side effects.
- Persist unit tests green; no panics on malformed input or DB issues.
- Metrics visible; knobs documented.

---

## Implementation Order (Checklist)

- [x] Add `crates/persist` (rusqlite + optional zstd), schema + APIs.
- [x] Add `crates/apply`: dry‑run + SSA helpers; diff pruning/summary.
- [x] Wire CLI: `edit`, `diff`, `last-applied` subcommands (+ JSON output).
- [x] Integrate persist: save last‑applied after SSA; keep latest 3.
- [x] Unit tests: persist (put/get/rotate).
- [x] Unit tests: diff pruning; error mapping.
- [x] Docs: CLI usage + env in `crates/cli/README.md`.
- [x] Metrics: counters + histograms for apply/persist paths.

### Progress Notes

- Persist: `SqliteStore` with table `last_applied(uid BLOB, rv TEXT, ts INTEGER, yaml BLOB)` and index `(uid, ts DESC)`; rotates to keep latest 3 per UID. Optional zstd compression behind feature flag.
- Apply: SSA and server dry‑run with `fieldManager=orka`; prunes noisy fields (`managedFields`, `resourceVersion`, `status`, `generation`, `creationTimestamp`). Emits `DiffSummary { adds, updates, removes }`.
- CLI: implemented `edit -f`, `diff -f`, and `last-applied get` (supports `-o json`); `--validate` is feature‑gated via `validate` feature enabling schema JSONSchema checks.
- Metrics: `apply_attempts`, `apply_ok`, `apply_err`, `apply_dry_ok`; histograms `apply_latency_ms`, `persist_put_ms`, `persist_get_ms`.
- Docs: updated usage and env in `crates/cli/README.md`.
- Example: `examples/configmap.yaml` for quick dry‑run/apply.

> Do the simplest thing that works. If it’s not critical for end‑to‑end edits, it waits.
</file>

<file path="crates/core/src/lib.rs">
//! Orka core types (Milestone 0)

#![forbid(unsafe_code)]

use serde::{Deserialize, Serialize};
use smallvec::SmallVec;

pub type Uid = [u8; 16];

#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq)]
pub enum DeltaKind {
    Applied,
    Deleted,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Delta {
    pub uid: Uid,
    pub kind: DeltaKind,
    /// Raw object (possibly stripped of oversized fields under feature flags)
    pub raw: serde_json::Value,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LiteObj {
    pub uid: Uid,
    pub namespace: Option<String>,
    pub name: String,
    pub creation_ts: i64,
    /// Projected fields for search/listing (M1). For M0, this may be empty.
    pub projected: SmallVec<[(u32, String); 8]>,
    /// Kubernetes labels as key/value pairs.
    pub labels: SmallVec<[(String, String); 8]>,
    /// Kubernetes annotations as key/value pairs.
    pub annotations: SmallVec<[(String, String); 4]>,
}

#[derive(Debug, Clone, Serialize, Deserialize, Default)]
pub struct WorldSnapshot {
    pub epoch: u64,
    /// For Milestone 0, we hold items only for the selected GVK.
    pub items: Vec<LiteObj>,
}

pub mod prelude {
    pub use super::{Delta, DeltaKind, LiteObj, Uid, WorldSnapshot, Projector, ProjectedEntry};
}

/// Entry representing a projected field: `(PathId, RenderedValue)`
pub type ProjectedEntry = (u32, String);

/// Projector takes a raw JSON object and yields rendered projected scalars.
pub trait Projector: Send + Sync {
    fn project(&self, raw: &serde_json::Value) -> SmallVec<[(u32, String); 8]>;
}

// ---- M3 sharding primitives ----

/// Shard selection key composed from logical dimensions.
///
/// - `gvk_id`: a stable identifier for the Group/Version/Kind stream (implementation-defined)
/// - `ns_bucket`: namespace bucket (modulo or exact mapping), usually in [0, ORKA_SHARDS)
#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq, Hash)]
pub struct ShardKey {
    pub gvk_id: u32,
    pub ns_bucket: u16,
}

/// Planner responsible for mapping an object into a shard key.
/// Implementations can choose modulo bucketing or exact namespace mapping.
pub trait ShardPlanner: Send + Sync {
    /// Compute shard key for a given GVK and optional namespace.
    fn plan(&self, gvk_id: u32, namespace: Option<&str>) -> ShardKey;
}

/// Default planner: modulo bucketing by namespace using a simple FNV-1a hash.
#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
pub struct ModuloNsPlanner { buckets: u16 }

impl ModuloNsPlanner {
    pub fn new(buckets: usize) -> Self {
        Self { buckets: buckets.max(1).min(u16::MAX as usize) as u16 }
    }
}

impl ShardPlanner for ModuloNsPlanner {
    fn plan(&self, gvk_id: u32, namespace: Option<&str>) -> ShardKey {
        let ns = namespace.unwrap_or("");
        let mut h: u64 = 0xcbf29ce484222325; // 64-bit FNV-1a offset
        for b in ns.as_bytes() { h ^= *b as u64; h = h.wrapping_mul(0x100000001b3); }
        let ns_bucket = if self.buckets <= 1 { 0 } else { (h as u16) % self.buckets };
        ShardKey { gvk_id, ns_bucket }
    }
}
</file>

<file path="crates/search/Cargo.toml">
[package]
name = "orka-search"
version = "0.0.0"
edition = "2021"
license = "MIT OR Apache-2.0"
description = "Orka RAM search index (Milestone 1 stub)"

[dependencies]
orka-core = { path = "../core" }
anyhow = { workspace = true }
rustc-hash = { workspace = true }
fuzzy-matcher = { workspace = true }
smallvec = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
metrics = { workspace = true }
tracing = { workspace = true }

[dev-dependencies]
orka-store = { path = "../store" }
tokio = { workspace = true, features = ["rt-multi-thread", "macros", "time"] }
</file>

<file path="crates/store/Cargo.toml">
[package]
name = "orka-store"
version = "0.0.0"
edition = "2021"
license = "MIT OR Apache-2.0"
description = "Orka in-RAM store (Milestone 0)"

[dependencies]
orka-core = { path = "../core" }
arc-swap = { workspace = true }
rustc-hash = { workspace = true }
anyhow = { workspace = true }
tracing = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
tokio = { workspace = true }
chrono = { workspace = true }
metrics = { workspace = true }
smallvec = { workspace = true }
</file>

<file path="crates/cli/README.md">
# orkactl (Milestone 3)

CLI for Orka’s in-memory backend, with schema discovery for CRDs and a lightweight RAM search index. Commands use an in-process backend and the Kubernetes API via your current kubeconfig.

## Commands

- `orkactl discover`: list served resources (including CRDs)
- `orkactl ls <group/version/kind> [--ns <ns>]`: list objects from the latest snapshot
- `orkactl watch <group/version/kind> [--ns <ns>]`: stream changes as +/- lines
- `orkactl schema <group/version/kind>`: show CRD served version, printer columns, and projected paths
- `orkactl search <group/version/kind> "query" [--ns <ns>] [--limit N] [--max-candidates N] [--min-score F] [--explain]`: search current snapshot
- `orkactl edit -f file.yaml [--ns <ns>] [--dry-run|--apply] [--validate]`: dry-run or apply YAML via SSA
- `orkactl diff -f file.yaml [--ns <ns>]`: show minimal diffs vs live and last-applied
- `orkactl last-applied get <gvk> <name> [--ns <ns>] [--limit N] [-o json]`: inspect persisted last-applied snapshots
- `orkactl stats`: show runtime knobs (env-derived) and metrics endpoint

## Examples

```
$ orkactl discover
apiextensions.k8s.io/v1 • CustomResourceDefinition • cluster
v1 • ConfigMap • namespaced
...

$ ORKA_LOG=info ORKA_QUEUE_CAP=4096 orkactl ls v1/ConfigMap --ns default
NAMESPACE   NAME                 AGE
default     kube-root-ca.crt     3d4h

$ orkactl schema cert-manager.io/v1/Certificate
served: v1
printer-cols: Ready, Age, SecretName
projected: spec.dnsNames[0], status.conditions[?type==Ready].status, ...

$ orkactl search cert-manager.io/v1/Certificate "ns:prod k:Certificate payments" --limit 20
KIND      NAMESPACE/NAME                SCORE
Certificate  prod/payments-cert         0.86

$ orkactl watch v1/ConfigMap --ns default
+ default/my-app-config
- default/old-config

$ orkactl edit -f cm.yaml --dry-run
dry-run: +3 ~1 -0

$ orkactl edit -f cm.yaml --apply
applied rv=12345

$ orkactl diff -f cm.yaml
vs live: +2 ~0 -1
vs last: +1 ~1 -0

$ orkactl last-applied get v1/ConfigMap my-cm --ns default -o json
[
  { "ts": 1700000000, "rv": "12345", "yaml": "apiVersion: v1..." }
]
```

## Search Grammar

Typed filters combine with free text. Examples:

- `ns:<name>`: namespace filter
- `k:<Kind>` and `g:<group>`: restrict to a specific Kind or API group
- `label:<key>=<value>` or `label:<key>`: label value or existence
- `anno:<key>=<value>` or `anno:<key>`: annotation value or existence
- `field:<json.path>=<value>`: projected field exact match (paths from `schema`)

Free text is fuzzy-matched over `NAMESPACE/NAME` plus projected fields.

## Environment

- `ORKA_LOG`: tracing filter (e.g., `info`, `debug`, per-target is supported)
- `ORKA_QUEUE_CAP`: bounded channel capacity for deltas (default 2048)
- `ORKA_RELIST_SECS`: periodic relist interval for watchers (default 300)
- `ORKA_WATCH_BACKOFF_MAX_SECS`: maximum backoff for watch errors (default 30)
- `ORKA_SHARDS`: number of namespace buckets for ingest/search sharding (default 1)
- `ORKA_METRICS_ADDR`: if set to `host:port`, exposes Prometheus metrics at `/metrics`
- `ORKA_SEARCH_LIMIT`: default `--limit` for `search` (overridden by CLI)
- `ORKA_SEARCH_MAX_CANDIDATES`: cap candidate set size after typed filters
- `ORKA_SEARCH_MIN_SCORE`: minimum fuzzy score to include a hit
- `ORKA_MAX_LABELS_PER_OBJ`: cap number of labels captured per object (optional)
- `ORKA_MAX_ANNOS_PER_OBJ`: cap number of annotations captured per object (optional)
- `ORKA_MAX_POSTINGS_PER_KEY`: cap postings list size per key to control cardinality (optional)
- `ORKA_MAX_RSS_MB`: soft cap for in-memory snapshot size (approx); applies staged trimming (drop annotations → labels → projected) when exceeded
- `ORKA_MAX_INDEX_BYTES`: soft cap for index memory; prunes value postings when exceeded to preserve stability
- `ORKA_DB_PATH`: path to SQLite DB (default: `~/.orka/orka.db`)
- `ORKA_ZSTD_LEVEL`: compression level for persisted YAML when built with `zstd` feature (default: 3)
- `ORKA_DISABLE_APPLY_PREFLIGHT=1`: disable apply freshness guard (preflight GET)

## Notes

- Requires access to a Kubernetes cluster and RBAC to list/watch the selected kind.
- JSON output is available with `-o json` for most commands.
- Validation (YAML → JSON → JSON Schema) is available with CLI feature `validate` which enables schema crate feature `jsonschema-validate`.
</file>

<file path="crates/search/src/lib.rs">
//! Orka search (Milestones 1–3): lightweight in-RAM index and query over LiteObj.
//! M3 adds internal sharding (by namespace bucket) to scale candidates and keep
//! ranking stable across shards.

#![forbid(unsafe_code)]

use fuzzy_matcher::skim::SkimMatcherV2;
use fuzzy_matcher::FuzzyMatcher;
use orka_core::{WorldSnapshot, ShardPlanner};
use std::collections::HashMap;
use tracing::warn;

pub type DocId = u32;

#[derive(Debug, Clone, Copy)]
pub struct Hit { pub doc: DocId, pub score: f32 }

#[derive(Debug, Clone, serde::Serialize)]
pub struct SearchDebugInfo {
    pub total: usize,
    pub after_ns: usize,
    pub after_label_keys: usize,
    pub after_labels: usize,
    pub after_anno_keys: usize,
    pub after_annos: usize,
    pub after_fields: usize,
}

struct IndexShard {
    texts: Vec<String>,
    namespaces: Vec<String>,
    names: Vec<String>,
    uids: Vec<[u8; 16]>,
    projected: Vec<Vec<(u32, String)>>,
    // Map local doc index -> global doc index in the original snapshot
    doc_ids: Vec<usize>,
    label_post: HashMap<String, Vec<usize>>,    // key=value -> local doc indices
    anno_post: HashMap<String, Vec<usize>>,     // key=value -> local doc indices
    label_key_post: HashMap<String, Vec<usize>>,// key -> local doc indices
    anno_key_post: HashMap<String, Vec<usize>>, // key -> local doc indices
}

pub struct Index {
    // Global arrays used for tie-breaking and for mapping hits back to snapshot indices
    g_names: Vec<String>,
    g_namespaces: Vec<String>,
    g_uids: Vec<[u8; 16]>,
    // Field path ids shared across shards
    field_ids: HashMap<String, u32>,
    // Shards
    shards: Vec<IndexShard>,
    // Single-GVK metadata useful for typed filters (k:, g:) in M1
    kind: Option<String>,  // lowercased kind
    group: Option<String>, // lowercased group (empty for core)
}

#[derive(Debug, Clone, Copy, Default)]
pub struct SearchOpts {
    pub max_candidates: Option<usize>,
    pub min_score: Option<f32>,
}

impl Index {
    pub fn build_from_snapshot(snap: &WorldSnapshot) -> Self {
        Self::build_from_snapshot_with_meta(snap, None, None, None)
    }
    fn intersect_sorted(a: &Vec<usize>, b: &Vec<usize>) -> Vec<usize> {
        let mut i = 0usize;
        let mut j = 0usize;
        let mut out = Vec::new();
        while i < a.len() && j < b.len() {
            match a[i].cmp(&b[j]) {
                std::cmp::Ordering::Less => i += 1,
                std::cmp::Ordering::Greater => j += 1,
                std::cmp::Ordering::Equal => { out.push(a[i]); i += 1; j += 1; }
            }
        }
        out
    }

    pub fn build_from_snapshot_with_fields(
        snap: &WorldSnapshot,
        fields: Option<&[(String, u32)]>,
    ) -> Self {
        Self::build_from_snapshot_with_meta(snap, fields, None, None)
    }

    /// Build index, optionally providing field path ids and single-GVK metadata (kind, group).
    pub fn build_from_snapshot_with_meta(
        snap: &WorldSnapshot,
        fields: Option<&[(String, u32)]>,
        kind: Option<&str>,
        group: Option<&str>,
    ) -> Self {
        // Determine shard count from env (default 1)
        let shards_n: usize = std::env::var("ORKA_SHARDS").ok().and_then(|s| s.parse().ok()).unwrap_or(1).max(1);
        let mut shards: Vec<IndexShard> = (0..shards_n).map(|_| IndexShard {
            texts: Vec::new(),
            namespaces: Vec::new(),
            names: Vec::new(),
            uids: Vec::new(),
            projected: Vec::new(),
            doc_ids: Vec::new(),
            label_post: HashMap::new(),
            anno_post: HashMap::new(),
            label_key_post: HashMap::new(),
            anno_key_post: HashMap::new(),
        }).collect();

        let mut g_namespaces = Vec::with_capacity(snap.items.len());
        let mut g_names = Vec::with_capacity(snap.items.len());
        let mut g_uids = Vec::with_capacity(snap.items.len());
        let mut field_ids: HashMap<String, u32> = HashMap::new();
        if let Some(pairs) = fields { for (k, v) in pairs.iter() { field_ids.insert(k.clone(), *v); } }
        let postings_cap: Option<usize> = std::env::var("ORKA_MAX_POSTINGS_PER_KEY").ok().and_then(|s| s.parse::<usize>().ok());
        let mut truncated_keys_total: usize = 0;

        fn ns_bucket(ns: &str, shards: usize) -> usize {
            if shards <= 1 { return 0; }
            let mut h: u64 = 0xcbf29ce484222325;
            for b in ns.as_bytes() { h ^= *b as u64; h = h.wrapping_mul(0x100000001b3); }
            (h as usize) % shards
        }

        for (i, o) in snap.items.iter().enumerate() {
            let ns = o.namespace.as_deref().unwrap_or("");
            let sh = ns_bucket(ns, shards_n);

            // Global mirrors
            g_namespaces.push(ns.to_string());
            g_names.push(o.name.clone());
            g_uids.push(o.uid);

            // Build display text
            let mut display = String::new();
            if !ns.is_empty() { display.push_str(ns); display.push('/'); }
            display.push_str(&o.name);
            if !o.projected.is_empty() {
                display.push(' ');
                for (_id, val) in o.projected.iter() { display.push_str(val); display.push(' '); }
            }

            let shard = &mut shards[sh];
            let local_idx = shard.texts.len();
            shard.texts.push(display);
            shard.namespaces.push(ns.to_string());
            shard.names.push(o.name.clone());
            shard.uids.push(o.uid);
            shard.projected.push(o.projected.iter().map(|(id, val)| (*id, val.clone())).collect());
            shard.doc_ids.push(i);

            // labels/annotations postings (local indices)
            for (k, v) in o.labels.iter() {
                let key = format!("{}={}", k, v);
                let vec = shard.label_post.entry(key).or_default();
                if let Some(cap) = postings_cap { if vec.len() >= cap { truncated_keys_total += 1; } else { vec.push(local_idx); } } else { vec.push(local_idx); }
                let veck = shard.label_key_post.entry(k.clone()).or_default();
                if let Some(cap) = postings_cap { if veck.len() < cap { veck.push(local_idx); } } else { veck.push(local_idx); }
            }
            for (k, v) in o.annotations.iter() {
                let key = format!("{}={}", k, v);
                let vec = shard.anno_post.entry(key).or_default();
                if let Some(cap) = postings_cap { if vec.len() >= cap { truncated_keys_total += 1; } else { vec.push(local_idx); } } else { vec.push(local_idx); }
                let veck = shard.anno_key_post.entry(k.clone()).or_default();
                if let Some(cap) = postings_cap { if veck.len() < cap { veck.push(local_idx); } } else { veck.push(local_idx); }
            }
        }

        // Aggregate gauges and enforce soft cap
        metrics::gauge!("index_docs", snap.items.len() as f64);
        let mut approx_bytes: usize = 0;
        for sh in shards.iter() {
            approx_bytes += sh.texts.iter().map(|s| s.len()).sum::<usize>();
            approx_bytes += sh.namespaces.iter().map(|s| s.len()).sum::<usize>();
            approx_bytes += sh.names.iter().map(|s| s.len()).sum::<usize>();
            approx_bytes += sh.projected.iter().map(|v| v.iter().map(|(_id, s)| s.len()).sum::<usize>()).sum::<usize>();
            approx_bytes += sh.label_post.values().map(|v| v.len() * std::mem::size_of::<usize>()).sum::<usize>();
            approx_bytes += sh.anno_post.values().map(|v| v.len() * std::mem::size_of::<usize>()).sum::<usize>();
        }
        if let Some(cap) = std::env::var("ORKA_MAX_INDEX_BYTES").ok().and_then(|s| s.parse::<usize>().ok()) {
            if approx_bytes > cap {
                warn!(approx_bytes, cap, "index_bytes exceeds ORKA_MAX_INDEX_BYTES; pruning value postings");
                // Drop value postings (label_post, anno_post) to reduce memory footprint.
                // Keep key existence postings so that label:app (key-only) filters still work.
                let mut dropped_keys = 0usize;
                for sh in shards.iter_mut() {
                    dropped_keys += sh.label_post.len();
                    dropped_keys += sh.anno_post.len();
                    sh.label_post.clear();
                    sh.anno_post.clear();
                }
                truncated_keys_total += dropped_keys;
                // Recompute approx_bytes after pruning
                approx_bytes = 0;
                for sh in shards.iter() {
                    approx_bytes += sh.texts.iter().map(|s| s.len()).sum::<usize>();
                    approx_bytes += sh.namespaces.iter().map(|s| s.len()).sum::<usize>();
                    approx_bytes += sh.names.iter().map(|s| s.len()).sum::<usize>();
                    approx_bytes += sh.projected.iter().map(|v| v.iter().map(|(_id, s)| s.len()).sum::<usize>()).sum::<usize>();
                    approx_bytes += sh.label_post.values().map(|v| v.len() * std::mem::size_of::<usize>()).sum::<usize>();
                    approx_bytes += sh.anno_post.values().map(|v| v.len() * std::mem::size_of::<usize>()).sum::<usize>();
                }
            }
        }
        metrics::gauge!("index_bytes", approx_bytes as f64);
        metrics::gauge!("index_postings_truncated_keys", truncated_keys_total as f64);

        Self {
            g_names,
            g_namespaces,
            g_uids,
            field_ids,
            shards,
            kind: kind.map(|s| s.to_ascii_lowercase()),
            group: group.map(|s| s.to_ascii_lowercase()),
        }
    }

    /// Build index with an optional shard planner. If provided, the planner's namespace bucket
    /// is used for partitioning; otherwise, modulo by namespace is applied. The `gvk_id` can
    /// be supplied by the caller to allow planner policies that consider GVK.
    pub fn build_from_snapshot_with_meta_planner(
        snap: &WorldSnapshot,
        fields: Option<&[(String, u32)]>,
        kind: Option<&str>,
        group: Option<&str>,
        planner: Option<&dyn ShardPlanner>,
        gvk_id: Option<u32>,
    ) -> Self {
        // Determine shard count from env (default 1)
        let shards_n: usize = std::env::var("ORKA_SHARDS").ok().and_then(|s| s.parse().ok()).unwrap_or(1).max(1);
        let mut shards: Vec<IndexShard> = (0..shards_n).map(|_| IndexShard {
            texts: Vec::new(),
            namespaces: Vec::new(),
            names: Vec::new(),
            uids: Vec::new(),
            projected: Vec::new(),
            doc_ids: Vec::new(),
            label_post: HashMap::new(),
            anno_post: HashMap::new(),
            label_key_post: HashMap::new(),
            anno_key_post: HashMap::new(),
        }).collect();

        let mut g_namespaces = Vec::with_capacity(snap.items.len());
        let mut g_names = Vec::with_capacity(snap.items.len());
        let mut g_uids = Vec::with_capacity(snap.items.len());
        let mut field_ids: HashMap<String, u32> = HashMap::new();
        if let Some(pairs) = fields { for (k, v) in pairs.iter() { field_ids.insert(k.clone(), *v); } }
        let postings_cap: Option<usize> = std::env::var("ORKA_MAX_POSTINGS_PER_KEY").ok().and_then(|s| s.parse::<usize>().ok());
        let mut truncated_keys_total: usize = 0;

        fn ns_bucket(ns: &str, shards: usize) -> usize {
            if shards <= 1 { return 0; }
            let mut h: u64 = 0xcbf29ce484222325;
            for b in ns.as_bytes() { h ^= *b as u64; h = h.wrapping_mul(0x100000001b3); }
            (h as usize) % shards
        }

        for (i, o) in snap.items.iter().enumerate() {
            let ns = o.namespace.as_deref().unwrap_or("");
            let sh = if let Some(pl) = planner { (pl.plan(gvk_id.unwrap_or(0), if ns.is_empty() { None } else { Some(ns) }).ns_bucket as usize) % shards_n } else { ns_bucket(ns, shards_n) };

            // Global mirrors
            g_namespaces.push(ns.to_string());
            g_names.push(o.name.clone());
            g_uids.push(o.uid);

            let shard = &mut shards[sh];
            let local_idx = shard.texts.len();
            // Doc text = name + labels + projected fields
            let mut t = String::new();
            t.push_str(&o.name);
            for (k, v) in o.labels.iter() { t.push(' '); t.push_str(k); t.push(':'); t.push_str(v); }
            for (_id, v) in o.projected.iter() { t.push(' '); t.push_str(v); }
            shard.texts.push(t);
            shard.namespaces.push(ns.to_string());
            shard.names.push(o.name.clone());
            shard.uids.push(o.uid);
            shard.doc_ids.push(i);
            shard.projected.push(o.projected.clone().into_iter().collect());

            for (k, v) in o.labels.iter() {
                let key = format!("{}={}", k, v);
                let vec = shard.label_post.entry(key).or_default();
                if let Some(cap) = postings_cap { if vec.len() >= cap { truncated_keys_total += 1; } else { vec.push(local_idx); } } else { vec.push(local_idx); }
                let veck = shard.label_key_post.entry(k.clone()).or_default();
                if let Some(cap) = postings_cap { if veck.len() < cap { veck.push(local_idx); } } else { veck.push(local_idx); }
            }
            for (k, v) in o.annotations.iter() {
                let key = format!("{}={}", k, v);
                let vec = shard.anno_post.entry(key).or_default();
                if let Some(cap) = postings_cap { if vec.len() >= cap { truncated_keys_total += 1; } else { vec.push(local_idx); } } else { vec.push(local_idx); }
                let veck = shard.anno_key_post.entry(k.clone()).or_default();
                if let Some(cap) = postings_cap { if veck.len() < cap { veck.push(local_idx); } } else { veck.push(local_idx); }
            }
        }

        // Aggregate gauges
        metrics::gauge!("index_docs", snap.items.len() as f64);
        let mut approx_bytes: usize = 0;
        for sh in shards.iter() {
            approx_bytes += sh.texts.iter().map(|s| s.len()).sum::<usize>();
            approx_bytes += sh.namespaces.iter().map(|s| s.len()).sum::<usize>();
            approx_bytes += sh.names.iter().map(|s| s.len()).sum::<usize>();
            approx_bytes += sh.projected.iter().map(|v| v.iter().map(|(_id, s)| s.len()).sum::<usize>()).sum::<usize>();
            approx_bytes += sh.label_post.values().map(|v| v.len() * std::mem::size_of::<usize>()).sum::<usize>();
            approx_bytes += sh.anno_post.values().map(|v| v.len() * std::mem::size_of::<usize>()).sum::<usize>();
        }
        metrics::gauge!("index_bytes", approx_bytes as f64);
        if let Some(cap) = std::env::var("ORKA_MAX_INDEX_BYTES").ok().and_then(|s| s.parse::<usize>().ok()) {
            if approx_bytes > cap { warn!(approx_bytes, cap, "index_bytes exceeds ORKA_MAX_INDEX_BYTES; consider increasing shards or capping postings"); }
        }
        metrics::gauge!("index_postings_truncated_keys", truncated_keys_total as f64);

        Self {
            g_names,
            g_namespaces,
            g_uids,
            field_ids,
            shards,
            kind: kind.map(|s| s.to_ascii_lowercase()),
            group: group.map(|s| s.to_ascii_lowercase()),
        }
    }

    pub fn search(&self, q: &str, limit: usize) -> Vec<Hit> {
        self.search_with_debug_opts(q, limit, SearchOpts::default()).0
    }

    pub fn search_with_debug(&self, q: &str, limit: usize) -> (Vec<Hit>, SearchDebugInfo) {
        self.search_with_debug_opts(q, limit, SearchOpts::default())
    }

    pub fn search_with_debug_opts(&self, q: &str, limit: usize, opts: SearchOpts) -> (Vec<Hit>, SearchDebugInfo) {
        let started = std::time::Instant::now();
        let matcher = SkimMatcherV2::default();
        let mut hits: Vec<Hit> = Vec::new();
        // Simple typed filters: ns:NAME, field:json.path=value, label:key=value, anno:key=value
        let mut ns_filter: Option<&str> = None;
        let mut kind_filters: Vec<String> = Vec::new();
        let mut group_filters: Vec<String> = Vec::new();
        let mut field_filters: Vec<(u32, String)> = Vec::new();
        let mut label_filters: Vec<String> = Vec::new();
        let mut anno_filters: Vec<String> = Vec::new();
        let mut label_key_filters: Vec<String> = Vec::new();
        let mut anno_key_filters: Vec<String> = Vec::new();
        let mut free_terms: Vec<&str> = Vec::new();
        for tok in q.split_whitespace() {
            if let Some(rest) = tok.strip_prefix("ns:") { ns_filter = Some(rest); continue; }
            if let Some(rest) = tok.strip_prefix("k:") { if !rest.is_empty() { kind_filters.push(rest.to_string()); continue; } }
            if let Some(rest) = tok.strip_prefix("g:") { if !rest.is_empty() { group_filters.push(rest.to_string()); continue; } }
            if let Some(rest) = tok.strip_prefix("field:") {
                if let Some(eq) = rest.find('=') {
                    let path = &rest[..eq];
                    let val = &rest[eq+1..];
                    if let Some(id) = self.field_ids.get(path) {
                        field_filters.push((*id, val.to_string()));
                        continue;
                    }
                }
            }
            if let Some(rest) = tok.strip_prefix("label:") {
                if rest.contains('=') { label_filters.push(rest.to_string()); continue; }
                if !rest.is_empty() { label_key_filters.push(rest.to_string()); continue; }
            }
            if let Some(rest) = tok.strip_prefix("anno:") {
                if rest.contains('=') { anno_filters.push(rest.to_string()); continue; }
                if !rest.is_empty() { anno_key_filters.push(rest.to_string()); continue; }
            }
            free_terms.push(tok);
        }
        let free_q = free_terms.join(" ");

        // Apply single-GVK kind/group filters early. Mismatch => no hits.
        if !kind_filters.is_empty() {
            let cur = self.kind.as_deref().unwrap_or("");
            let ok = kind_filters.iter().any(|k| k.eq_ignore_ascii_case(cur));
            if !ok { return (Vec::new(), SearchDebugInfo { total: self.g_names.len(), after_ns: 0, after_label_keys: 0, after_labels: 0, after_anno_keys: 0, after_annos: 0, after_fields: 0 }); }
        }
        if !group_filters.is_empty() {
            let cur = self.group.as_deref().unwrap_or("");
            let ok = group_filters.iter().any(|g| g.eq_ignore_ascii_case(cur));
            if !ok { return (Vec::new(), SearchDebugInfo { total: self.g_names.len(), after_ns: 0, after_label_keys: 0, after_labels: 0, after_anno_keys: 0, after_annos: 0, after_fields: 0 }); }
        }

        let total = self.g_names.len();
        let mut after_ns_sum = 0usize;
        let mut after_label_keys_sum = 0usize;
        let mut after_labels_sum = 0usize;
        let mut after_anno_keys_sum = 0usize;
        let mut after_annos_sum = 0usize;
        let mut passed_fields_total = 0usize;

        // Evaluate per shard
        for sh in self.shards.iter() {
            // Seed candidates (local indices)
            let mut candidates: Vec<usize> = if let Some(ns) = ns_filter {
                (0..sh.texts.len()).filter(|i| sh.namespaces.get(*i).map(|s| s == ns).unwrap_or(false)).collect()
            } else {
                (0..sh.texts.len()).collect()
            };
            after_ns_sum += candidates.len();

            // Intersect label key existence filters
            for key in label_key_filters.iter() {
                if let Some(post) = sh.label_key_post.get(key) {
                    candidates = Self::intersect_sorted(&candidates, post);
                } else { candidates.clear(); break; }
            }
            after_label_keys_sum += candidates.len();

            // Intersect label value filters
            for key in label_filters.iter() {
                if let Some(post) = sh.label_post.get(key) {
                    candidates = Self::intersect_sorted(&candidates, post);
                } else { candidates.clear(); break; }
            }
            after_labels_sum += candidates.len();

            // Intersect anno key existence filters
            for key in anno_key_filters.iter() {
                if let Some(post) = sh.anno_key_post.get(key) {
                    candidates = Self::intersect_sorted(&candidates, post);
                } else { candidates.clear(); break; }
            }
            after_anno_keys_sum += candidates.len();

            // Intersect anno value filters
            for key in anno_filters.iter() {
                if let Some(post) = sh.anno_post.get(key) {
                    candidates = Self::intersect_sorted(&candidates, post);
                } else { candidates.clear(); break; }
            }
            after_annos_sum += candidates.len();

            // Cap candidate set size per shard if configured
            if let Some(maxc) = opts.max_candidates { if candidates.len() > maxc { candidates.truncate(maxc); } }
            metrics::histogram!("search_candidates", candidates.len() as f64);

            // Apply field filters and optional fuzzy
            'doc: for li in candidates.into_iter() {
                for (pid, ref val) in field_filters.iter() {
                    let ok = sh.projected.get(li).map(|vec| vec.iter().any(|(id, v)| id == pid && v == val)).unwrap_or(false);
                    if !ok { continue 'doc; }
                }
                passed_fields_total += 1;
                let gidx = sh.doc_ids[li];
                if free_q.is_empty() {
                    let score = 0.0f32;
                    if opts.min_score.map(|m| score >= m).unwrap_or(true) {
                        hits.push(Hit { doc: gidx as u32, score });
                    }
                } else if let Some(score_i) = matcher.fuzzy_match(&sh.texts[li], &free_q) {
                    let score = score_i as f32;
                    if opts.min_score.map(|m| score >= m).unwrap_or(true) {
                        hits.push(Hit { doc: gidx as u32, score });
                    }
                }
            }
        }

        // Stable ranking across shards
        hits.sort_by(|a, b| {
            b.score
                .total_cmp(&a.score)
                .then_with(|| {
                    let an = &self.g_names[a.doc as usize];
                    let bn = &self.g_names[b.doc as usize];
                    an.cmp(bn)
                })
                .then_with(|| {
                    let au = &self.g_uids[a.doc as usize];
                    let bu = &self.g_uids[b.doc as usize];
                    au.cmp(bu)
                })
        });
        hits.truncate(limit);
        let dbg = SearchDebugInfo { total, after_ns: after_ns_sum, after_label_keys: after_label_keys_sum, after_labels: after_labels_sum, after_anno_keys: after_anno_keys_sum, after_annos: after_annos_sum, after_fields: passed_fields_total };
        let elapsed = started.elapsed();
        metrics::histogram!("search_eval_ms", elapsed.as_secs_f64() * 1_000.0);
        (hits, dbg)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use orka_core::{LiteObj, WorldSnapshot, Uid};
    

    fn uid(n: u8) -> Uid { let mut u = [0u8; 16]; u[0] = n; u }

    fn obj(
        id: u8,
        name: &str,
        ns: Option<&str>,
        labels: &[(&str, &str)],
        annos: &[(&str, &str)],
        projected: &[(u32, &str)],
        ts: i64,
    ) -> LiteObj {
        LiteObj {
            uid: uid(id),
            namespace: ns.map(|s| s.to_string()),
            name: name.to_string(),
            creation_ts: ts,
            projected: projected
                .iter()
                .map(|(k, v)| (*k, (*v).to_string()))
                .collect(),
            labels: labels.iter().map(|(k, v)| ((*k).to_string(), (*v).to_string())).collect(),
            annotations: annos.iter().map(|(k, v)| ((*k).to_string(), (*v).to_string())).collect(),
        }
    }

    fn snap(items: Vec<LiteObj>) -> WorldSnapshot { WorldSnapshot { epoch: 1, items } }

    #[test]
    fn ns_filter_works() {
        let s = snap(vec![
            obj(1, "a", Some("default"), &[], &[], &[], 0),
            obj(2, "b", Some("prod"), &[], &[], &[], 0),
        ]);
        let idx = Index::build_from_snapshot_with_meta(&s, None, Some("ConfigMap"), Some(""));
        let (hits, _dbg) = idx.search_with_debug("ns:default", 10);
        assert_eq!(hits.len(), 1);
        assert_eq!(s.items[hits[0].doc as usize].name, "a");
    }

    #[test]
    fn label_and_anno_filters() {
        let s = snap(vec![
            obj(1, "a", Some("default"), &[("app","web"), ("tier","frontend")], &[("team","core")], &[], 0),
            obj(2, "b", Some("default"), &[("app","api")], &[("team","platform")], &[], 0),
        ]);
        let idx = Index::build_from_snapshot(&s);
        let (hits, _dbg) = idx.search_with_debug("label:app=web", 10);
        assert_eq!(hits.len(), 1);
        assert_eq!(s.items[hits[0].doc as usize].name, "a");

        let (hits2, _dbg2) = idx.search_with_debug("label:app", 10);
        assert_eq!(hits2.len(), 2, "label key existence should match both items");

        let (hits3, _dbg3) = idx.search_with_debug("anno:team=platform", 10);
        assert_eq!(hits3.len(), 1);
        assert_eq!(s.items[hits3[0].doc as usize].name, "b");
    }

    #[test]
    fn field_filter_matches_projected() {
        let s = snap(vec![
            obj(1, "a", Some("default"), &[], &[], &[(1, "x"), (2, "y")], 0),
            obj(2, "b", Some("default"), &[], &[], &[(1, "z")], 0),
        ]);
        let pairs = vec![("spec.foo".to_string(), 1u32), ("spec.bar".to_string(), 2u32)];
        let idx = Index::build_from_snapshot_with_meta(&s, Some(&pairs), Some("ConfigMap"), Some(""));
        let (hits, _dbg) = idx.search_with_debug("field:spec.foo=x", 10);
        assert_eq!(hits.len(), 1);
        assert_eq!(s.items[hits[0].doc as usize].name, "a");

        let (hits2, _dbg2) = idx.search_with_debug("field:spec.bar=y", 10);
        assert_eq!(hits2.len(), 1);
        assert_eq!(s.items[hits2[0].doc as usize].name, "a");

        let (hits3, _dbg3) = idx.search_with_debug("field:spec.foo=notfound", 10);
        assert_eq!(hits3.len(), 0);
    }

    #[test]
    fn tie_break_by_name_then_uid() {
        let s = snap(vec![
            obj(2, "alpha", Some("b"), &[], &[], &[], 0),
            obj(1, "alpha", Some("a"), &[], &[], &[], 0),
            obj(3, "beta", Some("a"), &[], &[], &[], 0),
        ]);
        // No free text or filters -> all items, score 0.0 -> sort by name asc then uid asc
        let idx = Index::build_from_snapshot(&s);
        let (hits, _dbg) = idx.search_with_debug("", 10);
        let ordered: Vec<(String, [u8; 16])> = hits
            .iter()
            .map(|h| (s.items[h.doc as usize].name.clone(), s.items[h.doc as usize].uid))
            .collect();
        assert_eq!(ordered[0].0, "alpha");
        assert_eq!(ordered[1].0, "alpha");
        // uid with first byte 1 should come before 2 for same name
        assert_eq!(ordered[0].1[0], 1);
        assert_eq!(ordered[1].1[0], 2);
        assert_eq!(ordered[2].0, "beta");
    }

    #[test]
    fn kind_and_group_filters_gate_results() {
        let s = snap(vec![
            obj(1, "a", Some("default"), &[], &[], &[], 0),
            obj(2, "b", Some("default"), &[], &[], &[], 0),
        ]);
        let idx = Index::build_from_snapshot_with_meta(&s, None, Some("ConfigMap"), Some(""));
        assert_eq!(idx.search("k:ConfigMap", 10).len(), 2);
        assert_eq!(idx.search("k:Pod", 10).len(), 0);
        assert_eq!(idx.search("g:apps", 10).len(), 0);
    }
}

#[cfg(test)]
mod opts_tests {
    use super::*;
    use orka_core::{LiteObj, WorldSnapshot, Uid};

    fn uid(n: u8) -> Uid { let mut u = [0u8; 16]; u[0] = n; u }
    fn obj(name: &str, ns: Option<&str>, projected: &[(u32, &str)]) -> LiteObj {
        LiteObj {
            uid: uid(1),
            namespace: ns.map(|s| s.to_string()),
            name: name.to_string(),
            creation_ts: 0,
            projected: projected.iter().map(|(k, v)| (*k, (*v).to_string())).collect(),
            labels: smallvec::SmallVec::new(),
            annotations: smallvec::SmallVec::new(),
        }
    }
    fn snap(items: Vec<LiteObj>) -> WorldSnapshot { WorldSnapshot { epoch: 1, items } }

    #[test]
    fn max_candidates_caps_evaluation() {
        let s = snap(vec![
            obj("alpha", Some("ns"), &[]),
            obj("beta", Some("ns"), &[]),
            obj("gamma", Some("ns"), &[]),
        ]);
        let idx = Index::build_from_snapshot(&s);
        let (hits, _dbg) = idx.search_with_debug_opts("ns:ns", 10, SearchOpts { max_candidates: Some(2), min_score: None });
        // with no free text and no field filters, all pass, but candidate cap truncates prior to ranking
        assert_eq!(hits.len(), 2);
    }

    #[test]
    fn min_score_filters_low_scores() {
        let s = snap(vec![obj("alpha", Some("default"), &[])]);
        let idx = Index::build_from_snapshot(&s);
        // Free text "zzz" should not match; with min_score = 1.0, no hits
        let (hits, _dbg) = idx.search_with_debug_opts("zzz", 10, SearchOpts { max_candidates: None, min_score: Some(1.0) });
        assert_eq!(hits.len(), 0);
    }
}
</file>

<file path="crates/store/src/lib.rs">
//! Orka store (Milestone 0): Coalescer and World builder stubs

#![forbid(unsafe_code)]

use std::collections::VecDeque;

use orka_core::{Delta, LiteObj, WorldSnapshot, Projector, ShardPlanner, ShardKey};
use rustc_hash::FxHashMap;
use tokio::sync::{mpsc, watch};
use tracing::{debug, info};
use arc_swap::ArcSwap;
use std::sync::Arc;
use metrics::{counter, gauge, histogram};
use std::time::Instant;

/// Coalescing queue keyed by UID with FIFO order and fixed capacity.
pub struct Coalescer {
    map: FxHashMap<orka_core::Uid, Delta>,
    order: VecDeque<orka_core::Uid>,
    cap: usize,
    dropped: u64,
}

impl Coalescer {
    pub fn with_capacity(cap: usize) -> Self {
        Self { map: FxHashMap::default(), order: VecDeque::new(), cap, dropped: 0 }
    }

    pub fn len(&self) -> usize { self.map.len() }
    pub fn dropped(&self) -> u64 { self.dropped }

    pub fn push(&mut self, d: Delta) {
        let uid = d.uid;
        if !self.map.contains_key(&uid) {
            if self.order.len() >= self.cap {
                if let Some(old) = self.order.pop_front() {
                    self.map.remove(&old);
                    self.dropped += 1;
                    counter!("coalescer_dropped_total", 1);
                }
            }
            self.order.push_back(uid);
        }
        self.map.insert(uid, d);
        gauge!("coalescer_len", self.map.len() as f64);
    }

    /// Drain all currently coalesced deltas (simple version for M0).
    pub fn drain_ready(&mut self) -> Vec<Delta> {
        let mut out = Vec::with_capacity(self.order.len());
        while let Some(uid) = self.order.pop_front() {
            if let Some(d) = self.map.remove(&uid) {
                out.push(d);
            }
        }
        gauge!("coalescer_len", self.map.len() as f64);
        out
    }
}

/// Builds WorldSnapshot instances from deltas.
pub struct WorldBuilder {
    epoch: u64,
    items: Vec<LiteObj>,
    projector: Option<std::sync::Arc<dyn Projector + Send + Sync>>,
    max_labels_per_obj: Option<usize>,
    max_annos_per_obj: Option<usize>,
}

impl WorldBuilder {
    pub fn new() -> Self { Self::with_projector(None) }

    pub fn with_projector(projector: Option<std::sync::Arc<dyn Projector + Send + Sync>>) -> Self {
        let max_labels_per_obj = std::env::var("ORKA_MAX_LABELS_PER_OBJ").ok().and_then(|s| s.parse::<usize>().ok());
        let max_annos_per_obj = std::env::var("ORKA_MAX_ANNOS_PER_OBJ").ok().and_then(|s| s.parse::<usize>().ok());
        Self { epoch: 0, items: Vec::new(), projector, max_labels_per_obj, max_annos_per_obj }
    }

    /// Apply a batch of deltas and update in-memory items.
    /// M0: naive implementation; to be replaced with UID-indexed map.
    pub fn apply(&mut self, batch: Vec<Delta>) {
        for d in batch {
            match d.kind {
                orka_core::DeltaKind::Applied => {
                    // Convert raw to LiteObj (placeholder)
                    if let Some(meta) = d.raw.get("metadata") {
                        let name = meta.get("name").and_then(|v| v.as_str()).unwrap_or("").to_string();
                        let namespace = meta.get("namespace").and_then(|v| v.as_str()).map(|s| s.to_string());
                        let creation_ts = meta
                            .get("creationTimestamp")
                            .and_then(|v| v.as_str())
                            .and_then(|s| chrono::DateTime::parse_from_rfc3339(s).ok())
                            .map(|dt| dt.timestamp())
                            .unwrap_or(0);
                        let projected = if let Some(p) = &self.projector { p.project(&d.raw) } else { smallvec::SmallVec::<[(u32, String); 8]>::new() };
                        // Extract labels and annotations from metadata
                        let mut labels = smallvec::SmallVec::<[(String, String); 8]>::new();
                        let mut annotations = smallvec::SmallVec::<[(String, String); 4]>::new();
                        if let Some(meta_obj) = d.raw.get("metadata").and_then(|m| m.as_object()) {
                            if let Some(lbls) = meta_obj.get("labels").and_then(|m| m.as_object()) {
                                for (k, v) in lbls.iter() {
                                    if let Some(val) = v.as_str() { labels.push((k.clone(), val.to_string())); }
                                    if let Some(cap) = self.max_labels_per_obj { if labels.len() >= cap { break; } }
                                }
                            }
                            if let Some(ann) = meta_obj.get("annotations").and_then(|m| m.as_object()) {
                                for (k, v) in ann.iter() {
                                    if let Some(val) = v.as_str() { annotations.push((k.clone(), val.to_string())); }
                                    if let Some(cap) = self.max_annos_per_obj { if annotations.len() >= cap { break; } }
                                }
                            }
                        }

                        let lo = LiteObj { uid: d.uid, namespace, name, creation_ts, projected, labels, annotations };
                        // Replace existing by uid (linear scan for M0 stub)
                        if let Some(idx) = self.items.iter().position(|x| x.uid == d.uid) {
                            self.items[idx] = lo;
                        } else {
                            self.items.push(lo);
                        }
                    }
                }
                orka_core::DeltaKind::Deleted => {
                    self.items.retain(|x| x.uid != d.uid);
                }
            }
        }
        self.epoch = self.epoch.saturating_add(1);
    }

    pub fn freeze(&self) -> std::sync::Arc<WorldSnapshot> {
        std::sync::Arc::new(WorldSnapshot { epoch: self.epoch, items: self.items.clone() })
    }
}

/// Handle for readers to access the current snapshot and subscribe to swaps.
pub struct BackendHandle {
    snap: Arc<ArcSwap<WorldSnapshot> >,
    epoch_rx: watch::Receiver<u64>,
}

impl BackendHandle {
    pub fn current(&self) -> std::sync::Arc<WorldSnapshot> { self.snap.load_full() }
    pub fn subscribe_epoch(&self) -> watch::Receiver<u64> { self.epoch_rx.clone() }
}

/// Spawn an ingest loop consuming deltas and swapping snapshots. Returns a sender for deltas and a handle for reads.
pub fn spawn_ingest(cap: usize) -> (mpsc::Sender<Delta>, BackendHandle) {
    spawn_ingest_with_projector(cap, None)
}

/// Variant that accepts an optional projector used during LiteObj shaping.
pub fn spawn_ingest_with_projector(
    cap: usize,
    projector: Option<std::sync::Arc<dyn Projector + Send + Sync>>,
) -> (mpsc::Sender<Delta>, BackendHandle) {
    spawn_ingest_with_planner(cap, projector, None)
}

/// Variant that accepts a shard planner used for partitioning.
pub fn spawn_ingest_with_planner(
    cap: usize,
    projector: Option<std::sync::Arc<dyn Projector + Send + Sync>>,
    planner: Option<std::sync::Arc<dyn ShardPlanner + Send + Sync>>,
) -> (mpsc::Sender<Delta>, BackendHandle) {
    // Number of shards for ingest/build; default 1
    let shards: usize = std::env::var("ORKA_SHARDS").ok().and_then(|s| s.parse().ok()).unwrap_or(1).max(1);

    let (tx, mut rx) = mpsc::channel::<Delta>(cap);
    let snap = Arc::new(ArcSwap::from_pointee(WorldSnapshot::default()));
    let (epoch_tx, epoch_rx) = watch::channel(0u64);
    let snap_clone = Arc::clone(&snap);
    let planner_arc = planner;

    tokio::spawn(async move {
        // Build shard workers
        struct Shard { coalescer: Coalescer, builder: WorldBuilder }
        let mut shard_workers: Vec<Shard> = (0..shards)
            .map(|_| Shard { coalescer: Coalescer::with_capacity(cap), builder: WorldBuilder::with_projector(projector.clone()) })
            .collect();

        // Track per-shard dropped increments to export labeled counters without double counting
        let mut dropped_reported: Vec<u64> = vec![0; shards];

        let mut ticker = tokio::time::interval(std::time::Duration::from_millis(8));
        // Track arrival times to compute ingest lag across coalescing/batching
        let mut arrivals: FxHashMap<orka_core::Uid, Instant> = FxHashMap::default();

        // Namespace bucket function (simple hash modulo shards)
        fn ns_bucket(d: &Delta, shards: usize) -> usize {
            if shards <= 1 { return 0; }
            let ns = d.raw
                .get("metadata")
                .and_then(|m| m.get("namespace"))
                .and_then(|v| v.as_str())
                .unwrap_or("");
            // A tiny FNV-1a style hash
            let mut h: u64 = 0xcbf29ce484222325;
            for b in ns.as_bytes() { h ^= *b as u64; h = h.wrapping_mul(0x100000001b3); }
            (h as usize) % shards
        }
        fn gvk_id_from(raw: &serde_json::Value) -> u32 {
            let api = raw.get("apiVersion").and_then(|v| v.as_str()).unwrap_or("");
            let kind = raw.get("kind").and_then(|v| v.as_str()).unwrap_or("");
            let s = format!("{}/{}", api, kind);
            let mut h: u32 = 0x811c9dc5; // FNV-1a 32-bit offset
            for b in s.as_bytes() { h ^= *b as u32; h = h.wrapping_mul(0x01000193); }
            h
        }

        let mut global_epoch: u64 = 0;

        loop {
            tokio::select! {
                maybe = rx.recv() => {
                    match maybe {
                        Some(d) => {
                            // Choose shard via planner when provided; fallback to namespace modulo
                            let idx = if let Some(pl) = &planner_arc {
                                let ns = d.raw
                                    .get("metadata")
                                    .and_then(|m| m.get("namespace"))
                                    .and_then(|v| v.as_str());
                                let key: ShardKey = pl.plan(gvk_id_from(&d.raw), ns);
                                (key.ns_bucket as usize) % shards
                            } else {
                                ns_bucket(&d, shards)
                            };
                            arrivals.insert(d.uid, Instant::now());
                            shard_workers[idx].coalescer.push(d);
                            // Update per-shard coalescer length gauge
                            let len = shard_workers[idx].coalescer.len();
                            gauge!("coalescer_len", len as f64, "shard" => idx.to_string());
                        }
                        None => {
                            debug!("delta channel closed; draining shards and exiting ingest loop");
                            let mut any = false;
                            for (i, sh) in shard_workers.iter_mut().enumerate() {
                                let batch = sh.coalescer.drain_ready();
                                if !batch.is_empty() {
                                    let drained = batch.len();
                                    let dropped = sh.coalescer.dropped();
                                    // Emit ingest lag per drained delta
                                    let now = Instant::now();
                                    for d in batch.iter() {
                                        if let Some(t0) = arrivals.remove(&d.uid) {
                                            let ms = now.saturating_duration_since(t0).as_secs_f64() * 1000.0;
                                            histogram!("ingest_lag_ms", ms, "shard" => i.to_string());
                                        }
                                    }
                                    sh.builder.apply(batch);
                                    any = true;
                                    debug!(shard = i, drained, dropped, "ingest applied batch (final)");
                                    histogram!("ingest_batch_size", drained as f64);
                                }
                                // Update per-shard coalescer length post-drain
                                let len = sh.coalescer.len();
                                gauge!("coalescer_len", len as f64, "shard" => i.to_string());
                            }
                            if any {
                                global_epoch = global_epoch.saturating_add(1);
                                // Merge items from all shards into a single snapshot
                                let mut items: Vec<LiteObj> = Vec::new();
                                for sh in shard_workers.iter() {
                                    items.extend(sh.builder.items.clone());
                                }
                                // Apply soft memory trimming against ORKA_MAX_RSS_MB before storing
                                let approx_pre = approx_items_bytes(&items);
                                let approx_final = if let Some(max_mb) = std::env::var("ORKA_MAX_RSS_MB").ok().and_then(|s| s.parse::<usize>().ok()) {
                                    let cap = max_mb.saturating_mul(1024*1024);
                                    if approx_pre > cap { trim_items_for_memory(&mut items, cap) } else { approx_pre }
                                } else { approx_pre };
                                let merged = WorldSnapshot { epoch: global_epoch, items };
                                snap_clone.store(Arc::new(merged));
                                let _ = epoch_tx.send(global_epoch);
                                gauge!("ingest_epoch", global_epoch as f64);
                                let snap_loaded = snap_clone.load();
                                gauge!("snapshot_items", snap_loaded.items.len() as f64);
                                gauge!("docs_total", snap_loaded.items.len() as f64);
                                gauge!("snapshot_bytes", approx_final as f64);
                                // No raw retention post-shaping
                                gauge!("raw_bytes", 0.0);
                                // labels cardinality (distinct keys)
                                let mut set = std::collections::HashSet::new();
                                for o in &snap_loaded.items { for (k, _v) in &o.labels { set.insert(k); } }
                                gauge!("labels_cardinality", set.len() as f64);
                            }
                            break;
                        }
                    }
                }
                _ = ticker.tick() => {
                    let mut any = false;
                    for (i, sh) in shard_workers.iter_mut().enumerate() {
                        let batch = sh.coalescer.drain_ready();
                        if !batch.is_empty() {
                            let drained = batch.len();
                            let dropped = sh.coalescer.dropped();
                            // Record per-shard drop increments
                            if let Some(prev) = dropped_reported.get_mut(i) {
                                let inc = dropped.saturating_sub(*prev);
                                if inc > 0 {
                                    counter!("coalescer_dropped_total", inc as u64, "shard" => i.to_string());
                                    *prev = dropped;
                                }
                            }
                            let t_shard = std::time::Instant::now();
                            // Emit ingest lag per drained delta
                            let now = Instant::now();
                            for d in batch.iter() {
                                if let Some(t0) = arrivals.remove(&d.uid) {
                                    let ms = now.saturating_duration_since(t0).as_secs_f64() * 1000.0;
                                    histogram!("ingest_lag_ms", ms, "shard" => i.to_string());
                                }
                            }
                            sh.builder.apply(batch);
                            let shard_ms = t_shard.elapsed().as_secs_f64() * 1000.0;
                            any = true;
                            debug!(shard = i, drained, dropped, shard_ms = shard_ms, "ingest applied batch");
                            histogram!("ingest_batch_size", drained as f64, "shard" => i.to_string());
                            histogram!("shard_build_ms", shard_ms, "shard" => i.to_string());
                        }
                        // Update per-shard coalescer length post-drain
                        let len = sh.coalescer.len();
                        gauge!("coalescer_len", len as f64, "shard" => i.to_string());
                    }
                    if any {
                        global_epoch = global_epoch.saturating_add(1);
                        let t_merge = std::time::Instant::now();
                        let mut items: Vec<LiteObj> = Vec::new();
                        for sh in shard_workers.iter() {
                            items.extend(sh.builder.items.clone());
                        }
                        // Apply soft memory trimming against ORKA_MAX_RSS_MB before storing
                        let approx_pre = approx_items_bytes(&items);
                        let approx_final = if let Some(max_mb) = std::env::var("ORKA_MAX_RSS_MB").ok().and_then(|s| s.parse::<usize>().ok()) {
                            let cap = max_mb.saturating_mul(1024*1024);
                            if approx_pre > cap { trim_items_for_memory(&mut items, cap) } else { approx_pre }
                        } else { approx_pre };
                        let merged = WorldSnapshot { epoch: global_epoch, items };
                        let t_swap = std::time::Instant::now();
                        snap_clone.store(Arc::new(merged));
                        let swap_ms = t_swap.elapsed().as_secs_f64() * 1000.0;
                        histogram!("snapshot_swap_ms", swap_ms);
                        let _ = epoch_tx.send(global_epoch);
                        gauge!("ingest_epoch", global_epoch as f64);
                        let snap_loaded = snap_clone.load();
                        gauge!("snapshot_items", snap_loaded.items.len() as f64);
                        gauge!("docs_total", snap_loaded.items.len() as f64);
                        gauge!("snapshot_bytes", approx_final as f64);
                        gauge!("raw_bytes", 0.0);
                        let mut set = std::collections::HashSet::new();
                        for o in &snap_loaded.items { for (k, _v) in &o.labels { set.insert(k); } }
                        gauge!("labels_cardinality", set.len() as f64);
                        // Merge cost includes building the merged list plus swap
                        let merge_ms = t_merge.elapsed().as_secs_f64() * 1000.0;
                        histogram!("shard_merge_ms", merge_ms);
                    }
                }
            }
        }
        info!("ingest loop stopped");
    });

    (tx, BackendHandle { snap, epoch_rx })
}

fn approx_snapshot_bytes(snap: &WorldSnapshot) -> usize {
    approx_items_bytes(&snap.items)
}

fn approx_items_bytes(items: &Vec<LiteObj>) -> usize {
    let mut total: usize = std::mem::size_of::<WorldSnapshot>();
    for o in items.iter() {
        total += std::mem::size_of::<LiteObj>();
        total += o.name.len();
        if let Some(ns) = &o.namespace { total += ns.len(); }
        for (_id, v) in &o.projected { total += v.len(); }
        for (k, v) in &o.labels { total += k.len() + v.len(); }
        for (k, v) in &o.annotations { total += k.len() + v.len(); }
    }
    total
}

fn trim_items_for_memory(items: &mut Vec<LiteObj>, cap_bytes: usize) -> usize {
    // Stage 1: drop annotations
    let mut approx = approx_items_bytes(items);
    if approx > cap_bytes {
        for o in items.iter_mut() { o.annotations.clear(); }
        approx = approx_items_bytes(items);
        tracing::warn!(approx, cap_bytes, "memory pressure: dropped annotations to honor ORKA_MAX_RSS_MB");
    }
    // Stage 2: drop labels
    if approx > cap_bytes {
        for o in items.iter_mut() { o.labels.clear(); }
        approx = approx_items_bytes(items);
        tracing::warn!(approx, cap_bytes, "memory pressure: dropped labels to honor ORKA_MAX_RSS_MB");
    }
    // Stage 3: drop projected fields
    if approx > cap_bytes {
        for o in items.iter_mut() { o.projected.clear(); }
        approx = approx_items_bytes(items);
        tracing::warn!(approx, cap_bytes, "memory pressure: dropped projected fields to honor ORKA_MAX_RSS_MB");
    }
    approx
}

#[cfg(test)]
mod tests {
    use super::*;
    use orka_core::{DeltaKind, Uid};

    fn uid(n: u8) -> Uid {
        let mut u = [0u8; 16];
        u[0] = n;
        u
    }

    fn obj(name: &str, ns: Option<&str>) -> serde_json::Value {
        let mut meta = serde_json::json!({
            "name": name,
            "uid": format!("00000000-0000-0000-0000-{:012}", 1),
            "creationTimestamp": "2020-01-01T00:00:00Z",
        });
        if let Some(ns) = ns { meta["namespace"] = serde_json::Value::String(ns.to_string()); }
        serde_json::json!({ "metadata": meta })
    }

    #[test]
    fn coalescer_capacity_and_drop() {
        let mut c = Coalescer::with_capacity(2);
        // push 3 unique uids -> 1 drop expected
        for i in 0..3u8 {
            c.push(Delta { uid: uid(i), kind: DeltaKind::Applied, raw: serde_json::json!({}) });
        }
        assert_eq!(c.len(), 2);
        assert_eq!(c.dropped(), 1);

        let drained = c.drain_ready();
        assert_eq!(drained.len(), 2);
        assert_eq!(c.len(), 0);
    }

    #[test]
    fn coalescer_overwrite_same_uid() {
        let mut c = Coalescer::with_capacity(4);
        let u = uid(42);
        c.push(Delta { uid: u, kind: DeltaKind::Applied, raw: serde_json::json!({"a":1}) });
        c.push(Delta { uid: u, kind: DeltaKind::Applied, raw: serde_json::json!({"a":2}) });
        let drained = c.drain_ready();
        assert_eq!(drained.len(), 1);
        assert_eq!(drained[0].raw["a"], 2);
    }

    #[test]
    fn worldbuilder_apply_add_update_delete() {
        let mut wb = WorldBuilder::new();
        let u1 = uid(1);
        let u2 = uid(2);

        // add two
        wb.apply(vec![
            Delta { uid: u1, kind: DeltaKind::Applied, raw: obj("a", Some("ns")) },
            Delta { uid: u2, kind: DeltaKind::Applied, raw: obj("b", None) },
        ]);
        assert_eq!(wb.items.len(), 2);

        // update one (rename)
        let mut o = obj("a2", Some("ns"));
        o["metadata"]["uid"] = serde_json::Value::String("00000000-0000-0000-0000-000000000001".to_string());
        wb.apply(vec![Delta { uid: u1, kind: DeltaKind::Applied, raw: o }]);
        assert_eq!(wb.items.iter().find(|x| x.uid == u1).unwrap().name, "a2");

        // delete one
        wb.apply(vec![Delta { uid: u2, kind: DeltaKind::Deleted, raw: serde_json::json!({}) }]);
        assert_eq!(wb.items.len(), 1);
        assert_eq!(wb.items[0].name, "a2");
    }
}
</file>

<file path="MILESTONE-1.md">
# Orka — Milestone 1 (Schema & Search)

Status: Closed — 2025-09-07

> Goal: treat CRDs as first‑class: normalize schema, pick a few high‑signal fields for listing/search, validate edits, and make search feel instant with typed filters and fuzzy scoring.

---

## Scope (M1)

- Schema engine: read `openAPIV3Schema`, capture `additionalPrinterColumns`, derive projected scalar paths, and expose flags/quirks.
- Validation: YAML → JSON → JSON Schema validation with friendly errors; feature‑gated.
- Search: in‑RAM index built from metadata + projected fields; typed filters + fuzzy matcher for ranking.
- CLI: `schema` to inspect a GVK; `search` to query across resources.

Non‑goals (M1): apply/SSA, diffs, persistence/SQLite, RPC surface, tantivy/FST acceleration, UI.

Success = `schema` and `search` work against real and replayed data with predictable latency and memory bounds.

---

## Architecture Slice

```
Discovery ──► Schema Engine ──► Projection (fields)
                          │
Watchers/Coalescer ───────┴────► Ingest/WorldBuilder ──► ArcSwap<WorldSnapshot>
                                                    │                 │
                                                    └──► RAM Search ◄─┘
                                                              ▲
                                                        orkactl search
```

Rules:

- Projection chooses 3–6 stable, scalar paths per GVK; avoid churny/status where possible unless explicitly useful.
- Index updates are incremental from deltas; deletes remove docs fully.
- Reads are lock‑free; search intersects typed filters first, then applies fuzzy ranking to the candidate set.

---

## Workspace Additions (M1)

- `crates/schema`: normalize CRD schemas, printer columns, projection, validation glue.
- `crates/search`: RAM index, typed filter parsing, fuzzy scoring.
- Extend `crates/kubehub` to fetch CRD objects for schema; keep watchers unchanged.
- Extend `crates/store` to expose projected fields in `LiteObj`.
- Extend `crates/cli` with `schema` and `search` commands.

---

## Detailed Tasks

1) Schema Engine
- Parse CRDs; pick served version; normalize `openAPIV3Schema` (resolve `oneOf/anyOf/allOf` conservatively, mark YAML‑only where needed).
- Extract `additionalPrinterColumns` when present; sort/stabilize.
- Derive projected scalar paths when columns are missing or poor; prefer fields that differentiate rows (heuristics, depth cap, skip ephemeral/status by default).
- Shape outputs into `CrdSchema { served_version, printer_cols, projected_paths, flags }` and cache keyed by `{cluster, group, plural, version}`.

2) Validation
- YAML → `serde_json::Value` → `jsonschema` validate (feature `jsonschema-validate`).
- Human‑friendly error rendering with path, reason, and a short suggestion.

3) Store Integration
- Enrich `LiteObj` with `projected: SmallVec<(PathId, String)>` populated using `CrdSchema` renderers.
- Keep raw bytes optional; drop when memory pressure is high (unchanged from M0 policy).

4) Search Index
- Build per‑GVK shards: metadata (ns, name), labels/annotations, projected fields.
- Maintain typed postings for filters (`k:`, `g:`, `ns:`, `label:`, `anno:`, `field:`).
- Candidate set = intersection of typed postings; rank with `fuzzy-matcher` on concatenated display text.
- Support `limit` and stable tiebreaking (name/uid).

5) CLI
- `orkactl schema group/version/kind`: print served version, printer columns, projected paths, flags. `-o json` for machine use.
- `orkactl search "query" --limit N`: typed filters + fuzzy text; print `KIND NS/NAME SCORE`. `-o json` supported.

6) Tests & Benches
- Unit tests: schema normalization, projection picker, validator edges, filter parser.
- Replay test: feed recorded deltas; assert deterministic candidates and top‑k order for fixed seeds.
- Bench: 100k synthetic docs → ingest cost, index memory, search p50/p99.

7) Observability & Limits
- Metrics: `index_docs`, `index_bytes`, `search_candidates`, `search_eval`, `search_p50_ms`, `search_p99_ms`.
- Env knobs: `ORKA_SEARCH_LIMIT`, `ORKA_SEARCH_MAX_CANDIDATES`, `ORKA_SEARCH_MIN_SCORE`.

---

## Minimal Types (M1)

```rust
// crates/schema
pub struct PrinterCol { pub name: String, pub json_path: String };
pub struct PathSpec { pub id: u32, pub json_path: String, pub renderer: Renderer };
pub struct SchemaFlags { pub yaml_only_nodes: bool, pub preserves_unknown: bool }
pub struct CrdSchema {
    pub served_version: String,
    pub printer_cols: Vec<PrinterCol>,
    pub projected_paths: Vec<PathSpec>,
    pub flags: SchemaFlags,
}

// crates/search
pub type DocId = u32;
pub struct Hit { pub doc: DocId, pub score: f32 }
```

---

## Index Sketch

```rust
pub struct Shard {
    // typed postings
    by_kind: FxHashMap<String, Vec<DocId>>,
    by_group: FxHashMap<String, Vec<DocId>>,
    by_ns: FxHashMap<String, Vec<DocId>>,
    labels: FxHashMap<(String,String), Vec<DocId>>,
    annos: FxHashMap<(String,String), Vec<DocId>>,
    fields: FxHashMap<(u32,String), Vec<DocId>>, // (PathId, rendered)

    // display text for fuzzy
    text: Vec<String>, // index by DocId
}
```

Update: on add/update → refresh postings and `text[doc]`; on delete → remove doc from all postings and clear text.

---

## Validator Path

- Input: YAML from CLI/editor.
- Transform: YAML → JSON → validate against `CrdSchema` with `jsonschema`.
- Output: ok or list of `(path, error, hint)`; never panics on user data.

---

## CLI Specs (M1)

- `orkactl schema group/version/kind`
  - Output: served version, printer columns, projected paths, flags.
  - Flags: `-o json`.

- `orkactl search "k:Application ns:prod payments" --limit 20`
  - Output: `KIND   NAMESPACE/NAME         SCORE`
  - Flags: `--cluster`, `--limit`, `-o json`.

Examples:

```
$ orkactl schema cert-manager.io/v1/Certificate
served: v1
printer-cols: Ready, Age, SecretName
projected: spec.dnsNames[0], status.conditions[?type==Ready].status, ...

$ orkactl search "k:Certificate ns:prod payments"
Certificate  prod/payments-cert         0.86
```

---

## Performance Targets (M1)

- Search: ≤ 10 ms p99 @ 100k docs (single thread) with `limit=50`.
- Index build on steady ingest: ≤ 15% overhead over M0 ingest time.
- Schema load and projection planning: ≤ 50 ms per GVK cold; cached afterwards.
- Memory: default cap ≤ 800 MB on large clusters; index size tracked.

---

## Risks & Mitigations

- CRD schema variance → tolerant normalizer; YAML fallback for complex nodes.
- Large label cardinality → cap postings per key; fall back to text search.
- Fuzzy false positives → threshold and tiebreakers; expose `--limit` and score.
- Index rebuild cost → incremental updates; avoid full rebuilds.

---

## Definition of Done (M1)

- `orkactl schema` prints accurate info for several CRDs across operators.
- `orkactl search` returns relevant hits with typed filters and stable ranking.
- Unit + replay tests green; basic bench meets p99 budget on CI hardware.
- Metrics visible; knobs documented; no panics on malformed input.

---

## Implementation Order (Checklist)

- [x] Add `crates/schema` crate with `CrdSchema`, `PrinterCol`, `PathSpec`, `SchemaFlags`.
- [x] Load CRDs and parse versions; extract `additionalPrinterColumns` (JSON traversal; basic normalization).
- [x] Implement projection selection and renderers; derive from `openAPIV3Schema` when columns absent.
- [x] Add YAML→JSON→JSON Schema validation (feature `jsonschema-validate`).
- [x] Extend `LiteObj` to carry `projected` values (plus labels/annotations) using schema renderers.
- [x] Add `crates/search` crate: postings + text store (built from snapshot); label/anno postings.
- [x] Implement typed filter parser: `ns:`, `label:`, `anno:`, `field:` (+ free text), plus `k:`/`g:` (exact match; wildcards optional).
- [x] Integrate `fuzzy-matcher` for ranking; `limit` and stable name/uid tiebreaker done.
- [x] Expose a search API returning `(doc, score)` and mapped `LiteObj` (+ debug counters).
- [x] CLI: `schema gvk [-o json]` with human/json output.
- [x] CLI: `search "query" [--limit N] [--explain] [-o json]`; watcher scopes from `--ns` or `ns:` token; primed List for fast first snapshot.
- [~] Unit tests: filter parser + scoring/ranking and basic schema/projection tests added; deeper normalization tests pending.
- [x] Replay test: synthetic deltas → deterministic candidates and ordering.
- [x] Bench: 100k docs; record p50/p99. Basic release build on dev hardware hits p99 target; memory tracking approximated via metrics.
- [x] Docs: grammar and examples in `crates/cli/README.md`.

> If it adds latency or complexity without clear payoff, skip it. The point of M1 is fast insight, not perfect semantics.

---

## Status & Next Steps (M1)

Done
- Schema discovery (served/storage version), printer-cols extraction, and projection derivation from OpenAPI.
- Projector wired into ingest; `LiteObj` now includes projected fields, labels, and annotations.
- Search index with typed filters (`ns:`, `label:` key/value and existence, `anno:` key/value and existence, `field:`, `k:`, `g:`), fuzzy ranking, stable name/uid tiebreaker, and debug/explain.
- CLI: `schema` and `search` implemented; watcher scopes from namespace; initial List primes snapshot. Search table prints KIND.

Next Steps
- Unit tests: expand schema normalization and projection picker edge cases; projector path extraction for complex nodes.
- Observability: consider adding p50/p99 summaries on `search_eval_ms`; ensure Prometheus exporter visibility.
- Baseline bench at ~100k docs (simple harness acceptable for now).
- Optional: JSON Schema validation behind `jsonschema-validate`; simple wildcards for `k:`/`g:`.
- Optional: JSON Schema validation behind `jsonschema-validate`; simple wildcards for `k:`/`g:`.
</file>

<file path="crates/cli/Cargo.toml">
[package]
name = "orkactl"
version = "0.0.0"
edition = "2021"
license = "MIT OR Apache-2.0"
description = "Orka CLI (Milestone 0)"

[dependencies]
orka-core = { path = "../core" }
orka-store = { path = "../store" }
orka-kubehub = { path = "../kubehub" }
orka-schema = { path = "../schema" }
orka-search = { path = "../search" }
orka-apply = { path = "../apply" }
orka-persist = { path = "../persist" }
orka_ops = { path = "../ops" }
orka_api = { path = "../api" }
anyhow = { workspace = true }
tracing = { workspace = true }
tracing-subscriber = { workspace = true }
clap = { workspace = true }
tokio = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
serde_yaml = "0.9"
metrics = { workspace = true }
metrics-exporter-prometheus = { workspace = true }
kube = { workspace = true }
uuid = { workspace = true }
regex = { workspace = true }

[[bin]]
name = "orkactl"
path = "src/main.rs"

[features]
default = []
# Enable CRD JSONSchema validation path in CLI via orka-schema feature
validate = ["orka-schema/jsonschema-validate"]
</file>

<file path="crates/kubehub/src/lib.rs">
//! Orka kubehub (Milestone 0) – discovery and watcher wiring

#![forbid(unsafe_code)]

use anyhow::{anyhow, Context, Result};
use serde::{Deserialize, Serialize};
use tracing::{debug, info, warn};
use metrics::{counter, histogram};

use futures::TryStreamExt;
use kube::{
    api::Api,
    core::{DynamicObject, GroupVersionKind},
    discovery::{Discovery, Scope},
    runtime::watcher::{self, Event},
    Client,
};
use orka_core::{Delta, DeltaKind};
use tokio::sync::mpsc;
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DiscoveredResource {
    pub group: String,
    pub version: String,
    pub kind: String,
    pub namespaced: bool,
}

impl DiscoveredResource {
    pub fn gvk_key(&self) -> String {
        if self.group.is_empty() {
            format!("{}/{}", self.version, self.kind)
        } else {
            format!("{}/{}/{}", self.group, self.version, self.kind)
        }
    }
}

/// Discover served resources (incl. CRDs) using kube Discovery.
pub async fn discover(_prefer_crd: bool) -> Result<Vec<DiscoveredResource>> {
    let client = Client::try_default().await?;
    let discovery = Discovery::new(client).run().await?;
    let mut out = Vec::new();
    for group in discovery.groups() {
        for (ar, caps) in group.recommended_resources() {
            let namespaced = matches!(caps.scope, Scope::Namespaced);
            out.push(DiscoveredResource {
                group: ar.group.clone(),
                version: ar.version.clone(),
                kind: ar.kind.clone(),
                namespaced,
            });
        }
    }
    // Stable-ish order
    out.sort_by(|a, b| a.group.cmp(&b.group).then(a.version.cmp(&b.version)).then(a.kind.cmp(&b.kind)));
    Ok(out)
}

fn parse_gvk_key(key: &str) -> Result<GroupVersionKind> {
    let parts: Vec<_> = key.split('/').collect();
    match parts.as_slice() {
        [version, kind] => Ok(GroupVersionKind { group: String::new(), version: version.to_string(), kind: kind.to_string() }),
        [group, version, kind] => Ok(GroupVersionKind { group: (*group).to_string(), version: (*version).to_string(), kind: (*kind).to_string() }),
        _ => Err(anyhow!("invalid gvk key: {} (expect v1/Kind or group/v1/Kind)", key)),
    }
}

async fn find_api_resource(client: Client, gvk: &GroupVersionKind) -> Result<(kube::core::ApiResource, bool)> {
    let discovery = Discovery::new(client).run().await?;
    for group in discovery.groups() {
        for (ar, caps) in group.recommended_resources() {
            if ar.group == gvk.group && ar.version == gvk.version && ar.kind == gvk.kind {
                let namespaced = matches!(caps.scope, Scope::Namespaced);
                return Ok((ar.clone(), namespaced));
            }
        }
    }
    Err(anyhow!("GVK not found: {}/{}/{}", gvk.group, gvk.version, gvk.kind))
}

fn strip_managed_fields(v: &mut serde_json::Value) {
    if let Some(meta) = v.get_mut("metadata") {
        if let Some(obj) = meta.as_object_mut() {
            obj.remove("managedFields");
        }
    }
}

fn to_uid(uid_str: &str) -> Result<orka_core::Uid> {
    let u = Uuid::parse_str(uid_str).context("parsing metadata.uid as uuid")?;
    let bytes = *u.as_bytes();
    Ok(bytes)
}

fn delta_from(obj: &DynamicObject, kind: DeltaKind) -> Result<Delta> {
    let uid_str = obj
        .metadata
        .uid
        .as_deref()
        .ok_or_else(|| anyhow!("object missing metadata.uid"))?;
    let uid = to_uid(uid_str)?;
    let mut raw = serde_json::to_value(obj).context("serializing DynamicObject")?;
    strip_managed_fields(&mut raw);
    Ok(Delta { uid, kind, raw })
}

/// Start list+watch for a given GVK key and send coalesced deltas into provided channel.
pub async fn start_watcher(gvk_key: &str, namespace: Option<&str>, delta_tx: mpsc::Sender<Delta>) -> Result<()> {
    let client = Client::try_default().await?;
    let gvk = parse_gvk_key(gvk_key)?;
    let (ar, namespaced) = find_api_resource(client.clone(), &gvk).await?;

    // Periodic relist interval (seconds)
    let relist_secs: u64 = std::env::var("ORKA_RELIST_SECS")
        .ok()
        .and_then(|s| s.parse::<u64>().ok())
        .unwrap_or(300);
    // Backoff max (seconds) for watch errors
    let backoff_max: u64 = std::env::var("ORKA_WATCH_BACKOFF_MAX_SECS")
        .ok()
        .and_then(|s| s.parse::<u64>().ok())
        .unwrap_or(30);

    info!(gvk = %gvk_key, ns = ?namespace, relist_secs, "watcher starting");

    let mut backoff: u64 = 1;
    loop {
        let api: Api<DynamicObject> = if namespaced {
            match namespace {
                Some(ns) => Api::namespaced_with(client.clone(), ns, &ar),
                None => Api::all_with(client.clone(), &ar),
            }
        } else {
            Api::all_with(client.clone(), &ar)
        };

        let cfg = watcher::Config::default();
        let stream = watcher::watcher(api, cfg);
        futures::pin_mut!(stream);
        // Jittered relist: ±10%
        let jitter = ((relist_secs as f64) * 0.1) as i64;
        let jval = if jitter > 0 {
            // Fast, dependency-free pseudo-random using time
            let now = std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap_or_default().subsec_nanos() as i64;
            let sign = if (now & 1) == 0 { 1 } else { -1 };
            (now % (jitter as i64 + 1)) * sign
        } else { 0 };
        let relist_actual = (relist_secs as i64 + jval).max(1) as u64;
        let relist_timer = tokio::time::sleep(std::time::Duration::from_secs(relist_actual));
        tokio::pin!(relist_timer);
        info!(relist_actual, "watch stream opened");

        // Read until stream ends or relist timer fires
        let ended = loop {
            tokio::select! {
                maybe_ev = stream.try_next() => {
                    match maybe_ev {
                        Ok(Some(Event::Applied(o))) => {
                            let d = delta_from(&o, DeltaKind::Applied)?;
                            if delta_tx.send(d).await.is_err() {
                                info!("delta channel closed; stopping watcher");
                                return Ok(());
                            }
                        }
                        Ok(Some(Event::Deleted(o))) => {
                            let d = delta_from(&o, DeltaKind::Deleted)?;
                            if delta_tx.send(d).await.is_err() {
                                info!("delta channel closed; stopping watcher");
                                return Ok(());
                            }
                        }
                        Ok(Some(Event::Restarted(list))) => {
                            debug!(count = list.len(), "watch restart");
                            for o in list.iter() {
                                let d = delta_from(o, DeltaKind::Applied)?;
                                if delta_tx.send(d).await.is_err() {
                                    info!("delta channel closed; stopping watcher");
                                    return Ok(());
                                }
                            }
                        }
                        Ok(None) => break true, // stream ended
                        Err(e) => {
                            // Detect HTTP 410 Gone (Expired RV) and recover via full relist before restart.
                            let es = e.to_string();
                            if es.contains("410") || es.to_ascii_lowercase().contains("expired") {
                                warn!(error = %es, "watch stream expired (410); performing full relist to recover");
                                counter!("watch_errors_total", 1u64);
                                // Attempt a full relist to repair drift
                                if let Err(pe) = crate::prime_list(gvk_key, namespace, &delta_tx).await {
                                    warn!(error = %pe, "relist after 410 failed");
                                } else {
                                    counter!("relist_total", 1u64);
                                }
                                // After relist, break to restart watcher without extra delay
                                break true;
                            } else {
                                warn!(error = %e, "watch stream error; will backoff and restart");
                                counter!("watch_errors_total", 1u64);
                                break true;
                            }
                        }
                    }
                }
                _ = &mut relist_timer => {
                    info!("periodic relist interval reached; restarting watch");
                    counter!("relist_total", 1u64);
                    break false;
                }
            }
        };

        if ended {
            warn!("watcher stream ended");
            // Backoff before restart
            let dur = std::time::Duration::from_secs(backoff.min(backoff_max));
            histogram!("watch_backoff_ms", dur.as_millis() as f64);
            tokio::time::sleep(dur).await;
            backoff = (backoff * 2).min(backoff_max).max(1);
            counter!("watch_restarts_total", 1u64);
            continue;
        }
        // else: fallthrough and recreate stream (periodic relist)
        backoff = 1;
        counter!("watch_restarts_total", 1u64);
    }
    Ok(())
}

/// Perform an initial list for the given GVK and namespace and push Applied deltas.
/// Useful to prime the ingest snapshot before starting a long-running watch.
pub async fn prime_list(gvk_key: &str, namespace: Option<&str>, delta_tx: &mpsc::Sender<Delta>) -> Result<usize> {
    let client = Client::try_default().await?;
    let gvk = parse_gvk_key(gvk_key)?;
    let (ar, namespaced) = find_api_resource(client.clone(), &gvk).await?;

    let api: Api<DynamicObject> = if namespaced {
        match namespace {
            Some(ns) => Api::namespaced_with(client.clone(), ns, &ar),
            None => Api::all_with(client.clone(), &ar),
        }
    } else {
        Api::all_with(client.clone(), &ar)
    };

    let mut sent = 0usize;
    let list = api.list(&Default::default()).await?;
    for o in list {
        let d = delta_from(&o, DeltaKind::Applied)?;
        if delta_tx.send(d).await.is_ok() { sent += 1; }
    }
    Ok(sent)
}
</file>

<file path="Cargo.toml">
[workspace]
members = [
    "crates/core",
    "crates/api",
    "crates/kubehub",
    "crates/store",
    "crates/cli",
    "crates/schema",
    "crates/search",
    "crates/persist",
    "crates/apply",
    "crates/ops",
]
resolver = "2"

[workspace.dependencies]
anyhow = "1"
thiserror = "1"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
serde = { version = "1", features = ["derive"] }
serde_json = "1"
arc-swap = "1"
rustc-hash = "1"
tokio = { version = "1", features = ["rt-multi-thread", "macros", "signal", "sync", "time"] }
clap = { version = "4", features = ["derive", "env"] }
futures = "0.3"
uuid = { version = "1", features = ["v4"] }
chrono = { version = "0.4", default-features = false, features = ["std", "clock"] }
smallvec = { version = "1", features = ["serde"] }
fuzzy-matcher = "0.3"
regex = "1"

# Kube stack (exact version can be adjusted later)
kube = { version = "0.90", features = ["runtime", "client", "derive", "ws"] }
k8s-openapi = { version = "0.21", features = ["latest"] }
metrics = "0.21"
metrics-exporter-prometheus = "0.14"
</file>

<file path="crates/cli/src/main.rs">
use std::str::FromStr;

use anyhow::Result;
use clap::{ArgAction, Parser, Subcommand, ValueEnum};
use tracing::{error, info, warn};
use orka_store::spawn_ingest_with_planner;
use orka_api::{OrkaApi, InProcApi};
use std::sync::Arc;
use orka_core::ModuloNsPlanner;
use tokio::sync::mpsc;
use std::collections::HashMap;
use tokio::signal;
use std::time::{Duration, Instant};
use orka_persist::Store;
use orka_ops::{KubeOps, OrkaOps, LogOptions};
use regex::Regex;

#[derive(Parser, Debug)]
#[command(name = "orkactl", version, about = "Orka CLI (M2)")]
struct Cli {
    /// Output format
    #[arg(short = 'o', long = "output", value_enum, global = true, default_value_t = Output::Human)]
    output: Output,

    /// Kubernetes namespace (default: current context)
    #[arg(long = "ns", global = true)]
    namespace: Option<String>,

    #[command(subcommand)]
    command: Commands,
}

#[derive(Copy, Clone, Debug, Eq, PartialEq, ValueEnum)]
enum Output { Human, Json }

#[derive(Subcommand, Debug)]
enum Commands {
    /// Discover served resources (incl. CRDs)
    Discover {
        /// Prefer selecting a CRD (for demos)
        #[arg(long = "prefer-crd", action = ArgAction::SetTrue)]
        prefer_crd: bool,
    },
    /// List objects for a given group/version/kind key
    Ls {
        /// GVK key, e.g. "v1/ConfigMap" or "cert-manager.io/v1/Certificate"
        gvk: String,
    },
    /// Watch objects for a GVK and print +/- events
    Watch {
        /// GVK key, e.g. "v1/ConfigMap" or "cert-manager.io/v1/Certificate"
        gvk: String,
    },
    /// Inspect schema details for a GVK (CRDs only)
    Schema {
        /// GVK key, e.g. "cert-manager.io/v1/Certificate"
        gvk: String,
    },
    /// Search current snapshot (simple RAM index)
    Search {
        /// GVK key to watch while indexing
        gvk: String,
        /// Query string (supports free text + typed filters)
        query: String,
        /// Limit results
        #[arg(long = "limit", default_value_t = 20, env = "ORKA_SEARCH_LIMIT")]
        limit: usize,
        /// Maximum candidates after typed filters
        #[arg(long = "max-candidates", env = "ORKA_SEARCH_MAX_CANDIDATES")]
        max_candidates: Option<usize>,
        /// Minimum fuzzy score to include hit
        #[arg(long = "min-score", env = "ORKA_SEARCH_MIN_SCORE")]
        min_score: Option<f32>,
        /// Explain filter stages and counts
        #[arg(long = "explain", action = ArgAction::SetTrue)]
        explain: bool,
    },
    /// Edit a resource from a YAML file (dry-run or apply)
    Edit {
        /// YAML path or '-' for stdin
        #[arg(short = 'f', long = "file")]
        file: String,
        /// Validate against CRD JSONSchema (feature-gated)
        #[arg(long = "validate", action = ArgAction::SetTrue)]
        validate: bool,
        /// Perform a server-side dry-run
        #[arg(long = "dry-run", action = ArgAction::SetTrue)]
        dry_run: bool,
        /// Apply with SSA (fieldManager=orka)
        #[arg(long = "apply", action = ArgAction::SetTrue)]
        apply: bool,
    },
    /// Show minimal diffs vs live and last-applied
    Diff {
        /// YAML path or '-' for stdin
        #[arg(short = 'f', long = "file")]
        file: String,
    },
    /// Inspect last-applied snapshots for a resource
    #[command(name = "last-applied")]
    LastApplied {
        #[command(subcommand)]
        sub: LastAppliedCmd,
    },
    /// Show runtime configuration and metrics endpoint
    Stats {},
    /// Imperative operations against Kubernetes resources
    Ops {
        #[command(subcommand)]
        sub: OpsCmd,
    },
}

#[derive(Subcommand, Debug)]
enum OpsCmd {
    /// Stream logs from a pod/container
    Logs {
        /// Pod name
        pod: String,
        /// Container name (optional)
        #[arg(short = 'c', long = "container")]
        container: Vec<String>,
        /// Follow the log stream
        #[arg(long = "follow", default_value_t = true)]
        follow: bool,
        /// Tail the last N lines
        #[arg(long = "tail")] 
        tail_lines: Option<i64>,
        /// Since seconds filter
        #[arg(long = "since")] 
        since_seconds: Option<i64>,
        /// Regex filter (applied client-side)
        #[arg(long = "grep")]
        grep: Option<String>,
    },
    /// Execute a command in a pod (non-PTY by default)
    Exec {
        /// Pod name
        pod: String,
        /// Command and args (use -- to separate)
        #[arg(required=true)]
        cmd: Vec<String>,
        /// Container name
        #[arg(short = 'c', long = "container")]
        container: Option<String>,
        /// Allocate a TTY
        #[arg(long = "tty", default_value_t = false)]
        tty: bool,
    },
    /// Port-forward a remote pod port to local
    Pf {
        /// Pod name
        pod: String,
        /// Ports mapping: LOCAL:REMOTE (or single PORT for same)
        mapping: String,
    },
    /// Scale a resource to N replicas (supports subresource when available)
    Scale {
        /// GVK key, e.g. "apps/v1/Deployment"
        gvk: String,
        /// Resource name
        name: String,
        /// Replicas
        replicas: i32,
        /// Use Scale subresource (fallback to spec.replicas on failure)
        #[arg(long = "subresource", default_value_t = true)]
        subresource: bool,
    },
    /// Rollout restart (patch template annotation)
    #[command(name = "rr")]
    RolloutRestart {
        /// GVK key, e.g. "apps/v1/Deployment"
        gvk: String,
        /// Resource name
        name: String,
    },
    /// Delete a pod
    Delete {
        /// Pod name
        pod: String,
        /// Grace period seconds
        #[arg(long = "grace")] grace: Option<i64>,
    },
    /// Cordon/uncordon a node
    Cordon {
        /// Node name
        node: String,
        /// On/off (default: on)
        #[arg(long = "off", action = ArgAction::SetTrue)] off: bool,
    },
    /// Drain a node (basic eviction)
    Drain { node: String },
    /// Discover ops capabilities (RBAC + subresources)
    Caps {
        /// Optional GVK to probe Scale subresource (e.g. apps/v1/Deployment)
        #[arg(long = "gvk")]
        gvk: Option<String>,
    },
}

#[derive(Subcommand, Debug)]
enum LastAppliedCmd {
    /// Get last-applied snapshots for a resource
    Get {
        /// GVK key, e.g. "v1/ConfigMap" or "group/v1/Foo"
        gvk: String,
        /// Resource name
        name: String,
        /// Limit number of entries
        #[arg(long = "limit", default_value_t = 3)]
        limit: usize,
        /// Output YAML payloads as JSON array
        #[arg(short = 'o', long = "output", value_enum)]
        output: Option<Output>,
    },
}

fn init_tracing() {
    let env = std::env::var("ORKA_LOG").unwrap_or_else(|_| "info".to_string());
    let filter = tracing_subscriber::EnvFilter::from_str(&env).unwrap_or_else(|_| tracing_subscriber::EnvFilter::new("info"));
    tracing_subscriber::fmt().with_env_filter(filter).with_target(true).init();
}

fn init_metrics() {
    if let Ok(addr) = std::env::var("ORKA_METRICS_ADDR") {
        if let Ok(sock) = addr.parse::<std::net::SocketAddr>() {
            let builder = metrics_exporter_prometheus::PrometheusBuilder::new();
            match builder.with_http_listener(sock).install() {
                Ok(_) => tracing::info!(addr = %addr, "Prometheus metrics exporter listening"),
                Err(e) => tracing::warn!(error = %e, "failed to install metrics exporter"),
            }
        } else {
            tracing::warn!(addr = %addr, "invalid ORKA_METRICS_ADDR; expected host:port");
        }
    }
}

fn parse_gvk(key: &str) -> Option<(String, String, String)> {
    let parts: Vec<&str> = key.split('/').collect();
    match parts.as_slice() {
        [version, kind] => Some((String::new(), (*version).to_string(), (*kind).to_string())),
        [group, version, kind] => Some(((*group).to_string(), (*version).to_string(), (*kind).to_string())),
        _ => None,
    }
}

#[tokio::main]
async fn main() -> Result<()> {
    init_tracing();
    init_metrics();
    let cli = Cli::parse();
    let use_api = std::env::var("ORKA_USE_API").map(|v| v == "1" || v.eq_ignore_ascii_case("true")).unwrap_or(false);
    let api = if use_api { Some(InProcApi::new()) } else { None };

    match cli.command {
        Commands::Discover { prefer_crd } => {
            info!(prefer_crd, "discover invoked");
            if let Some(api) = &api {
                match api.discover().await {
                    Ok(resources) => match cli.output {
                        Output::Human => {
                            for r in resources {
                                let scope = if r.namespaced { "namespaced" } else { "cluster" };
                                let gv = if r.group.is_empty() { r.version.clone() } else { format!("{}/{}", r.group, r.version) };
                                println!("{} • {} • {}", gv, r.kind, scope);
                            }
                        }
                        Output::Json => println!("{}", serde_json::to_string_pretty(&resources)?),
                    },
                    Err(e) => {
                        error!(error = %e, "discover(api) failed");
                        eprintln!("discover error: {}", e);
                    }
                }
            } else {
                match orka_kubehub::discover(prefer_crd).await {
                    Ok(resources) => match cli.output {
                        Output::Human => {
                            for r in resources {
                                let scope = if r.namespaced { "namespaced" } else { "cluster" };
                                let gv = if r.group.is_empty() { r.version.clone() } else { format!("{}/{}", r.group, r.version) };
                                println!("{} • {} • {}", gv, r.kind, scope);
                            }
                        }
                        Output::Json => println!("{}", serde_json::to_string_pretty(&resources)?),
                    },
                    Err(e) => {
                        error!(error = ?e, "discover failed");
                        eprintln!("discover error: {}", e);
                    }
                }
            }
        }
        Commands::Ls { gvk } => {
            let ns = cli.namespace.as_deref();
            info!(gvk = %gvk, ns = ?ns, "ls invoked");
            if let Some(api) = &api {
                // Use API snapshot for single-GVK listing
                let (group_str, version_str, kind_str) = parse_gvk(&gvk).unwrap_or((String::new(), String::new(), String::new()));
                let sel = orka_api::Selector { gvk: orka_api::ResourceKind { group: group_str, version: version_str, kind: kind_str, namespaced: ns.is_some() }, namespace: ns.map(|s| s.to_string()) };
                let snap = api.snapshot(sel).await?;
                match cli.output {
                    Output::Human => {
                        println!("NAMESPACE   NAME                 AGE");
                        for item in snap.items.iter().filter(|o| ns.map(|n| o.namespace.as_deref() == Some(n)).unwrap_or(true)) {
                            let ns_col = item.namespace.clone().unwrap_or_else(|| "-".to_string());
                            let age = render_age(item.creation_ts);
                            println!("{:<11} {:<20} {}", ns_col, item.name, age);
                        }
                    }
                    Output::Json => {
                        // Filter by namespace if provided
                        let items: Vec<_> = snap.items
                            .iter()
                            .filter(|o| ns.map(|n| o.namespace.as_deref() == Some(n)).unwrap_or(true))
                            .cloned()
                            .collect();
                        println!("{}", serde_json::to_string_pretty(&items)?);
                    }
                }
            } else {
                let cap = std::env::var("ORKA_QUEUE_CAP").ok().and_then(|s| s.parse::<usize>().ok()).unwrap_or(2048);
                let projector = match orka_schema::fetch_crd_schema(&gvk).await {
                    Ok(Some(schema)) => Some(std::sync::Arc::new(schema.projector()) as std::sync::Arc<dyn orka_core::Projector + Send + Sync>),
                    _ => None,
                };
                let shards = std::env::var("ORKA_SHARDS").ok().and_then(|s| s.parse().ok()).unwrap_or(1);
                let planner = Arc::new(ModuloNsPlanner::new(shards));
                let (ingest_tx, backend) = spawn_ingest_with_planner(cap, projector, Some(planner));
                // Start watcher
                let watcher_handle = tokio::spawn({
                    let gvk = gvk.clone();
                    let ns = ns.map(|s| s.to_string());
                    let tx = ingest_tx.clone();
                    async move {
                        if let Err(e) = orka_kubehub::start_watcher(&gvk, ns.as_deref(), tx).await {
                            error!(error = ?e, "watcher failed");
                        }
                    }
                });
                // Prime initial list for faster first snapshot
                let _ = orka_kubehub::prime_list(&gvk, ns, &ingest_tx).await;

                // Wait for first epoch (configurable)
                let wait_secs = std::env::var("ORKA_WAIT_SECS").ok().and_then(|s| s.parse::<u64>().ok()).unwrap_or(8);
                let mut rx = backend.subscribe_epoch();
                let deadline = Instant::now() + Duration::from_secs(wait_secs);
                while *rx.borrow() == 0 {
                    let now = Instant::now();
                    if now >= deadline { break; }
                    let rem = deadline.duration_since(now).min(Duration::from_secs(2));
                    if tokio::time::timeout(rem, rx.changed()).await.is_err() { break; }
                }
                let snap = backend.current();

                match cli.output {
                    Output::Human => {
                        println!("NAMESPACE   NAME                 AGE");
                        for item in snap.items.iter().filter(|o| ns.map(|n| o.namespace.as_deref() == Some(n)).unwrap_or(true)) {
                            let ns_col = item.namespace.clone().unwrap_or_else(|| "-".to_string());
                            let age = render_age(item.creation_ts);
                            println!("{:<11} {:<20} {}", ns_col, item.name, age);
                        }
                    }
                    Output::Json => {
                        // Filter by namespace if provided
                        let items: Vec<_> = snap.items
                            .iter()
                            .filter(|o| ns.map(|n| o.namespace.as_deref() == Some(n)).unwrap_or(true))
                            .cloned()
                            .collect();
                        println!("{}", serde_json::to_string_pretty(&items)?);
                    }
                }
                // Graceful shutdown: drop last sender and abort watcher to close ingest and flush
                drop(ingest_tx);
                watcher_handle.abort();
            }
        }
        Commands::Watch { gvk } => {
            let ns = cli.namespace.as_deref();
            info!(gvk = %gvk, ns = ?ns, "watch invoked");
            if let Some(api) = &api {
                // API streaming path
                let (group_str, version_str, kind_str) = parse_gvk(&gvk).unwrap_or((String::new(), String::new(), String::new()));
                let sel = orka_api::Selector { gvk: orka_api::ResourceKind { group: group_str, version: version_str, kind: kind_str, namespaced: ns.is_some() }, namespace: ns.map(|s| s.to_string()) };
                let mut handle = api.watch_lite(sel).await?;
                loop {
                    tokio::select! {
                        maybe = handle.rx.recv() => {
                            match maybe {
                                Some(ev) => {
                                    match ev {
                                        orka_api::LiteEvent::Applied(lo) => {
                                            let key = if let Some(ns) = &lo.namespace { format!("{}/{}", ns, lo.name) } else { lo.name.clone() };
                                            println!("+ {}", key);
                                        }
                                        orka_api::LiteEvent::Deleted(lo) => {
                                            let key = if let Some(ns) = &lo.namespace { format!("{}/{}", ns, lo.name) } else { lo.name.clone() };
                                            println!("- {}", key);
                                        }
                                    }
                                }
                                None => { warn!("watch(api) channel closed"); break; }
                            }
                        }
                        _ = signal::ctrl_c() => { handle.cancel.cancel(); break; }
                    }
                }
                warn!("watch(api) ended");
            } else {
                // existing path
                let cap = std::env::var("ORKA_QUEUE_CAP").ok().and_then(|s| s.parse::<usize>().ok()).unwrap_or(2048);
                let projector = match orka_schema::fetch_crd_schema(&gvk).await {
                    Ok(Some(schema)) => Some(std::sync::Arc::new(schema.projector()) as std::sync::Arc<dyn orka_core::Projector + Send + Sync>),
                    _ => None,
                };
                let shards = std::env::var("ORKA_SHARDS").ok().and_then(|s| s.parse().ok()).unwrap_or(1);
                let planner = Arc::new(ModuloNsPlanner::new(shards));
                let (ingest_tx, _backend) = spawn_ingest_with_planner(cap, projector, Some(planner));
                let (tap_tx, mut tap_rx) = mpsc::channel::<orka_core::Delta>(cap);
                let watcher_handle = tokio::spawn({
                    let gvk = gvk.clone();
                    let ns = ns.map(|s| s.to_string());
                    let tap_tx = tap_tx.clone();
                    async move {
                        if let Err(e) = orka_kubehub::start_watcher(&gvk, ns.as_deref(), tap_tx).await {
                            error!(error = ?e, "watcher failed");
                        }
                    }
                });
                let mut seen_rv: HashMap<orka_core::Uid, String> = HashMap::new();
                loop {
                    tokio::select! {
                        maybe = tap_rx.recv() => {
                            match maybe {
                                Some(d) => {
                                    let key = json_key(&d.raw);
                                    match d.kind {
                                        orka_core::DeltaKind::Applied => {
                                            let rv = d.raw.get("metadata").and_then(|m| m.get("resourceVersion")).and_then(|v| v.as_str()).unwrap_or("").to_string();
                                            match seen_rv.get_mut(&d.uid) {
                                                None => { seen_rv.insert(d.uid, rv); println!("+ {}", key); }
                                                Some(prev_rv) => { if *prev_rv != rv { *prev_rv = rv; println!("+ {}", key); } }
                                            }
                                        }
                                        orka_core::DeltaKind::Deleted => { let _ = seen_rv.remove(&d.uid); println!("- {}", key); }
                                    }
                                }
                                None => { warn!("tap channel closed; exiting watch loop"); break; }
                            }
                        }
                        _ = signal::ctrl_c() => { info!("Ctrl-C received; shutting down watch loop"); break; }
                    }
                }
                drop(ingest_tx);
                watcher_handle.abort();
                warn!("watch loop ended (graceful shutdown)");
            }
        }
        Commands::Schema { gvk } => {
            info!(gvk = %gvk, "schema invoked");
            if let Some(api) = &api {
                match api.schema(&gvk).await {
                    Ok(Some(schema)) => match cli.output {
                        Output::Human => {
                            println!("served: {}", schema.served_version);
                            if schema.printer_cols.is_empty() {
                                println!("printer-cols: (none)");
                            } else {
                                let cols: Vec<_> = schema.printer_cols.iter().map(|c| c.name.as_str()).collect();
                                println!("printer-cols: {}", cols.join(", "));
                            }
                            if schema.projected_paths.is_empty() {
                                println!("projected: (heuristic defaults)");
                            } else {
                                let proj: Vec<_> = schema.projected_paths.iter().map(|p| p.json_path.as_str()).collect();
                                println!("projected: {}", proj.join(", "));
                            }
                        }
                        Output::Json => {
                            println!("{}", serde_json::to_string_pretty(&schema)?);
                        }
                    },
                    Ok(None) => { eprintln!("no CRD schema for builtin kind (or not found)"); }
                    Err(e) => { eprintln!("schema(api) error: {}", e); }
                }
            } else {
                match orka_schema::fetch_crd_schema(&gvk).await {
                    Ok(Some(schema)) => match cli.output {
                        Output::Human => {
                            println!("served: {}", schema.served_version);
                            if schema.printer_cols.is_empty() {
                                println!("printer-cols: (none)");
                            } else {
                                let cols: Vec<_> = schema.printer_cols.iter().map(|c| c.name.as_str()).collect();
                                println!("printer-cols: {}", cols.join(", "));
                            }
                            if schema.projected_paths.is_empty() {
                                println!("projected: (heuristic defaults)");
                            } else {
                                let proj: Vec<_> = schema.projected_paths.iter().map(|p| p.json_path.as_str()).collect();
                                println!("projected: {}", proj.join(", "));
                            }
                        }
                        Output::Json => { println!("{}", serde_json::to_string_pretty(&schema)?); }
                    },
                    Ok(None) => { eprintln!("no CRD schema for builtin kind (or not found)"); }
                    Err(e) => { eprintln!("schema error: {}", e); }
                }
            }
        }
        Commands::Search { gvk, query, limit, max_candidates, min_score, explain } => {
            // Choose watcher namespace: CLI --ns overrides, else extract from query ns:token
            let ns_from_query = query.split_whitespace().find_map(|t| t.strip_prefix("ns:")).map(|s| s.to_string());
            let effective_ns = cli.namespace.clone().or(ns_from_query);
            let ns = effective_ns.as_deref();
            info!(gvk = %gvk, ns = ?ns, query = %query, limit, "search invoked");
            if let Some(api) = &api {
                let (group_str, version_str, kind_str) = parse_gvk(&gvk).unwrap_or((String::new(), String::new(), String::new()));
                let sel = orka_api::Selector { gvk: orka_api::ResourceKind { group: group_str.clone(), version: version_str, kind: kind_str.clone(), namespaced: ns.is_some() }, namespace: ns.map(|s| s.to_string()) };
                let snap = api.snapshot(sel.clone()).await?;
                let (hits, dbg) = api.search(sel, &query, limit).await?;

                match cli.output {
                    Output::Human => {
                        println!("KIND   NAMESPACE/NAME                SCORE");
                        for h in hits {
                            if let Some(obj) = snap.items.get(h.doc as usize) {
                                let ns_col = obj.namespace.clone().unwrap_or_else(|| "-".to_string());
                                println!("{:<6} {:<22} {:<20} {:.2}", kind_str, format!("{}/{}", ns_col, obj.name), "", h.score);
                            }
                        }
                    }
                    Output::Json => {
                        #[derive(serde::Serialize)]
                        struct Row<'a> { ns: &'a str, name: &'a str, score: f32 }
                        let rows: Vec<_> = hits
                            .into_iter()
                            .filter_map(|h| {
                                snap.items.get(h.doc as usize).map(|o| Row { ns: o.namespace.as_deref().unwrap_or("") , name: &o.name, score: h.score })
                            })
                            .collect();
                        if explain {
                            #[derive(serde::Serialize)]
                            struct Explain<'a, T> { hits: T, debug: &'a orka_search::SearchDebugInfo }
                            println!("{}", serde_json::to_string_pretty(&Explain { hits: rows, debug: &dbg })?);
                        } else {
                            println!("{}", serde_json::to_string_pretty(&rows)?);
                        }
                    }
                }
                if explain && matches!(cli.output, Output::Human) {
                    eprintln!("debug: total={} after_ns={} after_label_keys={} after_labels={} after_anno_keys={} after_annos={} after_fields={}", dbg.total, dbg.after_ns, dbg.after_label_keys, dbg.after_labels, dbg.after_anno_keys, dbg.after_annos, dbg.after_fields);
                }
                // done via API path
                return Ok(());
            }
            let cap = std::env::var("ORKA_QUEUE_CAP").ok().and_then(|s| s.parse::<usize>().ok()).unwrap_or(2048);
            let projector = match orka_schema::fetch_crd_schema(&gvk).await {
                Ok(Some(schema)) => Some(std::sync::Arc::new(schema.projector()) as std::sync::Arc<dyn orka_core::Projector + Send + Sync>),
                _ => None,
            };
            let shards = std::env::var("ORKA_SHARDS").ok().and_then(|s| s.parse().ok()).unwrap_or(1);
            let planner = Arc::new(ModuloNsPlanner::new(shards));
            let (ingest_tx, backend) = spawn_ingest_with_planner(cap, projector, Some(planner));
            // Start watcher
            let watcher_handle = tokio::spawn({
                let gvk = gvk.clone();
                let ns = ns.map(|s| s.to_string());
                let tx = ingest_tx.clone();
                async move {
                    if let Err(e) = orka_kubehub::start_watcher(&gvk, ns.as_deref(), tx).await {
                        error!(error = ?e, "watcher failed");
                    }
                }
            });
            // Prime initial list so snapshot has data before waiting
            let _ = orka_kubehub::prime_list(&gvk, ns, &ingest_tx).await;

            // Wait for first epoch (configurable)
            let wait_secs = std::env::var("ORKA_WAIT_SECS").ok().and_then(|s| s.parse::<u64>().ok()).unwrap_or(8);
            let mut rx = backend.subscribe_epoch();
            let deadline = Instant::now() + Duration::from_secs(wait_secs);
            while *rx.borrow() == 0 {
                let now = Instant::now();
                if now >= deadline { break; }
                let rem = deadline.duration_since(now).min(Duration::from_secs(2));
                if tokio::time::timeout(rem, rx.changed()).await.is_err() { break; }
            }
            let snap = backend.current();
            // Build index with field mapping (if schema known)
            let field_pairs: Option<Vec<(String, u32)>> = match orka_schema::fetch_crd_schema(&gvk).await {
                Ok(Some(schema)) => Some(schema.projected_paths.iter().map(|p| (p.json_path.clone(), p.id)).collect()),
                _ => None,
            };
            let (group_str, _version_str, kind_str) = parse_gvk(&gvk).unwrap_or((String::new(), String::new(), String::new()));
            let index = match field_pairs {
                Some(pairs) => orka_search::Index::build_from_snapshot_with_meta(&snap, Some(&pairs), Some(&kind_str), Some(&group_str)),
                None => orka_search::Index::build_from_snapshot_with_meta(&snap, None, Some(&kind_str), Some(&group_str)),
            };
            let opts = orka_search::SearchOpts { max_candidates, min_score };
            let (hits, dbg) = index.search_with_debug_opts(&query, limit, opts);

            match cli.output {
                Output::Human => {
                    println!("KIND   NAMESPACE/NAME                SCORE");
                    for h in hits {
                        if let Some(obj) = snap.items.get(h.doc as usize) {
                            let ns_col = obj.namespace.clone().unwrap_or_else(|| "-".to_string());
                            println!("{:<6} {:<22} {:<20} {:.2}", kind_str, format!("{}/{}", ns_col, obj.name), "", h.score);
                        }
                    }
                }
                Output::Json => {
                    #[derive(serde::Serialize)]
                    struct Row<'a> { ns: &'a str, name: &'a str, score: f32 }
                    let rows: Vec<_> = hits
                        .into_iter()
                        .filter_map(|h| {
                            snap.items.get(h.doc as usize).map(|o| Row { ns: o.namespace.as_deref().unwrap_or("") , name: &o.name, score: h.score })
                        })
                        .collect();
                    if explain {
                        #[derive(serde::Serialize)]
                        struct Explain<'a, T> { hits: T, debug: &'a orka_search::SearchDebugInfo }
                        println!("{}", serde_json::to_string_pretty(&Explain { hits: rows, debug: &dbg })?);
                    } else {
                        println!("{}", serde_json::to_string_pretty(&rows)?);
                    }
                }
            }
            if explain && matches!(cli.output, Output::Human) {
                eprintln!("debug: total={} after_ns={} after_label_keys={} after_labels={} after_anno_keys={} after_annos={} after_fields={}", dbg.total, dbg.after_ns, dbg.after_label_keys, dbg.after_labels, dbg.after_anno_keys, dbg.after_annos, dbg.after_fields);
            }

            // Shutdown
            drop(ingest_tx);
            watcher_handle.abort();
        }
        Commands::Edit { file, validate, dry_run, apply } => {
            let ns = cli.namespace.as_deref();
            let yaml = read_input(&file)?;
            if validate {
                #[cfg(feature = "validate")]
                {
                    // Detect GVK from YAML for schema lookup
                    let j: serde_yaml::Value = serde_yaml::from_str(&yaml)?;
                    let api_ver = j.get("apiVersion").and_then(|v| v.as_str()).unwrap_or("");
                    let kind = j.get("kind").and_then(|v| v.as_str()).unwrap_or("");
                    let gvk_key = if api_ver.contains('/') { format!("{}/{}", api_ver, kind) } else { format!("{}/{}", api_ver, kind) };
                    let issues = orka_schema::validate::validate_yaml_for_gvk(&gvk_key, &yaml).await?;
                    if !issues.is_empty() {
                        eprintln!("validation issues ({}):", issues.len());
                        for it in issues { eprintln!("- {}: {}{}", it.path, it.error, it.hint.as_deref().map(|h| format!(" ({})", h)).unwrap_or_default()); }
                    }
                }
                #[cfg(not(feature = "validate"))]
                {
                    warn!("validate flag set but CLI built without 'validate' feature");
                }
            }
            let do_apply = if apply { true } else { !dry_run };
            if let Some(api) = &api {
                let res = if do_apply {
                    api.apply(&yaml).await
                } else {
                    api.dry_run(&yaml).await.map(|d| orka_apply::ApplyResult { dry_run: true, applied: false, new_rv: None, warnings: vec![], summary: d })
                };
                match res {
                    Ok(res) => match cli.output {
                        Output::Human => {
                            if res.dry_run {
                                println!("dry-run: +{} ~{} -{}", res.summary.adds, res.summary.updates, res.summary.removes);
                            } else if res.applied {
                                println!("applied rv={}", res.new_rv.unwrap_or_default());
                            } else {
                                println!("no-op");
                            }
                        }
                        Output::Json => println!("{}", serde_json::to_string_pretty(&res)?),
                    },
                    Err(e) => eprintln!("edit error: {}", e),
                }
            } else {
                match orka_apply::edit_from_yaml(&yaml, ns, validate, do_apply).await {
                    Ok(res) => match cli.output {
                        Output::Human => {
                            if res.dry_run {
                                println!("dry-run: +{} ~{} -{}", res.summary.adds, res.summary.updates, res.summary.removes);
                            } else if res.applied {
                                println!("applied rv={}", res.new_rv.unwrap_or_default());
                            } else {
                                println!("no-op");
                            }
                        }
                        Output::Json => println!("{}", serde_json::to_string_pretty(&res)?),
                    },
                    Err(e) => { eprintln!("edit error: {}", e); }
                }
            }
        }
        Commands::Diff { file } => {
            let ns = cli.namespace.as_deref();
            let yaml = read_input(&file)?;
            match orka_apply::diff_from_yaml(&yaml, ns).await {
                Ok((live, last)) => match cli.output {
                    Output::Human => {
                        println!("vs live: +{} ~{} -{}", live.adds, live.updates, live.removes);
                        if let Some(ls) = last { println!("vs last: +{} ~{} -{}", ls.adds, ls.updates, ls.removes); }
                    }
                    Output::Json => {
                        #[derive(serde::Serialize)]
                        struct D { live: orka_apply::DiffSummary, last: Option<orka_apply::DiffSummary> }
                        println!("{}", serde_json::to_string_pretty(&D { live, last })?);
                    }
                },
                Err(e) => eprintln!("diff error: {}", e),
            }
        }
        Commands::LastApplied { sub } => {
            match sub {
                LastAppliedCmd::Get { gvk, name, limit, output } => {
                    // Resolve UID by fetching live object
                    let ns = cli.namespace.as_deref();
                    let uid_hex = fetch_uid_for(&gvk, &name, ns).await?;
                    let uid = parse_uid(&uid_hex)?;
                    let store = match orka_persist::SqliteStore::open_default() { Ok(s) => s, Err(e) => { eprintln!("open db error: {}", e); return Ok(()); } };
                    let rows = store.get_last(uid, Some(limit)).unwrap_or_default();
                    match output.unwrap_or(cli.output) {
                        Output::Human => {
                            for r in rows.iter() {
                                let ts = r.ts;
                                println!("ts={} rv={}", ts, r.rv);
                            }
                        }
                        Output::Json => {
                            #[derive(serde::Serialize)]
                            struct Row { ts: i64, rv: String, yaml: String }
                            let out: Vec<Row> = rows.into_iter().map(|r| Row { ts: r.ts, rv: r.rv, yaml: orka_persist::maybe_decompress(&r.yaml_zstd) }).collect();
                            println!("{}", serde_json::to_string_pretty(&out)?);
                        }
                    }
                }
            }
        }
        Commands::Stats {} => {
            // Gather via API when enabled, else from env directly
            #[derive(serde::Serialize)]
            struct StatsOut {
                shards: usize,
                relist_secs: u64,
                watch_backoff_max_secs: u64,
                max_labels_per_obj: Option<usize>,
                max_annos_per_obj: Option<usize>,
                max_postings_per_key: Option<usize>,
                max_rss_mb: Option<usize>,
                max_index_bytes: Option<usize>,
                metrics_addr: Option<String>,
            }
            let out = if let Some(api) = &api {
                let s = api.stats().await?;
                StatsOut { shards: s.shards, relist_secs: s.relist_secs, watch_backoff_max_secs: s.watch_backoff_max_secs, max_labels_per_obj: s.max_labels_per_obj, max_annos_per_obj: s.max_annos_per_obj, max_postings_per_key: s.max_postings_per_key, max_rss_mb: s.max_rss_mb, max_index_bytes: s.max_index_bytes, metrics_addr: s.metrics_addr }
            } else {
                let shards = std::env::var("ORKA_SHARDS").ok().and_then(|s| s.parse().ok()).unwrap_or(1);
                let relist_secs = std::env::var("ORKA_RELIST_SECS").ok().and_then(|s| s.parse().ok()).unwrap_or(300);
                let watch_backoff_max_secs = std::env::var("ORKA_WATCH_BACKOFF_MAX_SECS").ok().and_then(|s| s.parse().ok()).unwrap_or(30);
                let max_labels_per_obj = std::env::var("ORKA_MAX_LABELS_PER_OBJ").ok().and_then(|s| s.parse().ok());
                let max_annos_per_obj = std::env::var("ORKA_MAX_ANNOS_PER_OBJ").ok().and_then(|s| s.parse().ok());
                let max_postings_per_key = std::env::var("ORKA_MAX_POSTINGS_PER_KEY").ok().and_then(|s| s.parse().ok());
                let max_rss_mb = std::env::var("ORKA_MAX_RSS_MB").ok().and_then(|s| s.parse().ok());
                let max_index_bytes = std::env::var("ORKA_MAX_INDEX_BYTES").ok().and_then(|s| s.parse().ok());
                let metrics_addr = std::env::var("ORKA_METRICS_ADDR").ok();
                StatsOut { shards, relist_secs, watch_backoff_max_secs, max_labels_per_obj, max_annos_per_obj, max_postings_per_key, max_rss_mb, max_index_bytes, metrics_addr }
            };

            match cli.output {
                Output::Human => {
                    println!("shards: {}", out.shards);
                    println!("relist_secs: {}", out.relist_secs);
                    println!("watch_backoff_max_secs: {}", out.watch_backoff_max_secs);
                    println!("max_labels_per_obj: {}", out.max_labels_per_obj.map(|v| v.to_string()).unwrap_or_else(|| "(none)".into()));
                    println!("max_annos_per_obj: {}", out.max_annos_per_obj.map(|v| v.to_string()).unwrap_or_else(|| "(none)".into()));
                    println!("max_postings_per_key: {}", out.max_postings_per_key.map(|v| v.to_string()).unwrap_or_else(|| "(none)".into()));
                    println!("max_rss_mb: {}", out.max_rss_mb.map(|v| v.to_string()).unwrap_or_else(|| "(none)".into()));
                    println!("max_index_bytes: {}", out.max_index_bytes.map(|v| v.to_string()).unwrap_or_else(|| "(none)".into()));
                    if let Some(addr) = out.metrics_addr { println!("metrics_addr: {} (exposes Prometheus /metrics)", addr); } else { println!("metrics_addr: (not set)"); }
                }
                Output::Json => println!("{}", serde_json::to_string_pretty(&out)?),
            }
        }
        Commands::Ops { sub } => {
            match sub {
                OpsCmd::Logs { pod, container, follow, tail_lines, since_seconds, grep } => {
                    let ns = cli.namespace.as_deref();
                    if ns.is_none() { eprintln!("--ns is required for pod logs"); return Ok(()); }
                    let ops = KubeOps::new();
                    let opts = LogOptions { follow, tail_lines, since_seconds };
                    let re = match grep { Some(pat) => match Regex::new(&pat) { Ok(r) => Some(r), Err(e) => { eprintln!("invalid regex: {}", e); return Ok(()); } }, None => None };
                    if container.len() <= 1 {
                        let single = container.get(0).map(|s| s.as_str());
                        match ops.logs(ns, &pod, single, opts).await {
                            Ok(mut handle) => {
                                loop {
                                    tokio::select! {
                                        _ = signal::ctrl_c() => { handle.cancel.cancel(); break; }
                                        maybe = handle.rx.recv() => {
                                            match maybe {
                                                Some(chunk) => {
                                                    if let Some(ref r) = re { if !r.is_match(&chunk.line) { continue; } }
                                                    match cli.output {
                                                        Output::Human => println!("{}", chunk.line),
                                                        Output::Json => println!("{}", serde_json::to_string(&chunk).unwrap_or_else(|_| "{}".into())),
                                                    }
                                                }
                                                None => break,
                                            }
                                        }
                                    }
                                }
                            }
                            Err(e) => {
                                if let Some(ae) = e.downcast_ref::<kube::Error>() { if let kube::Error::Api(api_err) = ae { if api_err.code == 403 { eprintln!("forbidden: missing get on pods/log in ns"); } else { eprintln!("logs error: {}", e); } } else { eprintln!("logs error: {}", e); } } else { eprintln!("logs error: {}", e); }
                            }
                        }
                    } else {
                        // Multi-container: spawn one stream per container and merge
                        use tokio::sync::mpsc;
                        #[derive(serde::Serialize)]
                        struct MultiLine<'a> { container: &'a str, line: &'a str }
                        let (tx, mut rx) = mpsc::channel::<(String, String)>(1024);
                        let mut handles = Vec::new();
                        let ns_owned = cli.namespace.clone();
                        for c in container.iter().cloned() {
                            let pod = pod.clone();
                            let opts = LogOptions { follow, tail_lines, since_seconds };
                            let ops = KubeOps::new();
                            let txc = tx.clone();
                            let re = re.as_ref().map(|r| r.as_str().to_string());
                            let ns_owned = ns_owned.clone();
                            let h = tokio::spawn(async move {
                                match ops.logs(ns_owned.as_deref(), &pod, Some(&c), opts).await {
                                    Ok(mut handle) => {
                                        while let Some(chunk) = handle.rx.recv().await {
                                            if let Some(ref pat) = re { if let Ok(r) = Regex::new(pat) { if !r.is_match(&chunk.line) { continue; } } }
                                            let _ = txc.send((c.clone(), chunk.line)).await;
                                        }
                                    }
                                    Err(_) => { /* ignore individual errors; main thread will report */ }
                                }
                            });
                            handles.push(h);
                        }
                        drop(tx);
                        loop {
                            tokio::select! {
                                _ = signal::ctrl_c() => { break; }
                                maybe = rx.recv() => {
                                    match maybe {
                                        Some((c, line)) => match cli.output {
                                            Output::Human => println!("[{}] {}", c, line),
                                            Output::Json => println!("{}", serde_json::to_string(&MultiLine { container: &c, line: &line }).unwrap_or_else(|_| "{}".into())),
                                        },
                                        None => break,
                                    }
                                }
                            }
                        }
                        for h in handles { h.abort(); }
                    }
                }
                OpsCmd::Exec { pod, cmd, container, tty } => {
                    let ns = cli.namespace.as_deref();
                    if ns.is_none() { eprintln!("--ns is required for exec"); return Ok(()); }
                    let ops = KubeOps::new();
                    if let Err(e) = ops.exec(ns, &pod, container.as_deref(), &cmd, tty).await {
                        if let Some(ae) = e.downcast_ref::<kube::Error>() { if let kube::Error::Api(api_err) = ae { if api_err.code == 403 { eprintln!("forbidden: missing create on pods/exec in ns"); } else { eprintln!("exec error: {}", e); } } else { eprintln!("exec error: {}", e); } } else { eprintln!("exec error: {}", e); }
                    }
                }
                OpsCmd::Pf { pod, mapping } => {
                    let ns = cli.namespace.as_deref();
                    if ns.is_none() { eprintln!("--ns is required for pf"); return Ok(()); }
                    let (local, remote) = parse_port_mapping(&mapping)?;
                    let ops = KubeOps::new();
                    match ops.port_forward(ns, &pod, local, remote).await {
                        Ok(mut handle) => {
                            // Wait for events; exit on Ctrl-C
                            loop {
                                tokio::select! {
                                    _ = signal::ctrl_c() => { handle.cancel.cancel(); break; }
                                    maybe = handle.rx.recv() => {
                                        match maybe {
                                            Some(e) => match cli.output {
                                                Output::Human => render_pf_event(&e),
                                                Output::Json => println!("{}", serde_json::to_string(&e).unwrap_or_else(|_| "{}".into())),
                                            },
                                            None => break,
                                        }
                                    }
                                }
                            }
                        }
                        Err(e) => {
                            if let Some(ae) = e.downcast_ref::<kube::Error>() { if let kube::Error::Api(api_err) = ae { if api_err.code == 403 { eprintln!("forbidden: missing create on pods/portforward in ns"); } else { eprintln!("pf error: {}", e); } } else { eprintln!("pf error: {}", e); } } else { eprintln!("pf error: {}", e); }
                        }
                    }
                }
                OpsCmd::Scale { gvk, name, replicas, subresource } => {
                    let ns = cli.namespace.as_deref();
                    let ops = KubeOps::new();
                    match ops.scale(&gvk, ns, &name, replicas, subresource).await {
                        Ok(_) => match cli.output {
                            Output::Human => println!("scaled {} to {}", name, replicas),
                            Output::Json => {
                                #[derive(serde::Serialize)]
                                struct Out<'a> { op: &'a str, gvk: &'a str, name: &'a str, replicas: i32, used_subresource: bool, status: &'a str }
                                println!("{}", serde_json::to_string_pretty(&Out { op: "scale", gvk: &gvk, name: &name, replicas, used_subresource: subresource, status: "ok" })?);
                            }
                        },
                        Err(e) => {
                            if let Some(ae) = e.downcast_ref::<kube::Error>() { if let kube::Error::Api(api_err) = ae { if api_err.code == 403 { eprintln!("forbidden: missing patch on {}/scale or {} in ns", gvk, gvk); } else { eprintln!("scale error: {}", e); } } else { eprintln!("scale error: {}", e); } } else { eprintln!("scale error: {}", e); }
                        }
                    }
                }
                OpsCmd::RolloutRestart { gvk, name } => {
                    let ns = cli.namespace.as_deref();
                    let ops = KubeOps::new();
                    match ops.rollout_restart(&gvk, ns, &name).await {
                        Ok(_) => match cli.output {
                            Output::Human => println!("restarted {}", name),
                            Output::Json => {
                                #[derive(serde::Serialize)]
                                struct Out<'a> { op: &'a str, gvk: &'a str, name: &'a str, status: &'a str }
                                println!("{}", serde_json::to_string_pretty(&Out { op: "rollout_restart", gvk: &gvk, name: &name, status: "ok" })?);
                            }
                        },
                        Err(e) => {
                            if let Some(ae) = e.downcast_ref::<kube::Error>() { if let kube::Error::Api(api_err) = ae { if api_err.code == 403 { eprintln!("forbidden: missing patch on {} in ns", gvk); } else { eprintln!("rollout-restart error: {}", e); } } else { eprintln!("rollout-restart error: {}", e); } } else { eprintln!("rollout-restart error: {}", e); }
                        }
                    }
                }
                OpsCmd::Delete { pod, grace } => {
                    let ns = cli.namespace.as_deref().ok_or_else(|| anyhow::anyhow!("--ns required for delete"))?;
                    let ops = KubeOps::new();
                    match ops.delete_pod(ns, &pod, grace).await {
                        Ok(_) => match cli.output {
                            Output::Human => println!("deleted {}", pod),
                            Output::Json => {
                                #[derive(serde::Serialize)]
                                struct Out<'a> { op: &'a str, namespace: &'a str, pod: &'a str, grace: Option<i64>, status: &'a str }
                                println!("{}", serde_json::to_string_pretty(&Out { op: "delete_pod", namespace: ns, pod: &pod, grace, status: "ok" })?);
                            }
                        },
                        Err(e) => {
                            if let Some(ae) = e.downcast_ref::<kube::Error>() { if let kube::Error::Api(api_err) = ae { if api_err.code == 403 { eprintln!("forbidden: missing delete on pods in ns"); } else { eprintln!("delete error: {}", e); } } else { eprintln!("delete error: {}", e); } } else { eprintln!("delete error: {}", e); }
                        }
                    }
                }
                OpsCmd::Cordon { node, off } => {
                    let ops = KubeOps::new();
                    match ops.cordon(&node, !off).await {
                        Ok(_) => match cli.output {
                            Output::Human => println!("{} {}", if off {"uncordoned"} else {"cordoned"}, node),
                            Output::Json => {
                                #[derive(serde::Serialize)]
                                struct Out<'a> { op: &'a str, node: &'a str, on: bool, status: &'a str }
                                println!("{}", serde_json::to_string_pretty(&Out { op: "cordon", node: &node, on: !off, status: "ok" })?);
                            }
                        },
                        Err(e) => {
                            if let Some(ae) = e.downcast_ref::<kube::Error>() { if let kube::Error::Api(api_err) = ae { if api_err.code == 403 { eprintln!("forbidden: missing patch on nodes"); } else { eprintln!("cordon error: {}", e); } } else { eprintln!("cordon error: {}", e); } } else { eprintln!("cordon error: {}", e); }
                        }
                    }
                }
                OpsCmd::Drain { node } => {
                    let ops = KubeOps::new();
                    match ops.drain(&node).await {
                        Ok(_) => match cli.output {
                            Output::Human => println!("drain initiated for {}", node),
                            Output::Json => {
                                #[derive(serde::Serialize)]
                                struct Out<'a> { op: &'a str, node: &'a str, status: &'a str }
                                println!("{}", serde_json::to_string_pretty(&Out { op: "drain", node: &node, status: "ok" })?);
                            }
                        },
                        Err(e) => {
                            if let Some(ae) = e.downcast_ref::<kube::Error>() { if let kube::Error::Api(api_err) = ae { if api_err.code == 403 { eprintln!("forbidden: missing create on pods/eviction and/or delete on pods across namespaces"); } else { eprintln!("drain error: {}", e); } } else { eprintln!("drain error: {}", e); } } else { eprintln!("drain error: {}", e); }
                        }
                    }
                }
                OpsCmd::Caps { gvk } => {
                    let ns = cli.namespace.as_deref();
                    let ops = KubeOps::new();
                    match ops.caps(ns, gvk.as_deref()).await {
                        Ok(caps) => match cli.output {
                            Output::Human => {
                                println!("namespace: {}", caps.namespace.as_deref().unwrap_or("(none)"));
                                println!("pods/log get: {}", if caps.pods_log_get { "yes" } else { "no" });
                                println!("pods/exec create: {}", if caps.pods_exec_create { "yes" } else { "no" });
                                println!("pods/portforward create: {}", if caps.pods_portforward_create { "yes" } else { "no" });
                                println!("nodes patch: {}", if caps.nodes_patch { "yes" } else { "no" });
                                match caps.pods_eviction_create { Some(b) => println!("pods/eviction create: {}", if b { "yes" } else { "no" }), None => println!("pods/eviction create: (ns not set)") }
                                if let Some(sc) = caps.scale {
                                    println!("scale for {} ({}): subresource patch: {}, spec.replicas patch: {}", sc.gvk, sc.resource, if sc.subresource_patch {"yes"} else {"no"}, if sc.spec_replicas_patch {"yes"} else {"no"});
                                } else {
                                    println!("scale: (no gvk provided)");
                                }
                            }
                            Output::Json => println!("{}", serde_json::to_string_pretty(&caps)?),
                        },
                        Err(e) => eprintln!("caps error: {}", e),
                    }
                }
            }
        }
    }

    Ok(())
}

fn render_age(creation_ts: i64) -> String {
    if creation_ts <= 0 { return "-".to_string(); }
    let now = std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap_or_default().as_secs() as i64;
    let mut secs = (now - creation_ts).max(0) as u64;
    let days = secs / 86_400; secs %= 86_400;
    let hours = secs / 3600; secs %= 3600;
    let mins = secs / 60; secs %= 60;
    if days > 0 { format!("{}d{}h", days, hours) }
    else if hours > 0 { format!("{}h{}m", hours, mins) }
    else if mins > 0 { format!("{}m", mins) }
    else { format!("{}s", secs) }
}

fn json_key(v: &serde_json::Value) -> String {
    let meta = v.get("metadata");
    let name = meta.and_then(|m| m.get("name")).and_then(|v| v.as_str()).unwrap_or("");
    if let Some(ns) = meta.and_then(|m| m.get("namespace")).and_then(|v| v.as_str()) {
        format!("{}/{}", ns, name)
    } else {
        name.to_string()
    }
}

fn read_input(path: &str) -> Result<String> {
    if path == "-" {
        use std::io::Read;
        let mut s = String::new();
        std::io::stdin().read_to_string(&mut s)?;
        Ok(s)
    } else {
        Ok(std::fs::read_to_string(path)?)
    }
}

async fn fetch_uid_for(gvk_key: &str, name: &str, namespace: Option<&str>) -> Result<String> {
    use kube::{discovery::{Discovery, Scope}, api::Api, core::{DynamicObject, GroupVersionKind}};
    let client = kube::Client::try_default().await?;
    // Parse key
    let (group, version, kind) = parse_gvk(gvk_key).ok_or_else(|| anyhow::anyhow!("invalid gvk: {}", gvk_key))?;
    let gvk = GroupVersionKind { group, version, kind };
    // Find ApiResource
    let discovery = Discovery::new(client.clone()).run().await?;
    let mut ar_opt: Option<(kube::core::ApiResource, bool)> = None;
    for group in discovery.groups() {
        for (ar, caps) in group.recommended_resources() {
            if ar.group == gvk.group && ar.version == gvk.version && ar.kind == gvk.kind {
                ar_opt = Some((ar.clone(), matches!(caps.scope, Scope::Namespaced)));
                break;
            }
        }
    }
    let (ar, namespaced) = ar_opt.ok_or_else(|| anyhow::anyhow!("GVK not found: {}/{}/{}", gvk.group, gvk.version, gvk.kind))?;
    let api: Api<DynamicObject> = if namespaced {
        match namespace {
            Some(ns) => Api::namespaced_with(client.clone(), ns, &ar),
            None => return Err(anyhow::anyhow!("namespace required for namespaced kind")),
        }
    } else { Api::all_with(client.clone(), &ar) };
    let obj = api.get(name).await?;
    let uid = obj.metadata.uid.ok_or_else(|| anyhow::anyhow!("object missing metadata.uid"))?;
    Ok(uid)
}

fn parse_uid(uid_str: &str) -> Result<orka_core::Uid> {
    let u = uuid::Uuid::parse_str(uid_str).map_err(|e| anyhow::anyhow!("invalid uid: {}", e))?;
    Ok(*u.as_bytes())
}

fn parse_port_mapping(s: &str) -> Result<(u16, u16)> {
    if let Some((l, r)) = s.split_once(':') {
        let lp: u16 = l.parse().map_err(|_| anyhow::anyhow!("invalid local port: {}", l))?;
        let rp: u16 = r.parse().map_err(|_| anyhow::anyhow!("invalid remote port: {}", r))?;
        Ok((lp, rp))
    } else {
        let p: u16 = s.parse().map_err(|_| anyhow::anyhow!("invalid port: {}", s))?;
        Ok((p, p))
    }
}

fn render_pf_event(e: &orka_ops::ForwardEvent) {
    match e {
        orka_ops::ForwardEvent::Ready(addr) => println!("ready on {}", addr),
        orka_ops::ForwardEvent::Connected(peer) => println!("connected: {}", peer),
        orka_ops::ForwardEvent::Closed => println!("closed"),
        orka_ops::ForwardEvent::Error(err) => eprintln!("error: {}", err),
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn parse_gvk_core_and_group() {
        let core = parse_gvk("v1/Pod").expect("ok");
        assert_eq!(core, ("".into(), "v1".into(), "Pod".into()));
        let grp = parse_gvk("apps/v1/Deployment").expect("ok");
        assert_eq!(grp, ("apps".into(), "v1".into(), "Deployment".into()));
        assert!(parse_gvk("bad").is_none());
    }

    #[test]
    fn parse_port_mapping_variants() {
        assert_eq!(parse_port_mapping("8080").unwrap(), (8080, 8080));
        assert_eq!(parse_port_mapping("9999:80").unwrap(), (9999, 80));
        assert!(parse_port_mapping("notaport").is_err());
        assert!(parse_port_mapping("12:not").is_err());
    }

    #[test]
    fn parse_uid_valid_and_invalid() {
        // 16-byte UUID
        let u = uuid::Uuid::new_v4();
        let uid = parse_uid(&u.to_string()).expect("ok");
        assert_eq!(uid, *u.as_bytes());
        assert!(parse_uid("not-a-uuid").is_err());
    }

    #[test]
    fn json_key_with_and_without_ns() {
        let v1: serde_json::Value = serde_json::json!({"metadata": {"name": "n"}});
        assert_eq!(json_key(&v1), "n");
        let v2: serde_json::Value = serde_json::json!({"metadata": {"namespace": "ns", "name": "n"}});
        assert_eq!(json_key(&v2), "ns/n");
    }
}
</file>

</files>
